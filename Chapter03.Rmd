# Classification using Nearest Neighbors

```{r setup, include=FALSE}
library(gmodels)
library(class)
```

## Lazy Learning

Nearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples

The strengths and weaknesses of this algorithm are as follows:

Classification algorithms based on the nearest neighbor methods are considered lazy learning algorithms because, technically speaking, no abstraction occurs.

Uses information about an example's k-nearest neighbors to classify unlabeled examples.  The letter k is a variable term implying that any number of nearest neighbors to be used.

distance function, or a formula that measures the similarity between the two instances.

Euclidean distance, which is the distance one would measure if it were possible to use a ruler to connect two points.

- Euclidean distance is measured "as the crow flies," implying the shortest direct route.

- Manhattan distance, which is based on the paths a pedestrian would take by walking city blocks.

The balance between overfitting and underfitting the training data is a problem known as bias-variance tradeoff.

k equal to the square root of the number of training examples

weighted voting process in which the vote of the closer neighbors is considered more authoritative than the vote of the far away neighborh


## Example: Classifying Cancer Samples

### Step 1: collecting data

We will utilize the Wisconsin Breast Cancer Diagnostic dataset from the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml. This data was donated by researchers of the University of Wisconsin and includes the measurements from digitized images of fine-needle aspirate of a breast mass. The values represent the characteristics of the cell nuclei present in the digital image.

The breast cancer data includes 569 examples of cancer biopsies, each with 32 features. One feature is an identification number, another is the cancer diagnosis, and 30 are numeric-valued laboratory measurements. The diagnosis is coded as "M" to indicate malignant or "B" to indicate benign.

### Step 2: Exploring and preparing the data

- import the CSV file

```{r}
wbcd <- read.csv("Chapter 03/wisc_bc_data.csv")
```

- Examine the structure of the wbcd data frame

The other 30 numeric measurements comprise the mean, standard error, and worst (that is, largest) value for 10 different characteristics of the digitized cell nuclei.

```{r}
str(wbcd)
```

- Drop the id feature

As this is simply a unique identifier (ID) for each patient in the data, it does not provide useful information, and we will need to exclude it from the model.

```{r}
wbcd <- wbcd[-1]
```

- table of diagnosis

```{r}
table(wbcd$diagnosis)
```

- Recode diagnosis as a factor

```{r}
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
                         labels = c("Benign", "Malignant"))
```

- Table or proportions with more informative labels

```{r}
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
```

- Summarize three numeric features

```{r}
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
```

The distance calculation for k-NN is heavily dependent upon the measurement scale of the input features. The impact of area is going to be much larger than the smoothness in the distance calculation.

The traditional method of rescaling features for k-NN is `min-max normalization`. This process transforms a feature such that all of its values fall in a range between 0 and 1.

`z-score standardization` subtracts the mean value of feature X, and divides the outcome by the standard deviation of X.

The same rescaling method used on the k-NN training dataset must also be applied to the examples the algorithm will later classify.

- use z-score standardization under the assumption that the future examples will have similar mean and standard deviation as the training examples.

The Euclidean distance formula is not defined for nominal data: dummy coding, where a value of 1 indicates one category, and 0, the other

- n-category nominal feature can be dummy coded by creating the binary indicator variables for (n - 1) levels of the feature

- Create normalization function

```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

- Test normalization function - result should be identical

```{r}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
```

- Normalize the wbcd data

```{r}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
```

- Confirm that normalization worked

```{r}
summary(wbcd_n$area_mean)
```

- Create training and test data

```{r}
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
```

- Create labels for training and test data

```{r}
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
```

### Step 3: Training a model on the data

```{r}
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
                      cl = as.vector(wbcd_train_labels), k = 21)
```

### Step 4: Evaluating model performance

Create the cross tabulation of predicted vs. actual

```{r}
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
           prop.chisq = FALSE)
```
           
### Step 5: Improving model performance

- Use the scale() function to z-score standardize a data frame

```{r}
wbcd_z <- as.data.frame(scale(wbcd[-1]))
```

- Confirm that the transformation was applied correctly

```{r}
summary(wbcd_z$area_mean)
```

- Create training and test datasets

```{r}
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
```

- re-classify test cases

```{r}
wbcd_test_pred <- knn(train = wbcd_train, test =  wbcd_test,
                      cl = wbcd_train_labels, k = 21)
```

- Create the cross tabulation of predicted vs. actual

```{r}
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
           prop.chisq = FALSE)
```

- try several different values of k

```{r}
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=1)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=5)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=11)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=15)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=27)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
```

## Summary

- k-NN does not do any learning
- k-NN simply stores the training data verbatim
- capable of tackling extremely complex tasks, such as the identification of cancerous masses
