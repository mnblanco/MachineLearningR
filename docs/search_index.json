[
["index.html", "Machine Learning with R Solutions Welcome", " Machine Learning with R Solutions Marjorie Blanco 2019-01-02 Welcome This contains solutions to the exercise in Machine Learning with R by Brett Lantz. This work is licensed under a Creative Commons Attribution 4.0 International License "],
["introducing-machine-learning.html", "Chapter 1 Introducing Machine Learning", " Chapter 1 Introducing Machine Learning "],
["managing-and-understanding-data.html", "Chapter 2 Managing and Understanding Data 2.1 R data structures 2.2 Managing data with R 2.3 Exploring and understanding data 2.4 data exploration example using used car data 2.5 Exploring categorical variables", " Chapter 2 Managing and Understanding Data 2.1 R data structures 2.1.1 Vectors create vectors of data for three medical patients subject_name &lt;- c(&quot;John Doe&quot;, &quot;Jane Doe&quot;, &quot;Steve Graves&quot;) temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) access the second element in body temperature vector temperature[2] ## [1] 98.6 examples of accessing items in vector include items in the range 2 to 3 temperature[2:3] ## [1] 98.6 101.4 exclude item 2 using the minus sign temperature[-2] ## [1] 98.1 101.4 use a vector to indicate whether to include item temperature[c(TRUE, TRUE, FALSE)] ## [1] 98.1 98.6 2.1.2 Factors add gender factor gender &lt;- factor(c(&quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;)) gender ## [1] MALE FEMALE MALE ## Levels: FEMALE MALE add blood type factor blood &lt;- factor(c(&quot;O&quot;, &quot;AB&quot;, &quot;A&quot;), levels = c(&quot;A&quot;, &quot;B&quot;, &quot;AB&quot;, &quot;O&quot;)) blood ## [1] O AB A ## Levels: A B AB O add ordered factor symptoms &lt;- factor(c(&quot;SEVERE&quot;, &quot;MILD&quot;, &quot;MODERATE&quot;), levels = c(&quot;MILD&quot;, &quot;MODERATE&quot;, &quot;SEVERE&quot;), ordered = TRUE) symptoms ## [1] SEVERE MILD MODERATE ## Levels: MILD &lt; MODERATE &lt; SEVERE check for symptoms greater than moderate symptoms &gt; &quot;MODERATE&quot; ## [1] TRUE FALSE FALSE 2.1.3 Lists display information for a patient subject_name[1] ## [1] &quot;John Doe&quot; temperature[1] ## [1] 98.1 flu_status[1] ## [1] FALSE gender[1] ## [1] MALE ## Levels: FEMALE MALE blood[1] ## [1] O ## Levels: A B AB O symptoms[1] ## [1] SEVERE ## Levels: MILD &lt; MODERATE &lt; SEVERE create list for a patient subject1 &lt;- list(fullname = subject_name[1], temperature = temperature[1], flu_status = flu_status[1], gender = gender[1], blood = blood[1], symptoms = symptoms[1]) display the patient subject1 ## $fullname ## [1] &quot;John Doe&quot; ## ## $temperature ## [1] 98.1 ## ## $flu_status ## [1] FALSE ## ## $gender ## [1] MALE ## Levels: FEMALE MALE ## ## $blood ## [1] O ## Levels: A B AB O ## ## $symptoms ## [1] SEVERE ## Levels: MILD &lt; MODERATE &lt; SEVERE methods for accessing a list get a single list value by position (returns a sub-list) subject1[2] ## $temperature ## [1] 98.1 get a single list value by position (returns a numeric vector) subject1[[2]] ## [1] 98.1 get a single list value by name subject1$temperature ## [1] 98.1 get several list items by specifying a vector of names subject1[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## $temperature ## [1] 98.1 ## ## $flu_status ## [1] FALSE access a list like a vector get values 2 and 3 subject1[2:3] ## $temperature ## [1] 98.1 ## ## $flu_status ## [1] FALSE 2.1.4 Data frames create a data frame from medical patient data pt_data &lt;- data.frame(subject_name, temperature, flu_status, gender, blood, symptoms, stringsAsFactors = FALSE) display the data frame pt_data ## subject_name temperature flu_status gender blood symptoms ## 1 John Doe 98.1 FALSE MALE O SEVERE ## 2 Jane Doe 98.6 FALSE FEMALE AB MILD ## 3 Steve Graves 101.4 TRUE MALE A MODERATE accessing a data frame get a single column pt_data$subject_name ## [1] &quot;John Doe&quot; &quot;Jane Doe&quot; &quot;Steve Graves&quot; get several columns by specifying a vector of names pt_data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE this is the same as above, extracting temperature and flu_status pt_data[2:3] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE accessing by row and column pt_data[1, 2] ## [1] 98.1 accessing several rows and several columns using vectors pt_data[c(1, 3), c(2, 4)] ## temperature gender ## 1 98.1 MALE ## 3 101.4 MALE Leave a row or column blank to extract all rows or columns column 1, all rows pt_data[, 1] ## [1] &quot;John Doe&quot; &quot;Jane Doe&quot; &quot;Steve Graves&quot; row 1, all columns pt_data[1, ] ## subject_name temperature flu_status gender blood symptoms ## 1 John Doe 98.1 FALSE MALE O SEVERE all rows and all columns pt_data[ , ] ## subject_name temperature flu_status gender blood symptoms ## 1 John Doe 98.1 FALSE MALE O SEVERE ## 2 Jane Doe 98.6 FALSE FEMALE AB MILD ## 3 Steve Graves 101.4 TRUE MALE A MODERATE the following are equivalent pt_data[c(1, 3), c(&quot;temperature&quot;, &quot;gender&quot;)] ## temperature gender ## 1 98.1 MALE ## 3 101.4 MALE pt_data[-2, c(-1, -3, -5, -6)] ## temperature gender ## 1 98.1 MALE ## 3 101.4 MALE 2.1.5 Matrixes create a 2x2 matrix m &lt;- matrix(c(1, 2, 3, 4), nrow = 2) m ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 equivalent to the above m &lt;- matrix(c(1, 2, 3, 4), ncol = 2) m ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 create a 2x3 matrix m &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2) m ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 create a 3x2 matrix m &lt;- matrix(c(1, 2, 3, 4, 5, 6), ncol = 2) m ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 extract values from matrixes m[1, 1] ## [1] 1 m[3, 2] ## [1] 6 extract rows m[1, ] ## [1] 1 4 extract columns m[, 1] ## [1] 1 2 3 2.2 Managing data with R 2.2.1 saving, loading, and removing R data structures show all data structures in memory ls() ## [1] &quot;blood&quot; &quot;flu_status&quot; &quot;gender&quot; &quot;m&quot; ## [5] &quot;pt_data&quot; &quot;subject_name&quot; &quot;subject1&quot; &quot;symptoms&quot; ## [9] &quot;temperature&quot; remove the m and subject1 objects rm(m, subject1) ls() ## [1] &quot;blood&quot; &quot;flu_status&quot; &quot;gender&quot; &quot;pt_data&quot; ## [5] &quot;subject_name&quot; &quot;symptoms&quot; &quot;temperature&quot; rm(list=ls()) 2.3 Exploring and understanding data 2.4 data exploration example using used car data usedcars &lt;- read.csv(&quot;Chapter 02/usedcars.csv&quot;, stringsAsFactors = FALSE) 2.4.1 Exploring the structure of data get structure of used car data str(usedcars) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ year : int 2011 2011 2011 2011 2012 2010 2011 2010 2011 2010 ... ## $ model : chr &quot;SEL&quot; &quot;SEL&quot; &quot;SEL&quot; &quot;SEL&quot; ... ## $ price : int 21992 20995 19995 17809 17500 17495 17000 16995 16995 16995 ... ## $ mileage : int 7413 10926 7351 11613 8367 25125 27393 21026 32655 36116 ... ## $ color : chr &quot;Yellow&quot; &quot;Gray&quot; &quot;Silver&quot; &quot;Gray&quot; ... ## $ transmission: chr &quot;AUTO&quot; &quot;AUTO&quot; &quot;AUTO&quot; &quot;AUTO&quot; ... 2.4.2 Exploring numeric variables summarize numeric variables summary(usedcars$year) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2000 2008 2009 2009 2010 2012 summary(usedcars[c(&quot;price&quot;, &quot;mileage&quot;)]) ## price mileage ## Min. : 3800 Min. : 4867 ## 1st Qu.:10995 1st Qu.: 27200 ## Median :13592 Median : 36385 ## Mean :12962 Mean : 44261 ## 3rd Qu.:14904 3rd Qu.: 55124 ## Max. :21992 Max. :151479 calculate the mean income (36000 + 44000 + 56000) / 3 ## [1] 45333.33 mean(c(36000, 44000, 56000)) ## [1] 45333.33 the median income median(c(36000, 44000, 56000)) ## [1] 44000 the min/max of used car prices range(usedcars$price) ## [1] 3800 21992 the difference of the range diff(range(usedcars$price)) ## [1] 18192 IQR for used car prices IQR(usedcars$price) ## [1] 3909.5 use quantile to calculate five-number summary quantile(usedcars$price) ## 0% 25% 50% 75% 100% ## 3800.0 10995.0 13591.5 14904.5 21992.0 the 99th percentile quantile(usedcars$price, probs = c(0.01, 0.99)) ## 1% 99% ## 5428.69 20505.00 quintiles quantile(usedcars$price, seq(from = 0, to = 1, by = 0.20)) ## 0% 20% 40% 60% 80% 100% ## 3800.0 10759.4 12993.8 13992.0 14999.0 21992.0 boxplot of used car prices and mileage boxplot(usedcars$price, main=&quot;Boxplot of Used Car Prices&quot;, ylab=&quot;Price ($)&quot;) boxplot(usedcars$mileage, main=&quot;Boxplot of Used Car Mileage&quot;, ylab=&quot;Odometer (mi.)&quot;) histograms of used car prices and mileage hist(usedcars$price, main = &quot;Histogram of Used Car Prices&quot;, xlab = &quot;Price ($)&quot;) hist(usedcars$mileage, main = &quot;Histogram of Used Car Mileage&quot;, xlab = &quot;Odometer (mi.)&quot;) variance and standard deviation of the used car data var(usedcars$price) ## [1] 9749892 sd(usedcars$price) ## [1] 3122.482 var(usedcars$mileage) ## [1] 728033954 sd(usedcars$mileage) ## [1] 26982.1 2.5 Exploring categorical variables one-way tables for the used car data table(usedcars$year) ## ## 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 ## 3 1 1 1 3 2 6 11 14 42 49 16 1 table(usedcars$model) ## ## SE SEL SES ## 78 23 49 table(usedcars$color) ## ## Black Blue Gold Gray Green Red Silver White Yellow ## 35 17 1 16 5 25 32 16 3 compute table proportions model_table &lt;- table(usedcars$model) prop.table(model_table) ## ## SE SEL SES ## 0.5200000 0.1533333 0.3266667 round the data color_table &lt;- table(usedcars$color) color_pct &lt;- prop.table(color_table) * 100 round(color_pct, digits = 1) ## ## Black Blue Gold Gray Green Red Silver White Yellow ## 23.3 11.3 0.7 10.7 3.3 16.7 21.3 10.7 2.0 2.5.1 Exploring relationships between variables scatterplot of price vs. mileage plot(x = usedcars$mileage, y = usedcars$price, main = &quot;Scatterplot of Price vs. Mileage&quot;, xlab = &quot;Used Car Odometer (mi.)&quot;, ylab = &quot;Used Car Price ($)&quot;) new variable indicating conservative colors usedcars$conservative &lt;- usedcars$color %in% c(&quot;Black&quot;, &quot;Gray&quot;, &quot;Silver&quot;, &quot;White&quot;) checking our variable table(usedcars$conservative) ## ## FALSE TRUE ## 51 99 Crosstab of conservative by model CrossTable(x = usedcars$model, y = usedcars$conservative) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 150 ## ## ## | usedcars$conservative ## usedcars$model | FALSE | TRUE | Row Total | ## ---------------|-----------|-----------|-----------| ## SE | 27 | 51 | 78 | ## | 0.009 | 0.004 | | ## | 0.346 | 0.654 | 0.520 | ## | 0.529 | 0.515 | | ## | 0.180 | 0.340 | | ## ---------------|-----------|-----------|-----------| ## SEL | 7 | 16 | 23 | ## | 0.086 | 0.044 | | ## | 0.304 | 0.696 | 0.153 | ## | 0.137 | 0.162 | | ## | 0.047 | 0.107 | | ## ---------------|-----------|-----------|-----------| ## SES | 17 | 32 | 49 | ## | 0.007 | 0.004 | | ## | 0.347 | 0.653 | 0.327 | ## | 0.333 | 0.323 | | ## | 0.113 | 0.213 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 51 | 99 | 150 | ## | 0.340 | 0.660 | | ## ---------------|-----------|-----------|-----------| ## ## "],
["classification-using-nearest-neighbors.html", "Chapter 3 Classification using Nearest Neighbors 3.1 Lazy Learning 3.2 Example: Classifying Cancer Samples 3.3 Summary", " Chapter 3 Classification using Nearest Neighbors 3.1 Lazy Learning Nearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples The strengths and weaknesses of this algorithm are as follows: Classification algorithms based on the nearest neighbor methods are considered lazy learning algorithms because, technically speaking, no abstraction occurs. Uses information about an example’s k-nearest neighbors to classify unlabeled examples. The letter k is a variable term implying that any number of nearest neighbors to be used. distance function, or a formula that measures the similarity between the two instances. Euclidean distance, which is the distance one would measure if it were possible to use a ruler to connect two points. Euclidean distance is measured “as the crow flies,” implying the shortest direct route. Manhattan distance, which is based on the paths a pedestrian would take by walking city blocks. The balance between overfitting and underfitting the training data is a problem known as bias-variance tradeoff. k equal to the square root of the number of training examples weighted voting process in which the vote of the closer neighbors is considered more authoritative than the vote of the far away neighborh 3.2 Example: Classifying Cancer Samples 3.2.1 Step 1: collecting data We will utilize the Wisconsin Breast Cancer Diagnostic dataset from the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml. This data was donated by researchers of the University of Wisconsin and includes the measurements from digitized images of fine-needle aspirate of a breast mass. The values represent the characteristics of the cell nuclei present in the digital image. The breast cancer data includes 569 examples of cancer biopsies, each with 32 features. One feature is an identification number, another is the cancer diagnosis, and 30 are numeric-valued laboratory measurements. The diagnosis is coded as “M” to indicate malignant or “B” to indicate benign. 3.2.2 Step 2: Exploring and preparing the data import the CSV file wbcd &lt;- read.csv(&quot;Chapter 03/wisc_bc_data.csv&quot;) Examine the structure of the wbcd data frame The other 30 numeric measurements comprise the mean, standard error, and worst (that is, largest) value for 10 different characteristics of the digitized cell nuclei. str(wbcd) ## &#39;data.frame&#39;: 569 obs. of 32 variables: ## $ id : int 87139402 8910251 905520 868871 9012568 906539 925291 87880 862989 89827 ... ## $ diagnosis : Factor w/ 2 levels &quot;B&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 2 1 1 ... ## $ radius_mean : num 12.3 10.6 11 11.3 15.2 ... ## $ texture_mean : num 12.4 18.9 16.8 13.4 13.2 ... ## $ perimeter_mean : num 78.8 69.3 70.9 73 97.7 ... ## $ area_mean : num 464 346 373 385 712 ... ## $ smoothness_mean : num 0.1028 0.0969 0.1077 0.1164 0.0796 ... ## $ compactness_mean : num 0.0698 0.1147 0.078 0.1136 0.0693 ... ## $ concavity_mean : num 0.0399 0.0639 0.0305 0.0464 0.0339 ... ## $ points_mean : num 0.037 0.0264 0.0248 0.048 0.0266 ... ## $ symmetry_mean : num 0.196 0.192 0.171 0.177 0.172 ... ## $ dimension_mean : num 0.0595 0.0649 0.0634 0.0607 0.0554 ... ## $ radius_se : num 0.236 0.451 0.197 0.338 0.178 ... ## $ texture_se : num 0.666 1.197 1.387 1.343 0.412 ... ## $ perimeter_se : num 1.67 3.43 1.34 1.85 1.34 ... ## $ area_se : num 17.4 27.1 13.5 26.3 17.7 ... ## $ smoothness_se : num 0.00805 0.00747 0.00516 0.01127 0.00501 ... ## $ compactness_se : num 0.0118 0.03581 0.00936 0.03498 0.01485 ... ## $ concavity_se : num 0.0168 0.0335 0.0106 0.0219 0.0155 ... ## $ points_se : num 0.01241 0.01365 0.00748 0.01965 0.00915 ... ## $ symmetry_se : num 0.0192 0.035 0.0172 0.0158 0.0165 ... ## $ dimension_se : num 0.00225 0.00332 0.0022 0.00344 0.00177 ... ## $ radius_worst : num 13.5 11.9 12.4 11.9 16.2 ... ## $ texture_worst : num 15.6 22.9 26.4 15.8 15.7 ... ## $ perimeter_worst : num 87 78.3 79.9 76.5 104.5 ... ## $ area_worst : num 549 425 471 434 819 ... ## $ smoothness_worst : num 0.139 0.121 0.137 0.137 0.113 ... ## $ compactness_worst: num 0.127 0.252 0.148 0.182 0.174 ... ## $ concavity_worst : num 0.1242 0.1916 0.1067 0.0867 0.1362 ... ## $ points_worst : num 0.0939 0.0793 0.0743 0.0861 0.0818 ... ## $ symmetry_worst : num 0.283 0.294 0.3 0.21 0.249 ... ## $ dimension_worst : num 0.0677 0.0759 0.0788 0.0678 0.0677 ... Drop the id feature As this is simply a unique identifier (ID) for each patient in the data, it does not provide useful information, and we will need to exclude it from the model. wbcd &lt;- wbcd[-1] table of diagnosis table(wbcd$diagnosis) ## ## B M ## 357 212 Recode diagnosis as a factor wbcd$diagnosis &lt;- factor(wbcd$diagnosis, levels = c(&quot;B&quot;, &quot;M&quot;), labels = c(&quot;Benign&quot;, &quot;Malignant&quot;)) Table or proportions with more informative labels round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1) ## ## Benign Malignant ## 62.7 37.3 Summarize three numeric features summary(wbcd[c(&quot;radius_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;)]) ## radius_mean area_mean smoothness_mean ## Min. : 6.981 Min. : 143.5 Min. :0.05263 ## 1st Qu.:11.700 1st Qu.: 420.3 1st Qu.:0.08637 ## Median :13.370 Median : 551.1 Median :0.09587 ## Mean :14.127 Mean : 654.9 Mean :0.09636 ## 3rd Qu.:15.780 3rd Qu.: 782.7 3rd Qu.:0.10530 ## Max. :28.110 Max. :2501.0 Max. :0.16340 The distance calculation for k-NN is heavily dependent upon the measurement scale of the input features. The impact of area is going to be much larger than the smoothness in the distance calculation. The traditional method of rescaling features for k-NN is min-max normalization. This process transforms a feature such that all of its values fall in a range between 0 and 1. z-score standardization subtracts the mean value of feature X, and divides the outcome by the standard deviation of X. The same rescaling method used on the k-NN training dataset must also be applied to the examples the algorithm will later classify. use z-score standardization under the assumption that the future examples will have similar mean and standard deviation as the training examples. The Euclidean distance formula is not defined for nominal data: dummy coding, where a value of 1 indicates one category, and 0, the other n-category nominal feature can be dummy coded by creating the binary indicator variables for (n - 1) levels of the feature Create normalization function normalize &lt;- function(x) { return ((x - min(x)) / (max(x) - min(x))) } Test normalization function - result should be identical normalize(c(1, 2, 3, 4, 5)) ## [1] 0.00 0.25 0.50 0.75 1.00 normalize(c(10, 20, 30, 40, 50)) ## [1] 0.00 0.25 0.50 0.75 1.00 Normalize the wbcd data wbcd_n &lt;- as.data.frame(lapply(wbcd[2:31], normalize)) Confirm that normalization worked summary(wbcd_n$area_mean) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.1174 0.1729 0.2169 0.2711 1.0000 Create training and test data wbcd_train &lt;- wbcd_n[1:469, ] wbcd_test &lt;- wbcd_n[470:569, ] Create labels for training and test data wbcd_train_labels &lt;- wbcd[1:469, 1] wbcd_test_labels &lt;- wbcd[470:569, 1] 3.2.3 Step 3: Training a model on the data wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test, cl = as.vector(wbcd_train_labels), k = 21) 3.2.4 Step 4: Evaluating model performance Create the cross tabulation of predicted vs. actual CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | wbcd_test_pred ## wbcd_test_labels | Benign | Malignant | Row Total | ## -----------------|-----------|-----------|-----------| ## Benign | 61 | 0 | 61 | ## | 1.000 | 0.000 | 0.610 | ## | 0.968 | 0.000 | | ## | 0.610 | 0.000 | | ## -----------------|-----------|-----------|-----------| ## Malignant | 2 | 37 | 39 | ## | 0.051 | 0.949 | 0.390 | ## | 0.032 | 1.000 | | ## | 0.020 | 0.370 | | ## -----------------|-----------|-----------|-----------| ## Column Total | 63 | 37 | 100 | ## | 0.630 | 0.370 | | ## -----------------|-----------|-----------|-----------| ## ## 3.2.5 Step 5: Improving model performance Use the scale() function to z-score standardize a data frame wbcd_z &lt;- as.data.frame(scale(wbcd[-1])) Confirm that the transformation was applied correctly summary(wbcd_z$area_mean) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.4532 -0.6666 -0.2949 0.0000 0.3632 5.2459 Create training and test datasets wbcd_train &lt;- wbcd_z[1:469, ] wbcd_test &lt;- wbcd_z[470:569, ] re-classify test cases wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21) Create the cross tabulation of predicted vs. actual CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | wbcd_test_pred ## wbcd_test_labels | Benign | Malignant | Row Total | ## -----------------|-----------|-----------|-----------| ## Benign | 61 | 0 | 61 | ## | 1.000 | 0.000 | 0.610 | ## | 0.924 | 0.000 | | ## | 0.610 | 0.000 | | ## -----------------|-----------|-----------|-----------| ## Malignant | 5 | 34 | 39 | ## | 0.128 | 0.872 | 0.390 | ## | 0.076 | 1.000 | | ## | 0.050 | 0.340 | | ## -----------------|-----------|-----------|-----------| ## Column Total | 66 | 34 | 100 | ## | 0.660 | 0.340 | | ## -----------------|-----------|-----------|-----------| ## ## try several different values of k wbcd_train &lt;- wbcd_n[1:469, ] wbcd_test &lt;- wbcd_n[470:569, ] wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=1) CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | wbcd_test_pred ## wbcd_test_labels | Benign | Malignant | Row Total | ## -----------------|-----------|-----------|-----------| ## Benign | 58 | 3 | 61 | ## | 0.951 | 0.049 | 0.610 | ## | 0.983 | 0.073 | | ## | 0.580 | 0.030 | | ## -----------------|-----------|-----------|-----------| ## Malignant | 1 | 38 | 39 | ## | 0.026 | 0.974 | 0.390 | ## | 0.017 | 0.927 | | ## | 0.010 | 0.380 | | ## -----------------|-----------|-----------|-----------| ## Column Total | 59 | 41 | 100 | ## | 0.590 | 0.410 | | ## -----------------|-----------|-----------|-----------| ## ## wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=5) CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | wbcd_test_pred ## wbcd_test_labels | Benign | Malignant | Row Total | ## -----------------|-----------|-----------|-----------| ## Benign | 61 | 0 | 61 | ## | 1.000 | 0.000 | 0.610 | ## | 0.968 | 0.000 | | ## | 0.610 | 0.000 | | ## -----------------|-----------|-----------|-----------| ## Malignant | 2 | 37 | 39 | ## | 0.051 | 0.949 | 0.390 | ## | 0.032 | 1.000 | | ## | 0.020 | 0.370 | | ## -----------------|-----------|-----------|-----------| ## Column Total | 63 | 37 | 100 | ## | 0.630 | 0.370 | | ## -----------------|-----------|-----------|-----------| ## ## wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=11) CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | wbcd_test_pred ## wbcd_test_labels | Benign | Malignant | Row Total | ## -----------------|-----------|-----------|-----------| ## Benign | 61 | 0 | 61 | ## | 1.000 | 0.000 | 0.610 | ## | 0.953 | 0.000 | | ## | 0.610 | 0.000 | | ## -----------------|-----------|-----------|-----------| ## Malignant | 3 | 36 | 39 | ## | 0.077 | 0.923 | 0.390 | ## | 0.047 | 1.000 | | ## | 0.030 | 0.360 | | ## -----------------|-----------|-----------|-----------| ## Column Total | 64 | 36 | 100 | ## | 0.640 | 0.360 | | ## -----------------|-----------|-----------|-----------| ## ## wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=15) CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | wbcd_test_pred ## wbcd_test_labels | Benign | Malignant | Row Total | ## -----------------|-----------|-----------|-----------| ## Benign | 61 | 0 | 61 | ## | 1.000 | 0.000 | 0.610 | ## | 0.953 | 0.000 | | ## | 0.610 | 0.000 | | ## -----------------|-----------|-----------|-----------| ## Malignant | 3 | 36 | 39 | ## | 0.077 | 0.923 | 0.390 | ## | 0.047 | 1.000 | | ## | 0.030 | 0.360 | | ## -----------------|-----------|-----------|-----------| ## Column Total | 64 | 36 | 100 | ## | 0.640 | 0.360 | | ## -----------------|-----------|-----------|-----------| ## ## wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21) CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | wbcd_test_pred ## wbcd_test_labels | Benign | Malignant | Row Total | ## -----------------|-----------|-----------|-----------| ## Benign | 61 | 0 | 61 | ## | 1.000 | 0.000 | 0.610 | ## | 0.968 | 0.000 | | ## | 0.610 | 0.000 | | ## -----------------|-----------|-----------|-----------| ## Malignant | 2 | 37 | 39 | ## | 0.051 | 0.949 | 0.390 | ## | 0.032 | 1.000 | | ## | 0.020 | 0.370 | | ## -----------------|-----------|-----------|-----------| ## Column Total | 63 | 37 | 100 | ## | 0.630 | 0.370 | | ## -----------------|-----------|-----------|-----------| ## ## wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=27) CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | wbcd_test_pred ## wbcd_test_labels | Benign | Malignant | Row Total | ## -----------------|-----------|-----------|-----------| ## Benign | 61 | 0 | 61 | ## | 1.000 | 0.000 | 0.610 | ## | 0.938 | 0.000 | | ## | 0.610 | 0.000 | | ## -----------------|-----------|-----------|-----------| ## Malignant | 4 | 35 | 39 | ## | 0.103 | 0.897 | 0.390 | ## | 0.062 | 1.000 | | ## | 0.040 | 0.350 | | ## -----------------|-----------|-----------|-----------| ## Column Total | 65 | 35 | 100 | ## | 0.650 | 0.350 | | ## -----------------|-----------|-----------|-----------| ## ## 3.3 Summary k-NN does not do any learning k-NN simply stores the training data verbatim capable of tackling extremely complex tasks, such as the identification of cancerous masses "],
["classification-using-naive-bayes.html", "Chapter 4 Classification using Naive Bayes 4.1 Naive Bayes 4.2 Example: Filtering spam SMS messages 4.3 Summary", " Chapter 4 Classification using Naive Bayes 4.1 Naive Bayes 4.2 Example: Filtering spam SMS messages 4.2.1 Step 2: Exploring and preparing the data read the sms data into the sms data frame sms_raw &lt;- read.csv(&quot;Chapter 04/sms_spam.csv&quot;) examine the structure of the sms data str(sms_raw) ## &#39;data.frame&#39;: 5559 obs. of 2 variables: ## $ type: Factor w/ 2 levels &quot;ham&quot;,&quot;spam&quot;: 1 1 1 2 2 1 1 1 2 1 ... ## $ text: Factor w/ 5156 levels &quot; # in mca. But not conform.&quot;,..: 1651 2566 257 626 3308 190 357 3392 2726 1079 ... convert spam/ham to factor. sms_raw$type &lt;- factor(sms_raw$type) examine the type variable more carefully str(sms_raw$type) ## Factor w/ 2 levels &quot;ham&quot;,&quot;spam&quot;: 1 1 1 2 2 1 1 1 2 1 ... table(sms_raw$type) ## ## ham spam ## 4812 747 build a corpus using the text mining (tm) package sms_corpus &lt;- VCorpus(VectorSource(sms_raw$text)) examine the sms corpus print(sms_corpus) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 5559 inspect(sms_corpus[1:2]) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 2 ## ## [[1]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 49 ## ## [[2]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 23 as.character(sms_corpus[[1]]) ## [1] &quot;Hope you are having a good week. Just checking in&quot; lapply(sms_corpus[1:2], as.character) ## $`1` ## [1] &quot;Hope you are having a good week. Just checking in&quot; ## ## $`2` ## [1] &quot;K..give back my thanks.&quot; clean up the corpus using tm_map() sms_corpus_clean &lt;- tm_map(sms_corpus, content_transformer(tolower)) show the difference between sms_corpus and corpus_clean as.character(sms_corpus[[1]]) ## [1] &quot;Hope you are having a good week. Just checking in&quot; as.character(sms_corpus_clean[[1]]) ## [1] &quot;hope you are having a good week. just checking in&quot; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removeNumbers) # remove numbers sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removeWords, stopwords()) # remove stop words sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removePunctuation) # remove punctuation tip: create a custom function to replace (rather than remove) punctuation removePunctuation(&quot;hello...world&quot;) ## [1] &quot;helloworld&quot; replacePunctuation &lt;- function(x) { gsub(&quot;[[:punct:]]+&quot;, &quot; &quot;, x) } replacePunctuation(&quot;hello...world&quot;) ## [1] &quot;hello world&quot; illustration of word stemming wordStem(c(&quot;learn&quot;, &quot;learned&quot;, &quot;learning&quot;, &quot;learns&quot;)) ## [1] &quot;learn&quot; &quot;learn&quot; &quot;learn&quot; &quot;learn&quot; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stemDocument) sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stripWhitespace) # eliminate unneeded whitespace examine the final clean corpus lapply(sms_corpus[1:3], as.character) ## $`1` ## [1] &quot;Hope you are having a good week. Just checking in&quot; ## ## $`2` ## [1] &quot;K..give back my thanks.&quot; ## ## $`3` ## [1] &quot;Am also doing in cbe only. But have to pay.&quot; lapply(sms_corpus_clean[1:3], as.character) ## $`1` ## [1] &quot;hope good week just check&quot; ## ## $`2` ## [1] &quot;kgive back thank&quot; ## ## $`3` ## [1] &quot;also cbe pay&quot; create a document-term sparse matrix sms_dtm &lt;- DocumentTermMatrix(sms_corpus_clean) alternative solution: create a document-term sparse matrix directly from the SMS corpus sms_dtm2 &lt;- DocumentTermMatrix(sms_corpus, control = list( tolower = TRUE, removeNumbers = TRUE, stopwords = TRUE, removePunctuation = TRUE, stemming = TRUE )) alternative solution: using custom stop words function ensures identical result sms_dtm3 &lt;- DocumentTermMatrix(sms_corpus, control = list( tolower = TRUE, removeNumbers = TRUE, stopwords = function(x) { removeWords(x, stopwords()) }, removePunctuation = TRUE, stemming = TRUE )) compare the result sms_dtm ## &lt;&lt;DocumentTermMatrix (documents: 5559, terms: 6559)&gt;&gt; ## Non-/sparse entries: 42147/36419334 ## Sparsity : 100% ## Maximal term length: 40 ## Weighting : term frequency (tf) sms_dtm2 ## &lt;&lt;DocumentTermMatrix (documents: 5559, terms: 6961)&gt;&gt; ## Non-/sparse entries: 43221/38652978 ## Sparsity : 100% ## Maximal term length: 40 ## Weighting : term frequency (tf) sms_dtm3 ## &lt;&lt;DocumentTermMatrix (documents: 5559, terms: 6559)&gt;&gt; ## Non-/sparse entries: 42147/36419334 ## Sparsity : 100% ## Maximal term length: 40 ## Weighting : term frequency (tf) creating training and test datasets sms_dtm_train &lt;- sms_dtm[1:4169, ] sms_dtm_test &lt;- sms_dtm[4170:5559, ] also save the labels sms_train_labels &lt;- sms_raw[1:4169, ]$type sms_test_labels &lt;- sms_raw[4170:5559, ]$type check that the proportion of spam is similar prop.table(table(sms_train_labels)) ## sms_train_labels ## ham spam ## 0.8647158 0.1352842 prop.table(table(sms_test_labels)) ## sms_test_labels ## ham spam ## 0.8683453 0.1316547 word cloud visualization wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE) subset the training data into spam and ham groups spam &lt;- subset(sms_raw, type == &quot;spam&quot;) ham &lt;- subset(sms_raw, type == &quot;ham&quot;) wordcloud(spam$text, max.words = 40, scale = c(3, 0.5)) ## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): ## transformation drops documents ## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, ## tm::stopwords())): transformation drops documents wordcloud(ham$text, max.words = 40, scale = c(3, 0.5)) ## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): ## transformation drops documents ## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): ## transformation drops documents sms_dtm_freq_train &lt;- removeSparseTerms(sms_dtm_train, 0.999) sms_dtm_freq_train ## &lt;&lt;DocumentTermMatrix (documents: 4169, terms: 1104)&gt;&gt; ## Non-/sparse entries: 24827/4577749 ## Sparsity : 99% ## Maximal term length: 19 ## Weighting : term frequency (tf) indicator features for frequent words findFreqTerms(sms_dtm_train, 5) ## [1] &quot;£wk&quot; &quot;€˜m&quot; &quot;€˜s&quot; ## [4] &quot;abiola&quot; &quot;abl&quot; &quot;abt&quot; ## [7] &quot;accept&quot; &quot;access&quot; &quot;account&quot; ## [10] &quot;across&quot; &quot;act&quot; &quot;activ&quot; ## [13] &quot;actual&quot; &quot;add&quot; &quot;address&quot; ## [16] &quot;admir&quot; &quot;adult&quot; &quot;advanc&quot; ## [19] &quot;aft&quot; &quot;afternoon&quot; &quot;age&quot; ## [22] &quot;ago&quot; &quot;aha&quot; &quot;ahead&quot; ## [25] &quot;aight&quot; &quot;aint&quot; &quot;air&quot; ## [28] &quot;aiyo&quot; &quot;alex&quot; &quot;almost&quot; ## [31] &quot;alon&quot; &quot;alreadi&quot; &quot;alright&quot; ## [34] &quot;also&quot; &quot;alway&quot; &quot;angri&quot; ## [37] &quot;announc&quot; &quot;anoth&quot; &quot;answer&quot; ## [40] &quot;anymor&quot; &quot;anyon&quot; &quot;anyth&quot; ## [43] &quot;anytim&quot; &quot;anyway&quot; &quot;apart&quot; ## [46] &quot;app&quot; &quot;appli&quot; &quot;appreci&quot; ## [49] &quot;arcad&quot; &quot;ard&quot; &quot;area&quot; ## [52] &quot;argu&quot; &quot;argument&quot; &quot;armand&quot; ## [55] &quot;around&quot; &quot;arrang&quot; &quot;arriv&quot; ## [58] &quot;asap&quot; &quot;ask&quot; &quot;askd&quot; ## [61] &quot;attempt&quot; &quot;auction&quot; &quot;avail&quot; ## [64] &quot;ave&quot; &quot;avoid&quot; &quot;await&quot; ## [67] &quot;awak&quot; &quot;award&quot; &quot;away&quot; ## [70] &quot;awesom&quot; &quot;babe&quot; &quot;babi&quot; ## [73] &quot;back&quot; &quot;bad&quot; &quot;bag&quot; ## [76] &quot;bank&quot; &quot;bare&quot; &quot;basic&quot; ## [79] &quot;bath&quot; &quot;batteri&quot; &quot;bcoz&quot; ## [82] &quot;bday&quot; &quot;beauti&quot; &quot;becom&quot; ## [85] &quot;bed&quot; &quot;bedroom&quot; &quot;beer&quot; ## [88] &quot;begin&quot; &quot;believ&quot; &quot;best&quot; ## [91] &quot;better&quot; &quot;bid&quot; &quot;big&quot; ## [94] &quot;bill&quot; &quot;bird&quot; &quot;birthday&quot; ## [97] &quot;bit&quot; &quot;black&quot; &quot;blank&quot; ## [100] &quot;bless&quot; &quot;blue&quot; &quot;bluetooth&quot; ## [103] &quot;bold&quot; &quot;bonus&quot; &quot;boo&quot; ## [106] &quot;book&quot; &quot;boost&quot; &quot;bore&quot; ## [109] &quot;boss&quot; &quot;bother&quot; &quot;bout&quot; ## [112] &quot;box&quot; &quot;boy&quot; &quot;boytoy&quot; ## [115] &quot;break&quot; &quot;breath&quot; &quot;bring&quot; ## [118] &quot;brother&quot; &quot;bslvyl&quot; &quot;btnationalr&quot; ## [121] &quot;buck&quot; &quot;bus&quot; &quot;busi&quot; ## [124] &quot;buy&quot; &quot;cabin&quot; &quot;call&quot; ## [127] &quot;caller&quot; &quot;callertun&quot; &quot;camcord&quot; ## [130] &quot;came&quot; &quot;camera&quot; &quot;campus&quot; ## [133] &quot;can&quot; &quot;cancel&quot; &quot;cancer&quot; ## [136] &quot;cant&quot; &quot;car&quot; &quot;card&quot; ## [139] &quot;care&quot; &quot;carlo&quot; &quot;case&quot; ## [142] &quot;cash&quot; &quot;cashbal&quot; &quot;catch&quot; ## [145] &quot;caus&quot; &quot;celebr&quot; &quot;cell&quot; ## [148] &quot;centr&quot; &quot;chanc&quot; &quot;chang&quot; ## [151] &quot;charg&quot; &quot;chat&quot; &quot;cheap&quot; ## [154] &quot;cheaper&quot; &quot;check&quot; &quot;cheer&quot; ## [157] &quot;chennai&quot; &quot;chikku&quot; &quot;childish&quot; ## [160] &quot;children&quot; &quot;choic&quot; &quot;choos&quot; ## [163] &quot;christma&quot; &quot;claim&quot; &quot;class&quot; ## [166] &quot;clean&quot; &quot;clear&quot; &quot;close&quot; ## [169] &quot;club&quot; &quot;code&quot; &quot;coffe&quot; ## [172] &quot;cold&quot; &quot;colleagu&quot; &quot;collect&quot; ## [175] &quot;colleg&quot; &quot;colour&quot; &quot;come&quot; ## [178] &quot;comin&quot; &quot;comp&quot; &quot;compani&quot; ## [181] &quot;competit&quot; &quot;complet&quot; &quot;complimentari&quot; ## [184] &quot;comput&quot; &quot;condit&quot; &quot;confirm&quot; ## [187] &quot;congrat&quot; &quot;congratul&quot; &quot;connect&quot; ## [190] &quot;contact&quot; &quot;content&quot; &quot;contract&quot; ## [193] &quot;cook&quot; &quot;cool&quot; &quot;copi&quot; ## [196] &quot;correct&quot; &quot;cos&quot; &quot;cost&quot; ## [199] &quot;cost£pm&quot; &quot;costa&quot; &quot;coupl&quot; ## [202] &quot;cours&quot; &quot;cover&quot; &quot;coz&quot; ## [205] &quot;crave&quot; &quot;crazi&quot; &quot;creat&quot; ## [208] &quot;credit&quot; &quot;cri&quot; &quot;cross&quot; ## [211] &quot;cuddl&quot; &quot;cum&quot; &quot;cup&quot; ## [214] &quot;current&quot; &quot;custcar&quot; &quot;custom&quot; ## [217] &quot;cut&quot; &quot;cute&quot; &quot;cuz&quot; ## [220] &quot;dad&quot; &quot;daddi&quot; &quot;darl&quot; ## [223] &quot;darlin&quot; &quot;darren&quot; &quot;dat&quot; ## [226] &quot;date&quot; &quot;day&quot; &quot;dead&quot; ## [229] &quot;deal&quot; &quot;dear&quot; &quot;decid&quot; ## [232] &quot;decim&quot; &quot;decis&quot; &quot;deep&quot; ## [235] &quot;definit&quot; &quot;del&quot; &quot;deliv&quot; ## [238] &quot;deliveri&quot; &quot;den&quot; &quot;depend&quot; ## [241] &quot;detail&quot; &quot;didnt&quot; &quot;die&quot; ## [244] &quot;diet&quot; &quot;differ&quot; &quot;difficult&quot; ## [247] &quot;digit&quot; &quot;din&quot; &quot;dinner&quot; ## [250] &quot;direct&quot; &quot;dis&quot; &quot;discount&quot; ## [253] &quot;discuss&quot; &quot;disturb&quot; &quot;dnt&quot; ## [256] &quot;doc&quot; &quot;doctor&quot; &quot;doesnt&quot; ## [259] &quot;dog&quot; &quot;doin&quot; &quot;don&quot; ## [262] &quot;done&quot; &quot;dont&quot; &quot;door&quot; ## [265] &quot;doubl&quot; &quot;download&quot; &quot;draw&quot; ## [268] &quot;dream&quot; &quot;drink&quot; &quot;drive&quot; ## [271] &quot;drop&quot; &quot;drug&quot; &quot;dude&quot; ## [274] &quot;due&quot; &quot;dun&quot; &quot;dunno&quot; ## [277] &quot;dvd&quot; &quot;earli&quot; &quot;earlier&quot; ## [280] &quot;earth&quot; &quot;easi&quot; &quot;eat&quot; ## [283] &quot;eatin&quot; &quot;egg&quot; &quot;either&quot; ## [286] &quot;els&quot; &quot;email&quot; &quot;embarass&quot; ## [289] &quot;end&quot; &quot;energi&quot; &quot;england&quot; ## [292] &quot;enjoy&quot; &quot;enough&quot; &quot;enter&quot; ## [295] &quot;entitl&quot; &quot;entri&quot; &quot;envelop&quot; ## [298] &quot;etc&quot; &quot;euro&quot; &quot;eve&quot; ## [301] &quot;even&quot; &quot;ever&quot; &quot;everi&quot; ## [304] &quot;everybodi&quot; &quot;everyon&quot; &quot;everyth&quot; ## [307] &quot;exact&quot; &quot;exam&quot; &quot;excel&quot; ## [310] &quot;excit&quot; &quot;excus&quot; &quot;expect&quot; ## [313] &quot;experi&quot; &quot;expir&quot; &quot;extra&quot; ## [316] &quot;eye&quot; &quot;face&quot; &quot;facebook&quot; ## [319] &quot;fact&quot; &quot;fall&quot; &quot;famili&quot; ## [322] &quot;fanci&quot; &quot;fantasi&quot; &quot;fantast&quot; ## [325] &quot;far&quot; &quot;fast&quot; &quot;fat&quot; ## [328] &quot;father&quot; &quot;fault&quot; &quot;feb&quot; ## [331] &quot;feel&quot; &quot;felt&quot; &quot;fetch&quot; ## [334] &quot;fight&quot; &quot;figur&quot; &quot;file&quot; ## [337] &quot;fill&quot; &quot;film&quot; &quot;final&quot; ## [340] &quot;find&quot; &quot;fine&quot; &quot;finger&quot; ## [343] &quot;finish&quot; &quot;first&quot; &quot;fix&quot; ## [346] &quot;flag&quot; &quot;flat&quot; &quot;flight&quot; ## [349] &quot;flower&quot; &quot;follow&quot; &quot;fone&quot; ## [352] &quot;food&quot; &quot;forev&quot; &quot;forget&quot; ## [355] &quot;forgot&quot; &quot;forward&quot; &quot;found&quot; ## [358] &quot;freak&quot; &quot;free&quot; &quot;freemsg&quot; ## [361] &quot;freephon&quot; &quot;fren&quot; &quot;fri&quot; ## [364] &quot;friday&quot; &quot;friend&quot; &quot;friendship&quot; ## [367] &quot;frm&quot; &quot;frnd&quot; &quot;frnds&quot; ## [370] &quot;full&quot; &quot;fullonsmscom&quot; &quot;fun&quot; ## [373] &quot;funni&quot; &quot;futur&quot; &quot;gal&quot; ## [376] &quot;game&quot; &quot;gap&quot; &quot;gas&quot; ## [379] &quot;gave&quot; &quot;gay&quot; &quot;gentl&quot; ## [382] &quot;get&quot; &quot;gettin&quot; &quot;gift&quot; ## [385] &quot;girl&quot; &quot;girlfrnd&quot; &quot;give&quot; ## [388] &quot;glad&quot; &quot;god&quot; &quot;goe&quot; ## [391] &quot;goin&quot; &quot;gone&quot; &quot;gonna&quot; ## [394] &quot;good&quot; &quot;goodmorn&quot; &quot;goodnight&quot; ## [397] &quot;got&quot; &quot;goto&quot; &quot;gotta&quot; ## [400] &quot;great&quot; &quot;grin&quot; &quot;guarante&quot; ## [403] &quot;gud&quot; &quot;guess&quot; &quot;guy&quot; ## [406] &quot;gym&quot; &quot;haf&quot; &quot;haha&quot; ## [409] &quot;hai&quot; &quot;hair&quot; &quot;half&quot; ## [412] &quot;hand&quot; &quot;handset&quot; &quot;hang&quot; ## [415] &quot;happen&quot; &quot;happi&quot; &quot;hard&quot; ## [418] &quot;hate&quot; &quot;hav&quot; &quot;havent&quot; ## [421] &quot;head&quot; &quot;hear&quot; &quot;heard&quot; ## [424] &quot;heart&quot; &quot;heavi&quot; &quot;hee&quot; ## [427] &quot;hell&quot; &quot;hello&quot; &quot;help&quot; ## [430] &quot;hey&quot; &quot;hgsuiteland&quot; &quot;hit&quot; ## [433] &quot;hiya&quot; &quot;hmm&quot; &quot;hmmm&quot; ## [436] &quot;hmv&quot; &quot;hol&quot; &quot;hold&quot; ## [439] &quot;holder&quot; &quot;holiday&quot; &quot;home&quot; ## [442] &quot;hook&quot; &quot;hop&quot; &quot;hope&quot; ## [445] &quot;horni&quot; &quot;hospit&quot; &quot;hot&quot; ## [448] &quot;hotel&quot; &quot;hour&quot; &quot;hous&quot; ## [451] &quot;how&quot; &quot;howev&quot; &quot;howz&quot; ## [454] &quot;hrs&quot; &quot;httpwwwurawinnercom&quot; &quot;hug&quot; ## [457] &quot;huh&quot; &quot;hungri&quot; &quot;hurri&quot; ## [460] &quot;hurt&quot; &quot;ice&quot; &quot;idea&quot; ## [463] &quot;identifi&quot; &quot;ignor&quot; &quot;ill&quot; ## [466] &quot;immedi&quot; &quot;import&quot; &quot;inc&quot; ## [469] &quot;includ&quot; &quot;india&quot; &quot;info&quot; ## [472] &quot;inform&quot; &quot;insid&quot; &quot;instead&quot; ## [475] &quot;interest&quot; &quot;invit&quot; &quot;ipod&quot; ## [478] &quot;irrit&quot; &quot;ish&quot; &quot;island&quot; ## [481] &quot;issu&quot; &quot;ive&quot; &quot;izzit&quot; ## [484] &quot;januari&quot; &quot;jay&quot; &quot;job&quot; ## [487] &quot;john&quot; &quot;join&quot; &quot;joke&quot; ## [490] &quot;joy&quot; &quot;jst&quot; &quot;jus&quot; ## [493] &quot;just&quot; &quot;juz&quot; &quot;kate&quot; ## [496] &quot;keep&quot; &quot;kept&quot; &quot;kick&quot; ## [499] &quot;kid&quot; &quot;kill&quot; &quot;kind&quot; ## [502] &quot;kinda&quot; &quot;king&quot; &quot;kiss&quot; ## [505] &quot;knew&quot; &quot;know&quot; &quot;knw&quot; ## [508] &quot;ladi&quot; &quot;land&quot; &quot;landlin&quot; ## [511] &quot;laptop&quot; &quot;lar&quot; &quot;last&quot; ## [514] &quot;late&quot; &quot;later&quot; &quot;latest&quot; ## [517] &quot;laugh&quot; &quot;lazi&quot; &quot;ldn&quot; ## [520] &quot;lead&quot; &quot;learn&quot; &quot;least&quot; ## [523] &quot;leav&quot; &quot;lect&quot; &quot;left&quot; ## [526] &quot;leh&quot; &quot;lei&quot; &quot;less&quot; ## [529] &quot;lesson&quot; &quot;let&quot; &quot;letter&quot; ## [532] &quot;liao&quot; &quot;librari&quot; &quot;lie&quot; ## [535] &quot;life&quot; &quot;lift&quot; &quot;light&quot; ## [538] &quot;like&quot; &quot;line&quot; &quot;link&quot; ## [541] &quot;list&quot; &quot;listen&quot; &quot;littl&quot; ## [544] &quot;live&quot; &quot;lmao&quot; &quot;load&quot; ## [547] &quot;loan&quot; &quot;local&quot; &quot;locat&quot; ## [550] &quot;log&quot; &quot;lol&quot; &quot;london&quot; ## [553] &quot;long&quot; &quot;longer&quot; &quot;look&quot; ## [556] &quot;lookin&quot; &quot;lor&quot; &quot;lose&quot; ## [559] &quot;lost&quot; &quot;lot&quot; &quot;lovabl&quot; ## [562] &quot;love&quot; &quot;lover&quot; &quot;loyalti&quot; ## [565] &quot;ltd&quot; &quot;luck&quot; &quot;lucki&quot; ## [568] &quot;lunch&quot; &quot;luv&quot; &quot;mad&quot; ## [571] &quot;made&quot; &quot;mah&quot; &quot;mail&quot; ## [574] &quot;make&quot; &quot;malaria&quot; &quot;man&quot; ## [577] &quot;mani&quot; &quot;march&quot; &quot;mark&quot; ## [580] &quot;marri&quot; &quot;match&quot; &quot;mate&quot; ## [583] &quot;matter&quot; &quot;maxim&quot; &quot;maxmin&quot; ## [586] &quot;may&quot; &quot;mayb&quot; &quot;meal&quot; ## [589] &quot;mean&quot; &quot;meant&quot; &quot;med&quot; ## [592] &quot;medic&quot; &quot;meet&quot; &quot;meetin&quot; ## [595] &quot;meh&quot; &quot;member&quot; &quot;men&quot; ## [598] &quot;merri&quot; &quot;messag&quot; &quot;met&quot; ## [601] &quot;mid&quot; &quot;midnight&quot; &quot;might&quot; ## [604] &quot;min&quot; &quot;mind&quot; &quot;mine&quot; ## [607] &quot;minut&quot; &quot;miracl&quot; &quot;miss&quot; ## [610] &quot;mistak&quot; &quot;moan&quot; &quot;mob&quot; ## [613] &quot;mobil&quot; &quot;mobileupd&quot; &quot;mode&quot; ## [616] &quot;mom&quot; &quot;moment&quot; &quot;mon&quot; ## [619] &quot;monday&quot; &quot;money&quot; &quot;month&quot; ## [622] &quot;morn&quot; &quot;mother&quot; &quot;motorola&quot; ## [625] &quot;move&quot; &quot;movi&quot; &quot;mrng&quot; ## [628] &quot;mrt&quot; &quot;mrw&quot; &quot;msg&quot; ## [631] &quot;msgs&quot; &quot;mths&quot; &quot;much&quot; ## [634] &quot;mum&quot; &quot;murder&quot; &quot;music&quot; ## [637] &quot;must&quot; &quot;muz&quot; &quot;nah&quot; ## [640] &quot;nake&quot; &quot;name&quot; &quot;nation&quot; ## [643] &quot;natur&quot; &quot;naughti&quot; &quot;near&quot; ## [646] &quot;need&quot; &quot;net&quot; &quot;network&quot; ## [649] &quot;neva&quot; &quot;never&quot; &quot;new&quot; ## [652] &quot;news&quot; &quot;next&quot; &quot;nice&quot; ## [655] &quot;nigeria&quot; &quot;night&quot; &quot;nite&quot; ## [658] &quot;nobodi&quot; &quot;noe&quot; &quot;nokia&quot; ## [661] &quot;noon&quot; &quot;nope&quot; &quot;normal&quot; ## [664] &quot;normpton&quot; &quot;noth&quot; &quot;notic&quot; ## [667] &quot;now&quot; &quot;num&quot; &quot;number&quot; ## [670] &quot;nyt&quot; &quot;obvious&quot; &quot;offer&quot; ## [673] &quot;offic&quot; &quot;offici&quot; &quot;okay&quot; ## [676] &quot;oki&quot; &quot;old&quot; &quot;omg&quot; ## [679] &quot;one&quot; &quot;onlin&quot; &quot;onto&quot; ## [682] &quot;oop&quot; &quot;open&quot; &quot;oper&quot; ## [685] &quot;opinion&quot; &quot;opt&quot; &quot;optout&quot; ## [688] &quot;orang&quot; &quot;orchard&quot; &quot;order&quot; ## [691] &quot;oredi&quot; &quot;oso&quot; &quot;other&quot; ## [694] &quot;otherwis&quot; &quot;outsid&quot; &quot;pack&quot; ## [697] &quot;page&quot; &quot;paid&quot; &quot;pain&quot; ## [700] &quot;paper&quot; &quot;parent&quot; &quot;park&quot; ## [703] &quot;part&quot; &quot;parti&quot; &quot;partner&quot; ## [706] &quot;pass&quot; &quot;passion&quot; &quot;password&quot; ## [709] &quot;past&quot; &quot;pay&quot; &quot;peopl&quot; ## [712] &quot;per&quot; &quot;person&quot; &quot;pete&quot; ## [715] &quot;phone&quot; &quot;photo&quot; &quot;pic&quot; ## [718] &quot;pick&quot; &quot;pictur&quot; &quot;pin&quot; ## [721] &quot;piss&quot; &quot;pix&quot; &quot;pizza&quot; ## [724] &quot;place&quot; &quot;plan&quot; &quot;play&quot; ## [727] &quot;player&quot; &quot;pleas&quot; &quot;pleasur&quot; ## [730] &quot;plenti&quot; &quot;pls&quot; &quot;plus&quot; ## [733] &quot;plz&quot; &quot;pmin&quot; &quot;pmsg&quot; ## [736] &quot;pobox&quot; &quot;point&quot; &quot;poli&quot; ## [739] &quot;polic&quot; &quot;poor&quot; &quot;pop&quot; ## [742] &quot;possess&quot; &quot;possibl&quot; &quot;post&quot; ## [745] &quot;pound&quot; &quot;power&quot; &quot;ppm&quot; ## [748] &quot;pray&quot; &quot;present&quot; &quot;press&quot; ## [751] &quot;pretti&quot; &quot;previous&quot; &quot;price&quot; ## [754] &quot;princess&quot; &quot;privat&quot; &quot;prize&quot; ## [757] &quot;prob&quot; &quot;probabl&quot; &quot;problem&quot; ## [760] &quot;project&quot; &quot;promis&quot; &quot;pub&quot; ## [763] &quot;put&quot; &quot;qualiti&quot; &quot;question&quot; ## [766] &quot;quick&quot; &quot;quit&quot; &quot;quiz&quot; ## [769] &quot;quot&quot; &quot;rain&quot; &quot;random&quot; ## [772] &quot;rang&quot; &quot;rate&quot; &quot;rather&quot; ## [775] &quot;rcvd&quot; &quot;reach&quot; &quot;read&quot; ## [778] &quot;readi&quot; &quot;real&quot; &quot;reali&quot; ## [781] &quot;realli&quot; &quot;reason&quot; &quot;receipt&quot; ## [784] &quot;receiv&quot; &quot;recent&quot; &quot;record&quot; ## [787] &quot;refer&quot; &quot;regard&quot; &quot;regist&quot; ## [790] &quot;relat&quot; &quot;relax&quot; &quot;remain&quot; ## [793] &quot;rememb&quot; &quot;remind&quot; &quot;remov&quot; ## [796] &quot;rent&quot; &quot;rental&quot; &quot;repli&quot; ## [799] &quot;repres&quot; &quot;request&quot; &quot;respond&quot; ## [802] &quot;respons&quot; &quot;rest&quot; &quot;result&quot; ## [805] &quot;return&quot; &quot;reveal&quot; &quot;review&quot; ## [808] &quot;reward&quot; &quot;right&quot; &quot;ring&quot; ## [811] &quot;rington&quot; &quot;rite&quot; &quot;road&quot; ## [814] &quot;rock&quot; &quot;role&quot; &quot;room&quot; ## [817] &quot;roommat&quot; &quot;rose&quot; &quot;round&quot; ## [820] &quot;rowwjhl&quot; &quot;rpli&quot; &quot;rreveal&quot; ## [823] &quot;run&quot; &quot;rush&quot; &quot;sad&quot; ## [826] &quot;sae&quot; &quot;safe&quot; &quot;said&quot; ## [829] &quot;sale&quot; &quot;sat&quot; &quot;saturday&quot; ## [832] &quot;savamob&quot; &quot;save&quot; &quot;saw&quot; ## [835] &quot;say&quot; &quot;sch&quot; &quot;school&quot; ## [838] &quot;scream&quot; &quot;sea&quot; &quot;search&quot; ## [841] &quot;sec&quot; &quot;second&quot; &quot;secret&quot; ## [844] &quot;see&quot; &quot;seem&quot; &quot;seen&quot; ## [847] &quot;select&quot; &quot;self&quot; &quot;sell&quot; ## [850] &quot;semest&quot; &quot;send&quot; &quot;sens&quot; ## [853] &quot;sent&quot; &quot;serious&quot; &quot;servic&quot; ## [856] &quot;set&quot; &quot;settl&quot; &quot;sex&quot; ## [859] &quot;sexi&quot; &quot;shall&quot; &quot;share&quot; ## [862] &quot;shd&quot; &quot;ship&quot; &quot;shirt&quot; ## [865] &quot;shop&quot; &quot;short&quot; &quot;show&quot; ## [868] &quot;shower&quot; &quot;sick&quot; &quot;side&quot; ## [871] &quot;sigh&quot; &quot;sight&quot; &quot;sign&quot; ## [874] &quot;silent&quot; &quot;simpl&quot; &quot;sinc&quot; ## [877] &quot;singl&quot; &quot;sipix&quot; &quot;sir&quot; ## [880] &quot;sis&quot; &quot;sister&quot; &quot;sit&quot; ## [883] &quot;situat&quot; &quot;skxh&quot; &quot;skype&quot; ## [886] &quot;slave&quot; &quot;sleep&quot; &quot;slept&quot; ## [889] &quot;slow&quot; &quot;slowli&quot; &quot;small&quot; ## [892] &quot;smile&quot; &quot;smoke&quot; &quot;sms&quot; ## [895] &quot;smth&quot; &quot;snow&quot; &quot;sofa&quot; ## [898] &quot;sol&quot; &quot;somebodi&quot; &quot;someon&quot; ## [901] &quot;someth&quot; &quot;sometim&quot; &quot;somewher&quot; ## [904] &quot;song&quot; &quot;soni&quot; &quot;sonyericsson&quot; ## [907] &quot;soon&quot; &quot;sorri&quot; &quot;sort&quot; ## [910] &quot;sound&quot; &quot;south&quot; &quot;space&quot; ## [913] &quot;speak&quot; &quot;special&quot; &quot;specialcal&quot; ## [916] &quot;spend&quot; &quot;spent&quot; &quot;spoke&quot; ## [919] &quot;spree&quot; &quot;stand&quot; &quot;start&quot; ## [922] &quot;statement&quot; &quot;station&quot; &quot;stay&quot; ## [925] &quot;std&quot; &quot;step&quot; &quot;still&quot; ## [928] &quot;stockport&quot; &quot;stone&quot; &quot;stop&quot; ## [931] &quot;store&quot; &quot;stori&quot; &quot;street&quot; ## [934] &quot;student&quot; &quot;studi&quot; &quot;stuff&quot; ## [937] &quot;stupid&quot; &quot;style&quot; &quot;sub&quot; ## [940] &quot;subscrib&quot; &quot;success&quot; &quot;suck&quot; ## [943] &quot;suit&quot; &quot;summer&quot; &quot;sun&quot; ## [946] &quot;sunday&quot; &quot;sunshin&quot; &quot;sup&quot; ## [949] &quot;support&quot; &quot;suppos&quot; &quot;sure&quot; ## [952] &quot;surf&quot; &quot;surpris&quot; &quot;sweet&quot; ## [955] &quot;swing&quot; &quot;system&quot; &quot;take&quot; ## [958] &quot;talk&quot; &quot;tampa&quot; &quot;tariff&quot; ## [961] &quot;tcs&quot; &quot;tea&quot; &quot;teach&quot; ## [964] &quot;tear&quot; &quot;teas&quot; &quot;tel&quot; ## [967] &quot;tell&quot; &quot;ten&quot; &quot;tenerif&quot; ## [970] &quot;term&quot; &quot;test&quot; &quot;text&quot; ## [973] &quot;thank&quot; &quot;thanx&quot; &quot;that&quot; ## [976] &quot;thing&quot; &quot;think&quot; &quot;thinkin&quot; ## [979] &quot;thk&quot; &quot;tho&quot; &quot;though&quot; ## [982] &quot;thought&quot; &quot;throw&quot; &quot;thru&quot; ## [985] &quot;tht&quot; &quot;thur&quot; &quot;tick&quot; ## [988] &quot;ticket&quot; &quot;til&quot; &quot;till&quot; ## [991] &quot;time&quot; &quot;tire&quot; &quot;titl&quot; ## [994] &quot;tmr&quot; &quot;toclaim&quot; &quot;today&quot; ## [997] &quot;togeth&quot; &quot;told&quot; &quot;tomo&quot; ## [1000] &quot;tomorrow&quot; &quot;tone&quot; &quot;tonight&quot; ## [1003] &quot;tonit&quot; &quot;took&quot; &quot;top&quot; ## [1006] &quot;torch&quot; &quot;tot&quot; &quot;total&quot; ## [1009] &quot;touch&quot; &quot;tough&quot; &quot;tour&quot; ## [1012] &quot;toward&quot; &quot;town&quot; &quot;track&quot; ## [1015] &quot;train&quot; &quot;transact&quot; &quot;travel&quot; ## [1018] &quot;treat&quot; &quot;tri&quot; &quot;trip&quot; ## [1021] &quot;troubl&quot; &quot;true&quot; &quot;trust&quot; ## [1024] &quot;truth&quot; &quot;tscs&quot; &quot;ttyl&quot; ## [1027] &quot;tuesday&quot; &quot;turn&quot; &quot;twice&quot; ## [1030] &quot;two&quot; &quot;txt&quot; &quot;txting&quot; ## [1033] &quot;txts&quot; &quot;type&quot; &quot;ufind&quot; ## [1036] &quot;ugh&quot; &quot;ull&quot; &quot;uncl&quot; ## [1039] &quot;understand&quot; &quot;unless&quot; &quot;unlimit&quot; ## [1042] &quot;unredeem&quot; &quot;unsub&quot; &quot;unsubscrib&quot; ## [1045] &quot;updat&quot; &quot;ure&quot; &quot;urgent&quot; ## [1048] &quot;urself&quot; &quot;use&quot; &quot;user&quot; ## [1051] &quot;usf&quot; &quot;usual&quot; &quot;uve&quot; ## [1054] &quot;valentin&quot; &quot;valid&quot; &quot;valu&quot; ## [1057] &quot;via&quot; &quot;video&quot; &quot;vikki&quot; ## [1060] &quot;visit&quot; &quot;vodafon&quot; &quot;voic&quot; ## [1063] &quot;vomit&quot; &quot;voucher&quot; &quot;wait&quot; ## [1066] &quot;wake&quot; &quot;walk&quot; &quot;wan&quot; ## [1069] &quot;wana&quot; &quot;wanna&quot; &quot;want&quot; ## [1072] &quot;wap&quot; &quot;warm&quot; &quot;wast&quot; ## [1075] &quot;wat&quot; &quot;watch&quot; &quot;water&quot; ## [1078] &quot;way&quot; &quot;weak&quot; &quot;wear&quot; ## [1081] &quot;weather&quot; &quot;wed&quot; &quot;wednesday&quot; ## [1084] &quot;weed&quot; &quot;week&quot; &quot;weekend&quot; ## [1087] &quot;welcom&quot; &quot;well&quot; &quot;wen&quot; ## [1090] &quot;went&quot; &quot;what&quot; &quot;whatev&quot; ## [1093] &quot;whenev&quot; &quot;whole&quot; &quot;wid&quot; ## [1096] &quot;wif&quot; &quot;wife&quot; &quot;wil&quot; ## [1099] &quot;will&quot; &quot;win&quot; &quot;wine&quot; ## [1102] &quot;winner&quot; &quot;wish&quot; &quot;wit&quot; ## [1105] &quot;within&quot; &quot;without&quot; &quot;wiv&quot; ## [1108] &quot;wkli&quot; &quot;wks&quot; &quot;wnt&quot; ## [1111] &quot;woke&quot; &quot;won&quot; &quot;wonder&quot; ## [1114] &quot;wont&quot; &quot;word&quot; &quot;work&quot; ## [1117] &quot;workin&quot; &quot;world&quot; &quot;worri&quot; ## [1120] &quot;wors&quot; &quot;worth&quot; &quot;wot&quot; ## [1123] &quot;wow&quot; &quot;write&quot; &quot;wrong&quot; ## [1126] &quot;wwq&quot; &quot;wwwgetzedcouk&quot; &quot;xmas&quot; ## [1129] &quot;xxx&quot; &quot;yahoo&quot; &quot;yar&quot; ## [1132] &quot;yeah&quot; &quot;year&quot; &quot;yep&quot; ## [1135] &quot;yes&quot; &quot;yesterday&quot; &quot;yet&quot; ## [1138] &quot;yoga&quot; &quot;yup&quot; save frequently-appearing terms to a character vector sms_freq_words &lt;- findFreqTerms(sms_dtm_train, 5) str(sms_freq_words) ## chr [1:1139] &quot;£wk&quot; &quot;€˜m&quot; &quot;€˜s&quot; &quot;abiola&quot; &quot;abl&quot; &quot;abt&quot; &quot;accept&quot; &quot;access&quot; ... create DTMs with only the frequent terms sms_dtm_freq_train &lt;- sms_dtm_train[ , sms_freq_words] sms_dtm_freq_test &lt;- sms_dtm_test[ , sms_freq_words] convert counts to a factor convert_counts &lt;- function(x) { x &lt;- ifelse(x &gt; 0, &quot;Yes&quot;, &quot;No&quot;) } apply() convert_counts() to columns of train/test data sms_train &lt;- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts) sms_test &lt;- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts) 4.2.2 Step 3: Training a model on the data sms_classifier &lt;- naiveBayes(sms_train, sms_train_labels) 4.2.3 Step 4: Evaluating model performance sms_test_pred &lt;- predict(sms_classifier, sms_test) CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c(&#39;predicted&#39;, &#39;actual&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 1390 ## ## ## | actual ## predicted | ham | spam | Row Total | ## -------------|-----------|-----------|-----------| ## ham | 1201 | 30 | 1231 | ## | 0.995 | 0.164 | | ## -------------|-----------|-----------|-----------| ## spam | 6 | 153 | 159 | ## | 0.005 | 0.836 | | ## -------------|-----------|-----------|-----------| ## Column Total | 1207 | 183 | 1390 | ## | 0.868 | 0.132 | | ## -------------|-----------|-----------|-----------| ## ## 4.2.4 Step 5: Improving model performance sms_classifier2 &lt;- naiveBayes(sms_train, sms_train_labels, laplace = 1) sms_test_pred2 &lt;- predict(sms_classifier2, sms_test) CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c(&#39;predicted&#39;, &#39;actual&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 1390 ## ## ## | actual ## predicted | ham | spam | Row Total | ## -------------|-----------|-----------|-----------| ## ham | 1202 | 28 | 1230 | ## | 0.996 | 0.153 | | ## -------------|-----------|-----------|-----------| ## spam | 5 | 155 | 160 | ## | 0.004 | 0.847 | | ## -------------|-----------|-----------|-----------| ## Column Total | 1207 | 183 | 1390 | ## | 0.868 | 0.132 | | ## -------------|-----------|-----------|-----------| ## ## 4.3 Summary Naive Bayes algorithm constructs tables of probabilities that are used to estimate the likelihood that new examples belong to various classes probabilities are calculated using a formula known as Bayes’ theorem, which specifies how dependent events are related Bayes’ theorem can be computationally expensive simplified version that makes so-called “naive” assumptions about the independence of features is capable of handling extremely large datasets often used for text classification "],
["classification-using-decision-trees-and-rules.html", "Chapter 5 Classification using Decision Trees and Rules 5.1 Part 1: Decision Trees 5.2 Understanding Decision Trees 5.3 Example: Identifying Risky Bank Loans 5.4 Boosting the accuracy of decision trees 5.5 Making some mistakes more costly than others 5.6 Part 2: Rule Learners 5.7 Example: Identifying Poisonous Mushrooms 5.8 Summary", " Chapter 5 Classification using Decision Trees and Rules 5.1 Part 1: Decision Trees Decision tree learners are powerful classifiers, which utilize a tree structure to model the relationships among the features and the potential outcomes Classification rules represent knowledge in the form of logical if-else statements that assign a class to unlabeled examples The antecedent comprises certain combinations of feature values, while the consequent specifies the class value to assign when the rule’s conditions are met 5.2 Understanding Decision Trees calculate entropy of a two-class segment -0.60 * log2(0.60) - 0.40 * log2(0.40) ## [1] 0.9709506 curve(-x * log2(x) - (1 - x) * log2(1 - x), col = &quot;red&quot;, xlab = &quot;x&quot;, ylab = &quot;Entropy&quot;, lwd = 4) 5.3 Example: Identifying Risky Bank Loans 5.3.1 Step 2: Exploring and preparing the data credit &lt;- read.csv(&quot;Chapter 05/credit.csv&quot;) str(credit) ## &#39;data.frame&#39;: 1000 obs. of 17 variables: ## $ checking_balance : Factor w/ 4 levels &quot;&lt; 0 DM&quot;,&quot;&gt; 200 DM&quot;,..: 1 3 4 1 1 4 4 3 4 3 ... ## $ months_loan_duration: int 6 48 12 42 24 36 24 36 12 30 ... ## $ credit_history : Factor w/ 5 levels &quot;critical&quot;,&quot;good&quot;,..: 1 2 1 2 4 2 2 2 2 1 ... ## $ purpose : Factor w/ 6 levels &quot;business&quot;,&quot;car&quot;,..: 5 5 4 5 2 4 5 2 5 2 ... ## $ amount : int 1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ... ## $ savings_balance : Factor w/ 5 levels &quot;&lt; 100 DM&quot;,&quot;&gt; 1000 DM&quot;,..: 5 1 1 1 1 5 4 1 2 1 ... ## $ employment_duration : Factor w/ 5 levels &quot;&lt; 1 year&quot;,&quot;&gt; 7 years&quot;,..: 2 3 4 4 3 3 2 3 4 5 ... ## $ percent_of_income : int 4 2 2 2 3 2 3 2 2 4 ... ## $ years_at_residence : int 4 2 3 4 4 4 4 2 4 2 ... ## $ age : int 67 22 49 45 53 35 53 35 61 28 ... ## $ other_credit : Factor w/ 3 levels &quot;bank&quot;,&quot;none&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ housing : Factor w/ 3 levels &quot;other&quot;,&quot;own&quot;,..: 2 2 2 1 1 1 2 3 2 2 ... ## $ existing_loans_count: int 2 1 1 1 2 1 1 1 1 2 ... ## $ job : Factor w/ 4 levels &quot;management&quot;,&quot;skilled&quot;,..: 2 2 4 2 2 4 2 1 4 1 ... ## $ dependents : int 1 1 2 2 2 2 1 1 1 1 ... ## $ phone : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 1 1 1 2 1 2 1 1 ... ## $ default : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 1 1 2 1 1 1 1 2 ... look at two characteristics of the applicant table(credit$checking_balance) ## ## &lt; 0 DM &gt; 200 DM 1 - 200 DM unknown ## 274 63 269 394 table(credit$savings_balance) ## ## &lt; 100 DM &gt; 1000 DM 100 - 500 DM 500 - 1000 DM unknown ## 603 48 103 63 183 look at two characteristics of the loan summary(credit$months_loan_duration) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.0 12.0 18.0 20.9 24.0 72.0 summary(credit$amount) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 250 1366 2320 3271 3972 18424 look at the class variable table(credit$default) ## ## no yes ## 700 300 create a random sample for training and test data use set.seed to use the same random number sequence as the tutorial set.seed(123) train_sample &lt;- sample(1000, 900) str(train_sample) ## int [1:900] 288 788 409 881 937 46 525 887 548 453 ... split the data frames credit_train &lt;- credit[train_sample, ] credit_test &lt;- credit[-train_sample, ] check the proportion of class variable prop.table(table(credit_train$default)) ## ## no yes ## 0.7033333 0.2966667 prop.table(table(credit_test$default)) ## ## no yes ## 0.67 0.33 5.3.2 Step 3: Training a model on the data build the simplest decision tree credit_model &lt;- C5.0(credit_train[-17], credit_train$default) display simple facts about the tree credit_model ## ## Call: ## C5.0.default(x = credit_train[-17], y = credit_train$default) ## ## Classification Tree ## Number of samples: 900 ## Number of predictors: 16 ## ## Tree size: 57 ## ## Non-standard options: attempt to group attributes display detailed information about the tree summary(credit_model) ## ## Call: ## C5.0.default(x = credit_train[-17], y = credit_train$default) ## ## ## C5.0 [Release 2.07 GPL Edition] Wed Jan 2 16:31:55 2019 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 900 cases (17 attributes) from undefined.data ## ## Decision tree: ## ## checking_balance in {&gt; 200 DM,unknown}: no (412/50) ## checking_balance in {&lt; 0 DM,1 - 200 DM}: ## :...credit_history in {perfect,very good}: yes (59/18) ## credit_history in {critical,good,poor}: ## :...months_loan_duration &lt;= 22: ## :...credit_history = critical: no (72/14) ## : credit_history = poor: ## : :...dependents &gt; 1: no (5) ## : : dependents &lt;= 1: ## : : :...years_at_residence &lt;= 3: yes (4/1) ## : : years_at_residence &gt; 3: no (5/1) ## : credit_history = good: ## : :...savings_balance in {&gt; 1000 DM,500 - 1000 DM}: no (15/1) ## : savings_balance = 100 - 500 DM: ## : :...other_credit = bank: yes (3) ## : : other_credit in {none,store}: no (9/2) ## : savings_balance = unknown: ## : :...other_credit = bank: yes (1) ## : : other_credit in {none,store}: no (21/8) ## : savings_balance = &lt; 100 DM: ## : :...purpose in {business,car0,renovations}: no (8/2) ## : purpose = education: ## : :...checking_balance = &lt; 0 DM: yes (4) ## : : checking_balance = 1 - 200 DM: no (1) ## : purpose = car: ## : :...employment_duration = &gt; 7 years: yes (5) ## : : employment_duration = unemployed: no (4/1) ## : : employment_duration = &lt; 1 year: ## : : :...years_at_residence &lt;= 2: yes (5) ## : : : years_at_residence &gt; 2: no (3/1) ## : : employment_duration = 1 - 4 years: ## : : :...years_at_residence &lt;= 2: yes (2) ## : : : years_at_residence &gt; 2: no (6/1) ## : : employment_duration = 4 - 7 years: ## : : :...amount &lt;= 1680: yes (2) ## : : amount &gt; 1680: no (3) ## : purpose = furniture/appliances: ## : :...job in {management,unskilled}: no (23/3) ## : job = unemployed: yes (1) ## : job = skilled: ## : :...months_loan_duration &gt; 13: [S1] ## : months_loan_duration &lt;= 13: ## : :...housing in {other,own}: no (23/4) ## : housing = rent: ## : :...percent_of_income &lt;= 3: yes (3) ## : percent_of_income &gt; 3: no (2) ## months_loan_duration &gt; 22: ## :...savings_balance = &gt; 1000 DM: no (2) ## savings_balance = 500 - 1000 DM: yes (4/1) ## savings_balance = 100 - 500 DM: ## :...credit_history in {critical,poor}: no (14/3) ## : credit_history = good: ## : :...other_credit = bank: no (1) ## : other_credit in {none,store}: yes (12/2) ## savings_balance = unknown: ## :...checking_balance = 1 - 200 DM: no (17) ## : checking_balance = &lt; 0 DM: ## : :...credit_history = critical: no (1) ## : credit_history in {good,poor}: yes (12/3) ## savings_balance = &lt; 100 DM: ## :...months_loan_duration &gt; 47: yes (21/2) ## months_loan_duration &lt;= 47: ## :...housing = other: ## :...percent_of_income &lt;= 2: no (6) ## : percent_of_income &gt; 2: yes (9/3) ## housing = rent: ## :...other_credit = bank: no (1) ## : other_credit in {none,store}: yes (16/3) ## housing = own: ## :...employment_duration = &gt; 7 years: no (13/4) ## employment_duration = 4 - 7 years: ## :...job in {management,skilled, ## : : unemployed}: yes (9/1) ## : job = unskilled: no (1) ## employment_duration = unemployed: ## :...years_at_residence &lt;= 2: yes (4) ## : years_at_residence &gt; 2: no (3) ## employment_duration = 1 - 4 years: ## :...purpose in {business,car0,education}: yes (7/1) ## : purpose in {furniture/appliances, ## : : renovations}: no (7) ## : purpose = car: ## : :...years_at_residence &lt;= 3: yes (3) ## : years_at_residence &gt; 3: no (3) ## employment_duration = &lt; 1 year: ## :...years_at_residence &gt; 3: yes (5) ## years_at_residence &lt;= 3: ## :...other_credit = bank: no (0) ## other_credit = store: yes (1) ## other_credit = none: ## :...checking_balance = 1 - 200 DM: no (8/2) ## checking_balance = &lt; 0 DM: ## :...job in {management,skilled, ## : unemployed}: yes (2) ## job = unskilled: no (3/1) ## ## SubTree [S1] ## ## employment_duration in {&lt; 1 year,4 - 7 years}: no (4) ## employment_duration in {&gt; 7 years,1 - 4 years,unemployed}: yes (10) ## ## ## Evaluation on training data (900 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 56 133(14.8%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 598 35 (a): class no ## 98 169 (b): class yes ## ## ## Attribute usage: ## ## 100.00% checking_balance ## 54.22% credit_history ## 47.67% months_loan_duration ## 38.11% savings_balance ## 14.33% purpose ## 14.33% housing ## 12.56% employment_duration ## 9.00% job ## 8.67% other_credit ## 6.33% years_at_residence ## 2.22% percent_of_income ## 1.56% dependents ## 0.56% amount ## ## ## Time: 0.0 secs 5.3.3 Step 4: Evaluating model performance create a factor vector of predictions on test data credit_pred &lt;- predict(credit_model, credit_test) cross tabulation of predicted versus actual classes CrossTable(credit_test$default, credit_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&#39;actual default&#39;, &#39;predicted default&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | predicted default ## actual default | no | yes | Row Total | ## ---------------|-----------|-----------|-----------| ## no | 59 | 8 | 67 | ## | 0.590 | 0.080 | | ## ---------------|-----------|-----------|-----------| ## yes | 19 | 14 | 33 | ## | 0.190 | 0.140 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 78 | 22 | 100 | ## ---------------|-----------|-----------|-----------| ## ## 5.3.4 Step 5: Improving model performance 5.4 Boosting the accuracy of decision trees boosted decision tree with 10 trials credit_boost10 &lt;- C5.0(credit_train[-17], credit_train$default, trials = 10) credit_boost10 ## ## Call: ## C5.0.default(x = credit_train[-17], y = credit_train$default, trials = 10) ## ## Classification Tree ## Number of samples: 900 ## Number of predictors: 16 ## ## Number of boosting iterations: 10 ## Average tree size: 47.5 ## ## Non-standard options: attempt to group attributes summary(credit_boost10) ## ## Call: ## C5.0.default(x = credit_train[-17], y = credit_train$default, trials = 10) ## ## ## C5.0 [Release 2.07 GPL Edition] Wed Jan 2 16:31:56 2019 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 900 cases (17 attributes) from undefined.data ## ## ----- Trial 0: ----- ## ## Decision tree: ## ## checking_balance in {&gt; 200 DM,unknown}: no (412/50) ## checking_balance in {&lt; 0 DM,1 - 200 DM}: ## :...credit_history in {perfect,very good}: yes (59/18) ## credit_history in {critical,good,poor}: ## :...months_loan_duration &lt;= 22: ## :...credit_history = critical: no (72/14) ## : credit_history = poor: ## : :...dependents &gt; 1: no (5) ## : : dependents &lt;= 1: ## : : :...years_at_residence &lt;= 3: yes (4/1) ## : : years_at_residence &gt; 3: no (5/1) ## : credit_history = good: ## : :...savings_balance in {&gt; 1000 DM,500 - 1000 DM}: no (15/1) ## : savings_balance = 100 - 500 DM: ## : :...other_credit = bank: yes (3) ## : : other_credit in {none,store}: no (9/2) ## : savings_balance = unknown: ## : :...other_credit = bank: yes (1) ## : : other_credit in {none,store}: no (21/8) ## : savings_balance = &lt; 100 DM: ## : :...purpose in {business,car0,renovations}: no (8/2) ## : purpose = education: ## : :...checking_balance = &lt; 0 DM: yes (4) ## : : checking_balance = 1 - 200 DM: no (1) ## : purpose = car: ## : :...employment_duration = &gt; 7 years: yes (5) ## : : employment_duration = unemployed: no (4/1) ## : : employment_duration = &lt; 1 year: ## : : :...years_at_residence &lt;= 2: yes (5) ## : : : years_at_residence &gt; 2: no (3/1) ## : : employment_duration = 1 - 4 years: ## : : :...years_at_residence &lt;= 2: yes (2) ## : : : years_at_residence &gt; 2: no (6/1) ## : : employment_duration = 4 - 7 years: ## : : :...amount &lt;= 1680: yes (2) ## : : amount &gt; 1680: no (3) ## : purpose = furniture/appliances: ## : :...job in {management,unskilled}: no (23/3) ## : job = unemployed: yes (1) ## : job = skilled: ## : :...months_loan_duration &gt; 13: [S1] ## : months_loan_duration &lt;= 13: ## : :...housing in {other,own}: no (23/4) ## : housing = rent: ## : :...percent_of_income &lt;= 3: yes (3) ## : percent_of_income &gt; 3: no (2) ## months_loan_duration &gt; 22: ## :...savings_balance = &gt; 1000 DM: no (2) ## savings_balance = 500 - 1000 DM: yes (4/1) ## savings_balance = 100 - 500 DM: ## :...credit_history in {critical,poor}: no (14/3) ## : credit_history = good: ## : :...other_credit = bank: no (1) ## : other_credit in {none,store}: yes (12/2) ## savings_balance = unknown: ## :...checking_balance = 1 - 200 DM: no (17) ## : checking_balance = &lt; 0 DM: ## : :...credit_history = critical: no (1) ## : credit_history in {good,poor}: yes (12/3) ## savings_balance = &lt; 100 DM: ## :...months_loan_duration &gt; 47: yes (21/2) ## months_loan_duration &lt;= 47: ## :...housing = other: ## :...percent_of_income &lt;= 2: no (6) ## : percent_of_income &gt; 2: yes (9/3) ## housing = rent: ## :...other_credit = bank: no (1) ## : other_credit in {none,store}: yes (16/3) ## housing = own: ## :...employment_duration = &gt; 7 years: no (13/4) ## employment_duration = 4 - 7 years: ## :...job in {management,skilled, ## : : unemployed}: yes (9/1) ## : job = unskilled: no (1) ## employment_duration = unemployed: ## :...years_at_residence &lt;= 2: yes (4) ## : years_at_residence &gt; 2: no (3) ## employment_duration = 1 - 4 years: ## :...purpose in {business,car0,education}: yes (7/1) ## : purpose in {furniture/appliances, ## : : renovations}: no (7) ## : purpose = car: ## : :...years_at_residence &lt;= 3: yes (3) ## : years_at_residence &gt; 3: no (3) ## employment_duration = &lt; 1 year: ## :...years_at_residence &gt; 3: yes (5) ## years_at_residence &lt;= 3: ## :...other_credit = bank: no (0) ## other_credit = store: yes (1) ## other_credit = none: ## :...checking_balance = 1 - 200 DM: no (8/2) ## checking_balance = &lt; 0 DM: ## :...job in {management,skilled, ## : unemployed}: yes (2) ## job = unskilled: no (3/1) ## ## SubTree [S1] ## ## employment_duration in {&lt; 1 year,4 - 7 years}: no (4) ## employment_duration in {&gt; 7 years,1 - 4 years,unemployed}: yes (10) ## ## ----- Trial 1: ----- ## ## Decision tree: ## ## checking_balance = unknown: ## :...other_credit in {bank,store}: ## : :...purpose in {business,education,renovations}: yes (19.5/6.3) ## : : purpose in {car0,furniture/appliances}: no (24.8/6.6) ## : : purpose = car: ## : : :...dependents &lt;= 1: yes (20.1/4.8) ## : : dependents &gt; 1: no (2.4) ## : other_credit = none: ## : :...credit_history in {critical,perfect,very good}: no (102.8/4.4) ## : credit_history = good: ## : :...existing_loans_count &lt;= 1: no (112.7/17.5) ## : : existing_loans_count &gt; 1: yes (18.9/7.9) ## : credit_history = poor: ## : :...years_at_residence &lt;= 1: yes (4.4) ## : years_at_residence &gt; 1: ## : :...percent_of_income &lt;= 3: no (11.9) ## : percent_of_income &gt; 3: yes (14.3/5.6) ## checking_balance in {&lt; 0 DM,&gt; 200 DM,1 - 200 DM}: ## :...savings_balance in {&gt; 1000 DM,500 - 1000 DM}: no (42.9/11.3) ## savings_balance = unknown: ## :...credit_history in {perfect,poor}: no (8.5) ## : credit_history in {critical,good,very good}: ## : :...employment_duration in {&lt; 1 year,&gt; 7 years,4 - 7 years, ## : : unemployed}: no (52.3/17.3) ## : employment_duration = 1 - 4 years: yes (19.7/5.6) ## savings_balance = 100 - 500 DM: ## :...existing_loans_count &gt; 3: yes (3) ## : existing_loans_count &lt;= 3: ## : :...credit_history in {critical,poor,very good}: no (24.6/7.6) ## : credit_history = perfect: yes (2.4) ## : credit_history = good: ## : :...months_loan_duration &lt;= 27: no (23.7/10.5) ## : months_loan_duration &gt; 27: yes (5.6) ## savings_balance = &lt; 100 DM: ## :...months_loan_duration &gt; 42: yes (28/5.2) ## months_loan_duration &lt;= 42: ## :...percent_of_income &lt;= 2: ## :...employment_duration in {1 - 4 years,4 - 7 years, ## : : unemployed}: no (86.2/23.8) ## : employment_duration in {&lt; 1 year,&gt; 7 years}: ## : :...housing = other: no (4.8/1.6) ## : housing = rent: yes (10.7/2.4) ## : housing = own: ## : :...phone = yes: yes (12.9/4) ## : phone = no: ## : :...percent_of_income &lt;= 1: no (7.1/0.8) ## : percent_of_income &gt; 1: yes (17.5/7.1) ## percent_of_income &gt; 2: ## :...years_at_residence &lt;= 1: no (31.6/8.5) ## years_at_residence &gt; 1: ## :...credit_history in {perfect,poor}: yes (20.9/1.6) ## credit_history in {critical,good,very good}: ## :...job = skilled: yes (95/34.7) ## job = unemployed: no (1.6) ## job = management: ## :...amount &lt;= 11590: no (23.8/7) ## : amount &gt; 11590: yes (3.8) ## job = unskilled: ## :...checking_balance in {&lt; 0 DM, ## : &gt; 200 DM}: yes (23.8/9.5) ## checking_balance = 1 - 200 DM: no (17.9/6.2) ## ## ----- Trial 2: ----- ## ## Decision tree: ## ## checking_balance = unknown: ## :...other_credit = bank: ## : :...existing_loans_count &gt; 2: no (3.3) ## : : existing_loans_count &lt;= 2: ## : : :...months_loan_duration &lt;= 8: no (4) ## : : months_loan_duration &gt; 8: yes (43/16.6) ## : other_credit in {none,store}: ## : :...employment_duration in {&lt; 1 year,unemployed}: ## : :...purpose in {business,renovations}: yes (6.4) ## : : purpose in {car,car0,education}: no (13.2) ## : : purpose = furniture/appliances: ## : : :...amount &lt;= 4594: no (22.5/7.3) ## : : amount &gt; 4594: yes (9.1) ## : employment_duration in {&gt; 7 years,1 - 4 years,4 - 7 years}: ## : :...percent_of_income &lt;= 3: no (92.7/3.6) ## : percent_of_income &gt; 3: ## : :...age &gt; 30: no (73.6/5.5) ## : age &lt;= 30: ## : :...job in {management,unemployed,unskilled}: yes (14/4) ## : job = skilled: ## : :...credit_history = very good: no (0) ## : credit_history = poor: yes (3.6) ## : credit_history in {critical,good,perfect}: ## : :...age &lt;= 29: no (20.4/4.6) ## : age &gt; 29: yes (2.7) ## checking_balance in {&lt; 0 DM,&gt; 200 DM,1 - 200 DM}: ## :...housing = other: ## :...dependents &gt; 1: yes (28.3/7.6) ## : dependents &lt;= 1: ## : :...employment_duration in {&lt; 1 year,4 - 7 years, ## : : unemployed}: no (22.9/4.5) ## : employment_duration in {&gt; 7 years,1 - 4 years}: yes (29.6/10.5) ## housing = rent: ## :...credit_history = perfect: yes (5.3) ## : credit_history = poor: no (7.1/0.7) ## : credit_history in {critical,good,very good}: ## : :...employment_duration = &lt; 1 year: yes (28.3/9.3) ## : employment_duration in {&gt; 7 years,4 - 7 years, ## : : unemployed}: no (33.9/12.3) ## : employment_duration = 1 - 4 years: ## : :...checking_balance = &gt; 200 DM: no (2) ## : checking_balance in {&lt; 0 DM,1 - 200 DM}: ## : :...years_at_residence &lt;= 3: no (10.3/3.8) ## : years_at_residence &gt; 3: yes (20.4/3.1) ## housing = own: ## :...job in {management,unemployed}: yes (55.8/19.8) ## job in {skilled,unskilled}: ## :...months_loan_duration &lt;= 7: no (25.3/2) ## months_loan_duration &gt; 7: ## :...years_at_residence &gt; 3: no (92.2/29.6) ## years_at_residence &lt;= 3: ## :...purpose = renovations: yes (7/1.3) ## purpose in {business,car0,education}: no (32.2/5.3) ## purpose = car: ## :...months_loan_duration &gt; 40: no (7.2/0.7) ## : months_loan_duration &lt;= 40: ## : :...amount &lt;= 947: yes (12.9) ## : amount &gt; 947: ## : :...months_loan_duration &lt;= 16: no (23.2/8.5) ## : months_loan_duration &gt; 16: [S1] ## purpose = furniture/appliances: ## :...savings_balance in {&gt; 1000 DM,unknown}: no (15.4/3.2) ## savings_balance in {100 - 500 DM, ## : 500 - 1000 DM}: yes (14.6/4.5) ## savings_balance = &lt; 100 DM: ## :...months_loan_duration &gt; 36: yes (7.1) ## months_loan_duration &lt;= 36: ## :...existing_loans_count &gt; 1: no (14.1/4.3) ## existing_loans_count &lt;= 1: [S2] ## ## SubTree [S1] ## ## savings_balance in {&lt; 100 DM,&gt; 1000 DM,500 - 1000 DM,unknown}: yes (22.5/2.7) ## savings_balance = 100 - 500 DM: no (4.5/0.7) ## ## SubTree [S2] ## ## checking_balance = &lt; 0 DM: no (22.4/9.1) ## checking_balance in {&gt; 200 DM,1 - 200 DM}: yes (46.7/20) ## ## ----- Trial 3: ----- ## ## Decision tree: ## ## checking_balance in {&gt; 200 DM,unknown}: ## :...employment_duration = &gt; 7 years: no (98.9/17.1) ## : employment_duration = unemployed: yes (16/6.7) ## : employment_duration = &lt; 1 year: ## : :...amount &lt;= 1333: no (11.7) ## : : amount &gt; 1333: ## : : :...amount &lt;= 6681: no (38.2/16.3) ## : : amount &gt; 6681: yes (5.3) ## : employment_duration = 4 - 7 years: ## : :...checking_balance = &gt; 200 DM: yes (9.6/3.6) ## : : checking_balance = unknown: ## : : :...age &lt;= 22: yes (6.5/1.6) ## : : age &gt; 22: no (42.6/1.5) ## : employment_duration = 1 - 4 years: ## : :...percent_of_income &lt;= 1: no (20.6/1.5) ## : percent_of_income &gt; 1: ## : :...job in {skilled,unemployed}: no (64.9/17.6) ## : job in {management,unskilled}: ## : :...existing_loans_count &gt; 2: yes (2.4) ## : existing_loans_count &lt;= 2: ## : :...age &lt;= 34: yes (26.4/10.7) ## : age &gt; 34: no (10.5) ## checking_balance in {&lt; 0 DM,1 - 200 DM}: ## :...savings_balance in {&gt; 1000 DM,500 - 1000 DM}: no (35.8/12) ## savings_balance = 100 - 500 DM: ## :...amount &lt;= 1285: yes (12.8/0.5) ## : amount &gt; 1285: ## : :...existing_loans_count &lt;= 1: no (27/9.2) ## : existing_loans_count &gt; 1: yes (15.8/4.9) ## savings_balance = unknown: ## :...credit_history in {critical,perfect,poor}: no (15.5) ## : credit_history in {good,very good}: ## : :...age &gt; 56: no (4.5) ## : age &lt;= 56: ## : :...months_loan_duration &lt;= 18: yes (24.5/5.6) ## : months_loan_duration &gt; 18: no (28.4/12.3) ## savings_balance = &lt; 100 DM: ## :...months_loan_duration &lt;= 11: ## :...job = management: yes (13.7/4.9) ## : job in {skilled,unemployed,unskilled}: no (45.9/10) ## months_loan_duration &gt; 11: ## :...percent_of_income &lt;= 1: ## :...credit_history in {critical,poor,very good}: no (11.1) ## : credit_history in {good,perfect}: yes (24.4/11) ## percent_of_income &gt; 1: ## :...job = unemployed: yes (7/3.1) ## job = management: ## :...years_at_residence &lt;= 1: no (6.6) ## : years_at_residence &gt; 1: ## : :...checking_balance = &lt; 0 DM: no (23.1/7) ## : checking_balance = 1 - 200 DM: yes (15.8/4) ## job = unskilled: ## :...housing in {other,rent}: yes (12.2/2.2) ## : housing = own: ## : :...purpose = car: yes (18.1/3.9) ## : purpose in {business,car0,education, ## : furniture/appliances, ## : renovations}: no (32.1/11.1) ## job = skilled: ## :...checking_balance = &lt; 0 DM: ## :...credit_history in {poor,very good}: yes (16.6) ## : credit_history in {critical,good,perfect}: ## : :...purpose in {business,car0,education, ## : : renovations}: yes (10.2/1.5) ## : purpose = car: ## : :...age &lt;= 51: yes (34.6/8.1) ## : : age &gt; 51: no (4.4) ## : purpose = furniture/appliances: ## : :...years_at_residence &lt;= 1: no (4.4) ## : years_at_residence &gt; 1: ## : :...other_credit = bank: yes (2.4) ## : other_credit = store: no (0.5) ## : other_credit = none: ## : :...amount &lt;= 1743: no (11.5/2.4) ## : amount &gt; 1743: yes (29/6.6) ## checking_balance = 1 - 200 DM: ## :...months_loan_duration &gt; 36: yes (6.5) ## months_loan_duration &lt;= 36: ## :...other_credit in {bank,store}: yes (8/1.5) ## other_credit = none: ## :...dependents &gt; 1: yes (7.4/3.1) ## dependents &lt;= 1: ## :...percent_of_income &lt;= 2: no (12.7/1.1) ## percent_of_income &gt; 2: [S1] ## ## SubTree [S1] ## ## purpose in {business,renovations}: yes (3.9) ## purpose in {car,car0,education,furniture/appliances}: no (19.8/6.1) ## ## ----- Trial 4: ----- ## ## Decision tree: ## ## checking_balance in {&gt; 200 DM,unknown}: ## :...other_credit = store: no (20.6/9.6) ## : other_credit = none: ## : :...employment_duration in {&gt; 7 years,1 - 4 years,4 - 7 years, ## : : : unemployed}: no (211.3/45.7) ## : : employment_duration = &lt; 1 year: ## : : :...amount &lt;= 1333: no (8.8) ## : : amount &gt; 1333: ## : : :...purpose in {business,car0,education,furniture/appliances, ## : : : renovations}: yes (32.9/8.1) ## : : purpose = car: no (4.9) ## : other_credit = bank: ## : :...age &gt; 44: no (14.4/1.2) ## : age &lt;= 44: ## : :...years_at_residence &lt;= 1: no (5) ## : years_at_residence &gt; 1: ## : :...housing = rent: yes (4.3) ## : housing in {other,own}: ## : :...job = unemployed: yes (0) ## : job = management: no (4) ## : job in {skilled,unskilled}: ## : :...age &lt;= 26: no (3.7) ## : age &gt; 26: ## : :...savings_balance in {&lt; 100 DM,500 - 1000 DM, ## : : unknown}: yes (30.6/7.4) ## : savings_balance in {&gt; 1000 DM, ## : 100 - 500 DM}: no (4) ## checking_balance in {&lt; 0 DM,1 - 200 DM}: ## :...credit_history = perfect: ## :...housing in {other,rent}: yes (7.8) ## : housing = own: no (20.5/9) ## credit_history = poor: ## :...checking_balance = &lt; 0 DM: yes (10.4/2.2) ## : checking_balance = 1 - 200 DM: ## : :...other_credit in {bank,none}: no (24/4.3) ## : other_credit = store: yes (5.8/1.2) ## credit_history = very good: ## :...age &lt;= 23: no (5.7) ## : age &gt; 23: ## : :...months_loan_duration &lt;= 27: yes (28.4/3.7) ## : months_loan_duration &gt; 27: no (6.9/2) ## credit_history = critical: ## :...years_at_residence &lt;= 1: no (6.7) ## : years_at_residence &gt; 1: ## : :...purpose in {business,car,car0,renovations}: no (62.2/21.9) ## : purpose = education: yes (7.9/0.9) ## : purpose = furniture/appliances: ## : :...phone = yes: no (14.5/2.8) ## : phone = no: ## : :...amount &lt;= 1175: no (5.2) ## : amount &gt; 1175: yes (30.1/7.6) ## credit_history = good: ## :...savings_balance in {&gt; 1000 DM,500 - 1000 DM}: no (15.7/4.7) ## savings_balance = 100 - 500 DM: yes (32.1/11.7) ## savings_balance = unknown: ## :...job = unskilled: no (4.4) ## : job in {management,skilled,unemployed}: ## : :...checking_balance = &lt; 0 DM: yes (27.8/6) ## : checking_balance = 1 - 200 DM: no (26.8/10.4) ## savings_balance = &lt; 100 DM: ## :...dependents &gt; 1: ## :...existing_loans_count &gt; 1: no (2.6/0.4) ## : existing_loans_count &lt;= 1: ## : :...years_at_residence &lt;= 2: yes (10.2/2.9) ## : years_at_residence &gt; 2: no (20.4/5.9) ## dependents &lt;= 1: ## :...purpose in {business,car0}: no (9.7/2.5) ## purpose in {education,renovations}: yes (13/5.1) ## purpose = car: ## :...employment_duration in {&lt; 1 year,&gt; 7 years, ## : : 4 - 7 years}: yes (32/8.3) ## : employment_duration in {1 - 4 years, ## : unemployed}: no (24.9/9) ## purpose = furniture/appliances: ## :...months_loan_duration &gt; 39: yes (4.8) ## months_loan_duration &lt;= 39: ## :...phone = yes: yes (21.9/9.2) ## phone = no: ## :...employment_duration in {&lt; 1 year,&gt; 7 years, ## : 4 - 7 years}: no (34.1/8.1) ## employment_duration = unemployed: yes (3.3/0.4) ## employment_duration = 1 - 4 years: ## :...percent_of_income &lt;= 1: yes (3.8) ## percent_of_income &gt; 1: ## :...months_loan_duration &gt; 21: no (4.9/0.4) ## months_loan_duration &lt;= 21: ## :...years_at_residence &lt;= 3: no (20.9/8.8) ## years_at_residence &gt; 3: yes (5.8) ## ## ----- Trial 5: ----- ## ## Decision tree: ## ## checking_balance = unknown: ## :...other_credit = store: yes (16.9/7.5) ## : other_credit = bank: ## : :...housing = other: no (8.3/1.8) ## : : housing = rent: yes (4.4/0.8) ## : : housing = own: ## : : :...phone = no: no (26.9/9.7) ## : : phone = yes: yes (12.1/5) ## : other_credit = none: ## : :...credit_history in {critical,perfect,very good}: no (60.4/5.1) ## : credit_history in {good,poor}: ## : :...purpose in {business,car,car0,education}: no (53.6/12.8) ## : purpose = renovations: yes (7.3/1.1) ## : purpose = furniture/appliances: ## : :...job = unemployed: no (0) ## : job in {management,unskilled}: yes (19.2/7) ## : job = skilled: ## : :...phone = yes: no (14.6/1.8) ## : phone = no: ## : :...age &gt; 32: no (9.2) ## : age &lt;= 32: ## : :...employment_duration = 1 - 4 years: no (4.1) ## : employment_duration in {&lt; 1 year,&gt; 7 years, ## : : 4 - 7 years,unemployed}: ## : :...savings_balance in {&lt; 100 DM, ## : : 100 - 500 DM}: yes (20.5/3) ## : savings_balance in {&gt; 1000 DM,500 - 1000 DM, ## : unknown}: no (3.4) ## checking_balance in {&lt; 0 DM,&gt; 200 DM,1 - 200 DM}: ## :...percent_of_income &lt;= 2: ## :...amount &gt; 11054: yes (14.2/1.2) ## : amount &lt;= 11054: ## : :...other_credit = bank: no (32.3/9.7) ## : other_credit = store: yes (8.9/2.6) ## : other_credit = none: ## : :...purpose in {business,renovations}: yes (20.3/9.1) ## : purpose in {car0,education}: no (8.4/3.7) ## : purpose = car: ## : :...savings_balance in {&lt; 100 DM,&gt; 1000 DM,500 - 1000 DM, ## : : : unknown}: no (46.6/7.9) ## : : savings_balance = 100 - 500 DM: yes (13.8/3.3) ## : purpose = furniture/appliances: ## : :...employment_duration in {&gt; 7 years, ## : : 4 - 7 years}: no (18.2/2.6) ## : employment_duration in {1 - 4 years, ## : : unemployed}: yes (50.8/19.5) ## : employment_duration = &lt; 1 year: ## : :...job in {management,skilled,unemployed}: no (16.3/2.9) ## : job = unskilled: yes (6/1.6) ## percent_of_income &gt; 2: ## :...years_at_residence &lt;= 1: ## :...other_credit in {bank,store}: no (7.6) ## : other_credit = none: ## : :...months_loan_duration &gt; 42: no (2.9) ## : months_loan_duration &lt;= 42: ## : :...age &lt;= 36: no (26.6/8.4) ## : age &gt; 36: yes (5.3) ## years_at_residence &gt; 1: ## :...job = unemployed: no (5.2) ## job in {management,skilled,unskilled}: ## :...credit_history = perfect: yes (10.9) ## credit_history in {critical,good,poor,very good}: ## :...employment_duration = &lt; 1 year: ## :...checking_balance = &gt; 200 DM: no (2.7) ## : checking_balance in {&lt; 0 DM,1 - 200 DM}: ## : :...months_loan_duration &gt; 21: yes (23.4/0.7) ## : months_loan_duration &lt;= 21: ## : :...amount &lt;= 1928: yes (18.4/4.4) ## : amount &gt; 1928: no (4.5) ## employment_duration in {&gt; 7 years,1 - 4 years,4 - 7 years, ## : unemployed}: ## :...months_loan_duration &lt;= 11: ## :...age &gt; 47: no (12.2) ## : age &lt;= 47: ## : :...purpose in {business,car,car0, ## : : furniture/appliances, ## : : renovations}: no (25/9.2) ## : purpose = education: yes (3.5) ## months_loan_duration &gt; 11: ## :...savings_balance in {&gt; 1000 DM,100 - 500 DM}: ## :...age &lt;= 58: no (22.7/3.4) ## : age &gt; 58: yes (4.4) ## savings_balance in {&lt; 100 DM,500 - 1000 DM,unknown}: ## :...years_at_residence &lt;= 2: yes (76.1/22.8) ## years_at_residence &gt; 2: ## :...purpose in {business,car0, ## : education}: yes (24.7/7.1) ## purpose = renovations: no (1.1) ## purpose = furniture/appliances: [S1] ## purpose = car: ## :...amount &lt;= 1388: yes (17.8/2.2) ## amount &gt; 1388: ## :...housing = own: no (10.9) ## housing in {other,rent}: [S2] ## ## SubTree [S1] ## ## employment_duration = unemployed: no (4.4) ## employment_duration in {&gt; 7 years,1 - 4 years,4 - 7 years}: ## :...checking_balance = &lt; 0 DM: yes (35.6/12.4) ## checking_balance in {&gt; 200 DM,1 - 200 DM}: no (29/10.5) ## ## SubTree [S2] ## ## savings_balance in {&lt; 100 DM,500 - 1000 DM}: yes (21.4/6.4) ## savings_balance = unknown: no (6.8/1.5) ## ## ----- Trial 6: ----- ## ## Decision tree: ## ## checking_balance in {&gt; 200 DM,unknown}: ## :...purpose = car0: no (2.2) ## : purpose = renovations: yes (8.4/3.3) ## : purpose = education: ## : :...age &lt;= 44: yes (19.8/7.7) ## : : age &gt; 44: no (4.4) ## : purpose = business: ## : :...existing_loans_count &gt; 2: yes (3.3) ## : : existing_loans_count &lt;= 2: ## : : :...amount &lt;= 1823: no (8.1) ## : : amount &gt; 1823: ## : : :...percent_of_income &lt;= 3: no (12.1/3.3) ## : : percent_of_income &gt; 3: yes (13.2/3.4) ## : purpose = car: ## : :...job in {management,unemployed}: no (20.8/1.6) ## : : job = unskilled: ## : : :...years_at_residence &lt;= 3: no (11/1.3) ## : : : years_at_residence &gt; 3: yes (14.5/3.2) ## : : job = skilled: ## : : :...other_credit in {bank,store}: yes (17.6/4.9) ## : : other_credit = none: ## : : :...existing_loans_count &lt;= 2: no (24.6) ## : : existing_loans_count &gt; 2: yes (2.4/0.3) ## : purpose = furniture/appliances: ## : :...age &gt; 44: no (22.7) ## : age &lt;= 44: ## : :...job = unemployed: no (0) ## : job = unskilled: ## : :...existing_loans_count &lt;= 1: yes (20.9/5.6) ## : : existing_loans_count &gt; 1: no (4.5) ## : job in {management,skilled}: ## : :...dependents &gt; 1: no (6.6) ## : dependents &lt;= 1: ## : :...existing_loans_count &lt;= 1: ## : :...savings_balance in {&gt; 1000 DM,100 - 500 DM, ## : : : 500 - 1000 DM, ## : : : unknown}: no (16.9) ## : : savings_balance = &lt; 100 DM: ## : : :...age &lt;= 22: yes (8.5/1.3) ## : : age &gt; 22: no (43.1/8.8) ## : existing_loans_count &gt; 1: ## : :...housing in {other,rent}: yes (9.9/2.1) ## : housing = own: ## : :...credit_history in {critical,poor, ## : : very good}: no (18.6/1.6) ## : credit_history in {good,perfect}: yes (14.9/4.3) ## checking_balance in {&lt; 0 DM,1 - 200 DM}: ## :...credit_history = perfect: yes (28.1/9.6) ## credit_history = very good: ## :...age &lt;= 23: no (5.5) ## : age &gt; 23: yes (30/8.1) ## credit_history = poor: ## :...percent_of_income &lt;= 1: no (6.5) ## : percent_of_income &gt; 1: ## : :...savings_balance in {500 - 1000 DM,unknown}: no (6.4) ## : savings_balance in {&lt; 100 DM,&gt; 1000 DM,100 - 500 DM}: ## : :...dependents &lt;= 1: yes (25.1/8) ## : dependents &gt; 1: no (5/0.9) ## credit_history = critical: ## :...savings_balance = unknown: no (8.4) ## : savings_balance in {&lt; 100 DM,&gt; 1000 DM,100 - 500 DM,500 - 1000 DM}: ## : :...other_credit = bank: yes (16.2/4.3) ## : other_credit = store: no (3.7/0.9) ## : other_credit = none: ## : :...savings_balance in {&gt; 1000 DM,500 - 1000 DM}: yes (7.3/2.3) ## : savings_balance = 100 - 500 DM: no (5.9) ## : savings_balance = &lt; 100 DM: ## : :...purpose = business: no (4.5/2.2) ## : purpose in {car0,education,renovations}: yes (8.5/2.2) ## : purpose = car: ## : :...age &lt;= 29: yes (6.9) ## : : age &gt; 29: no (25.6/6.9) ## : purpose = furniture/appliances: ## : :...months_loan_duration &lt;= 36: no (38.4/10.9) ## : months_loan_duration &gt; 36: yes (3.8) ## credit_history = good: ## :...amount &gt; 8086: yes (24/3.8) ## amount &lt;= 8086: ## :...phone = yes: ## :...age &lt;= 28: yes (23.9/7.5) ## : age &gt; 28: no (69.4/17.9) ## phone = no: ## :...other_credit in {bank,store}: yes (25.1/7.2) ## other_credit = none: ## :...percent_of_income &lt;= 2: ## :...job in {management,unemployed,unskilled}: no (15.6/2.7) ## : job = skilled: ## : :...amount &lt;= 1386: yes (9.9/1) ## : amount &gt; 1386: ## : :...age &lt;= 24: yes (13.4/4.6) ## : age &gt; 24: no (27.8/3.1) ## percent_of_income &gt; 2: ## :...checking_balance = &lt; 0 DM: yes (62.5/21.4) ## checking_balance = 1 - 200 DM: ## :...months_loan_duration &gt; 42: yes (4.9) ## months_loan_duration &lt;= 42: ## :...existing_loans_count &gt; 1: no (5) ## existing_loans_count &lt;= 1: ## :...age &lt;= 35: no (39.4/13.2) ## age &gt; 35: yes (14.7/4.2) ## ## ----- Trial 7: ----- ## ## Decision tree: ## ## checking_balance = unknown: ## :...employment_duration in {&gt; 7 years,4 - 7 years}: no (101.1/20.4) ## : employment_duration = unemployed: yes (16.6/8) ## : employment_duration = &lt; 1 year: ## : :...amount &lt;= 4594: no (30/5.7) ## : : amount &gt; 4594: yes (10.6/0.3) ## : employment_duration = 1 - 4 years: ## : :...dependents &gt; 1: no (8) ## : dependents &lt;= 1: ## : :...months_loan_duration &lt;= 16: no (32.8/5.3) ## : months_loan_duration &gt; 16: ## : :...existing_loans_count &gt; 2: yes (2.7) ## : existing_loans_count &lt;= 2: ## : :...percent_of_income &lt;= 3: no (20.9/5.9) ## : percent_of_income &gt; 3: ## : :...purpose in {business,car0,education}: yes (10.8) ## : purpose in {car,furniture/appliances, ## : renovations}: no (19.7/7.5) ## checking_balance in {&lt; 0 DM,&gt; 200 DM,1 - 200 DM}: ## :...purpose in {car0,education,renovations}: no (67.2/29.2) ## purpose = business: ## :...age &gt; 46: yes (5.2) ## : age &lt;= 46: ## : :...months_loan_duration &lt;= 18: no (17.5) ## : months_loan_duration &gt; 18: ## : :...other_credit in {bank,store}: no (10/0.5) ## : other_credit = none: ## : :...employment_duration in {&gt; 7 years, ## : : unemployed}: yes (6.6) ## : employment_duration in {&lt; 1 year,1 - 4 years,4 - 7 years}: ## : :...age &lt;= 25: yes (4) ## : age &gt; 25: no (19.2/5.6) ## purpose = car: ## :...amount &lt;= 1297: yes (52.4/12.9) ## : amount &gt; 1297: ## : :...percent_of_income &lt;= 2: ## : :...phone = no: no (32.7/6.1) ## : : phone = yes: ## : : :...years_at_residence &lt;= 3: no (20/4.9) ## : : years_at_residence &gt; 3: yes (14.7/3.8) ## : percent_of_income &gt; 2: ## : :...percent_of_income &lt;= 3: yes (33.1/11.3) ## : percent_of_income &gt; 3: ## : :...months_loan_duration &lt;= 18: no (18.2/1.6) ## : months_loan_duration &gt; 18: ## : :...existing_loans_count &lt;= 1: no (19.5/7.2) ## : existing_loans_count &gt; 1: yes (13.8/1) ## purpose = furniture/appliances: ## :...savings_balance = &gt; 1000 DM: no (5.2) ## savings_balance = 100 - 500 DM: yes (18.6/6) ## savings_balance in {&lt; 100 DM,500 - 1000 DM,unknown}: ## :...existing_loans_count &gt; 1: ## :...existing_loans_count &gt; 2: no (3.6) ## : existing_loans_count &lt;= 2: ## : :...housing = other: yes (3.3) ## : housing in {own,rent}: ## : :...savings_balance = 500 - 1000 DM: yes (3.5/1) ## : savings_balance = unknown: no (6.9) ## : savings_balance = &lt; 100 DM: ## : :...age &gt; 54: yes (2.1) ## : age &lt;= 54: [S1] ## existing_loans_count &lt;= 1: ## :...credit_history in {critical,perfect}: yes (20.3/7.6) ## credit_history in {poor,very good}: no (20.8/9.5) ## credit_history = good: ## :...months_loan_duration &lt;= 7: no (11.4) ## months_loan_duration &gt; 7: ## :...other_credit = bank: no (14.2/4.6) ## other_credit = store: yes (11.7/3.9) ## other_credit = none: ## :...percent_of_income &lt;= 1: no (20.5/5.2) ## percent_of_income &gt; 1: ## :...amount &gt; 6078: yes (10.9/1.1) ## amount &lt;= 6078: ## :...dependents &gt; 1: yes (8.7/2.5) ## dependents &lt;= 1: [S2] ## ## SubTree [S1] ## ## employment_duration in {&lt; 1 year,4 - 7 years}: yes (15/2.5) ## employment_duration in {&gt; 7 years,1 - 4 years,unemployed}: no (25.7/2.9) ## ## SubTree [S2] ## ## employment_duration = &gt; 7 years: no (17.9/2.5) ## employment_duration in {&lt; 1 year,1 - 4 years,4 - 7 years,unemployed}: ## :...job = management: no (6.6) ## job = unemployed: yes (1.1) ## job in {skilled,unskilled}: ## :...years_at_residence &lt;= 1: no (11.8/1.8) ## years_at_residence &gt; 1: ## :...checking_balance = &gt; 200 DM: no (14.7/6.3) ## checking_balance = 1 - 200 DM: yes (25.1/8.8) ## checking_balance = &lt; 0 DM: ## :...months_loan_duration &lt;= 16: no (13.8/3.4) ## months_loan_duration &gt; 16: yes (19.1/5.5) ## ## ----- Trial 8: ----- ## ## Decision tree: ## ## checking_balance in {&lt; 0 DM,1 - 200 DM}: ## :...credit_history = perfect: ## : :...housing in {other,rent}: yes (8.3) ## : : housing = own: ## : : :...age &lt;= 34: no (16.6/4.7) ## : : age &gt; 34: yes (5.8) ## : credit_history = poor: ## : :...checking_balance = &lt; 0 DM: yes (12/2.7) ## : : checking_balance = 1 - 200 DM: ## : : :...housing = rent: no (8.6) ## : : housing in {other,own}: ## : : :...amount &lt;= 2279: yes (6.8/0.6) ## : : amount &gt; 2279: no (20/5.7) ## : credit_history = very good: ## : :...existing_loans_count &gt; 1: yes (2.5) ## : : existing_loans_count &lt;= 1: ## : : :...age &lt;= 23: no (3.7) ## : : age &gt; 23: ## : : :...amount &lt;= 8386: yes (32.9/8.1) ## : : amount &gt; 8386: no (2.5) ## : credit_history = critical: ## : :...years_at_residence &lt;= 1: no (8) ## : : years_at_residence &gt; 1: ## : : :...savings_balance in {&gt; 1000 DM,100 - 500 DM,500 - 1000 DM, ## : : : unknown}: no (25.5/5.7) ## : : savings_balance = &lt; 100 DM: ## : : :...age &gt; 61: no (6) ## : : age &lt;= 61: ## : : :...existing_loans_count &gt; 2: no (10.7/2.4) ## : : existing_loans_count &lt;= 2: ## : : :...age &gt; 56: yes (5.4) ## : : age &lt;= 56: ## : : :...amount &gt; 2483: yes (34.1/8.9) ## : : amount &lt;= 2483: ## : : :...purpose in {business,education}: yes (4.4) ## : : purpose in {car,car0,furniture/appliances, ## : : renovations}: no (41.4/10.8) ## : credit_history = good: ## : :...amount &gt; 8086: yes (26.6/4.8) ## : amount &lt;= 8086: ## : :...savings_balance in {&gt; 1000 DM,500 - 1000 DM}: no (17.5/5.1) ## : savings_balance = 100 - 500 DM: ## : :...months_loan_duration &lt;= 27: no (21.3/7.1) ## : : months_loan_duration &gt; 27: yes (5.1) ## : savings_balance = unknown: ## : :...age &lt;= 56: yes (44.7/16.9) ## : : age &gt; 56: no (4.4) ## : savings_balance = &lt; 100 DM: ## : :...job = unemployed: yes (0.9) ## : job = management: ## : :...employment_duration in {&lt; 1 year,1 - 4 years,4 - 7 years, ## : : : unemployed}: no (17.3/1.6) ## : : employment_duration = &gt; 7 years: yes (8/1.2) ## : job = unskilled: ## : :...months_loan_duration &lt;= 26: no (59/19.7) ## : : months_loan_duration &gt; 26: yes (3.3) ## : job = skilled: ## : :...purpose in {business,car0,education, ## : : renovations}: yes (16.6/4.1) ## : purpose = car: ## : :...dependents &lt;= 1: yes (27.7/10.6) ## : : dependents &gt; 1: no (8.1/1.4) ## : purpose = furniture/appliances: ## : :...years_at_residence &lt;= 1: no (18.7/6.5) ## : years_at_residence &gt; 1: ## : :...other_credit = bank: yes (4.5) ## : other_credit = store: no (2.3) ## : other_credit = none: ## : :...percent_of_income &lt;= 3: yes (33.5/15) ## : percent_of_income &gt; 3: no (27.3/9.3) ## checking_balance in {&gt; 200 DM,unknown}: ## :...years_at_residence &gt; 2: no (135.6/32.2) ## years_at_residence &lt;= 2: ## :...months_loan_duration &lt;= 8: no (12.9) ## months_loan_duration &gt; 8: ## :...months_loan_duration &lt;= 9: yes (10.4/1.3) ## months_loan_duration &gt; 9: ## :...months_loan_duration &lt;= 16: no (31.3/4.2) ## months_loan_duration &gt; 16: ## :...purpose in {business,car0,renovations}: no (21.3/8.4) ## purpose = education: yes (6.3/0.8) ## purpose = car: ## :...credit_history in {critical,very good}: yes (17.3/2.6) ## : credit_history in {good,perfect,poor}: no (9.6) ## purpose = furniture/appliances: ## :...credit_history in {critical,perfect, ## : very good}: no (5.6) ## credit_history = poor: yes (4.9) ## credit_history = good: ## :...housing in {other,rent}: no (2.6) ## housing = own: ## :...age &lt;= 25: no (6.8) ## age &gt; 25: yes (29.2/10.2) ## ## ----- Trial 9: ----- ## ## Decision tree: ## ## checking_balance = unknown: ## :...dependents &gt; 1: no (26) ## : dependents &lt;= 1: ## : :...amount &lt;= 1474: no (39.7) ## : amount &gt; 1474: ## : :...employment_duration in {&gt; 7 years,4 - 7 years}: ## : :...years_at_residence &gt; 2: no (21.8) ## : : years_at_residence &lt;= 2: ## : : :...age &lt;= 23: yes (4.1) ## : : age &gt; 23: no (19.7/4.2) ## : employment_duration in {&lt; 1 year,1 - 4 years,unemployed}: ## : :...purpose in {business,renovations}: yes (23.2/3.6) ## : purpose in {car,car0,education,furniture/appliances}: ## : :...other_credit in {bank,store}: yes (29.1/10.5) ## : other_credit = none: ## : :...purpose in {car,car0}: no (12.3) ## : purpose in {education,furniture/appliances}: ## : :...amount &lt;= 4455: no (23.7/4.4) ## : amount &gt; 4455: yes (11.1/1.3) ## checking_balance in {&lt; 0 DM,&gt; 200 DM,1 - 200 DM}: ## :...percent_of_income &lt;= 2: ## :...amount &gt; 11054: yes (15.7/3.6) ## : amount &lt;= 11054: ## : :...savings_balance in {&gt; 1000 DM,500 - 1000 DM, ## : : unknown}: no (41.5/11.2) ## : savings_balance = 100 - 500 DM: ## : :...other_credit = bank: no (5.1) ## : : other_credit in {none,store}: yes (21.7/9.4) ## : savings_balance = &lt; 100 DM: ## : :...employment_duration in {&gt; 7 years,unemployed}: no (34.6/11.5) ## : employment_duration = 1 - 4 years: ## : :...job = management: yes (5.1/0.8) ## : : job in {skilled,unemployed,unskilled}: no (65.4/15.8) ## : employment_duration = &lt; 1 year: ## : :...amount &lt;= 2327: ## : : :...age &lt;= 34: yes (20.5/1.9) ## : : : age &gt; 34: no (3) ## : : amount &gt; 2327: ## : : :...other_credit = bank: yes (2.8) ## : : other_credit in {none,store}: no (20.1/3.9) ## : employment_duration = 4 - 7 years: ## : :...dependents &gt; 1: no (4.6) ## : dependents &lt;= 1: ## : :...amount &lt;= 6527: no (16.8/7.2) ## : amount &gt; 6527: yes (7) ## percent_of_income &gt; 2: ## :...housing = rent: ## :...checking_balance in {&lt; 0 DM,1 - 200 DM}: yes (69/22.1) ## : checking_balance = &gt; 200 DM: no (3.4) ## housing = other: ## :...existing_loans_count &gt; 1: yes (18.7/5.3) ## : existing_loans_count &lt;= 1: ## : :...savings_balance in {&lt; 100 DM,&gt; 1000 DM, ## : : 500 - 1000 DM}: yes (29.1/8.6) ## : savings_balance in {100 - 500 DM,unknown}: no (15.3/3.2) ## housing = own: ## :...credit_history in {perfect,poor}: yes (26.9/7.4) ## credit_history = very good: no (14.9/5.6) ## credit_history = critical: ## :...other_credit = bank: yes (11.7/3.4) ## : other_credit in {none,store}: no (63/20.3) ## credit_history = good: ## :...other_credit = store: yes (8.9/1.4) ## other_credit in {bank,none}: ## :...age &gt; 54: no (9.5) ## age &lt;= 54: ## :...existing_loans_count &gt; 1: no (10.2/2.7) ## existing_loans_count &lt;= 1: ## :...purpose in {business,renovations}: no (10.1/3.6) ## purpose in {car0,education}: yes (4.7) ## purpose = car: ## :...other_credit = bank: yes (4.9) ## : other_credit = none: ## : :...years_at_residence &gt; 2: no (14.8/4.5) ## : years_at_residence &lt;= 2: ## : :...amount &lt;= 2150: no (14.9/6.2) ## : amount &gt; 2150: yes (11.1) ## purpose = furniture/appliances: ## :...savings_balance = 100 - 500 DM: yes (3.8) ## savings_balance in {&gt; 1000 DM, ## : 500 - 1000 DM}: no (2.8) ## savings_balance in {&lt; 100 DM,unknown}: ## :...months_loan_duration &gt; 39: yes (3.3) ## months_loan_duration &lt;= 39: ## :...dependents &lt;= 1: no (57.6/19.4) ## dependents &gt; 1: yes (4.6/1.1) ## ## ## Evaluation on training data (900 cases): ## ## Trial Decision Tree ## ----- ---------------- ## Size Errors ## ## 0 56 133(14.8%) ## 1 34 211(23.4%) ## 2 39 201(22.3%) ## 3 47 179(19.9%) ## 4 46 174(19.3%) ## 5 50 197(21.9%) ## 6 55 187(20.8%) ## 7 50 190(21.1%) ## 8 51 192(21.3%) ## 9 47 169(18.8%) ## boost 34( 3.8%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 629 4 (a): class no ## 30 237 (b): class yes ## ## ## Attribute usage: ## ## 100.00% checking_balance ## 100.00% purpose ## 97.11% years_at_residence ## 96.67% employment_duration ## 94.78% credit_history ## 94.67% other_credit ## 92.56% job ## 92.11% percent_of_income ## 90.33% amount ## 85.11% months_loan_duration ## 82.78% age ## 82.78% existing_loans_count ## 75.78% dependents ## 71.56% housing ## 70.78% savings_balance ## 49.22% phone ## ## ## Time: 0.1 secs credit_boost_pred10 &lt;- predict(credit_boost10, credit_test) CrossTable(credit_test$default, credit_boost_pred10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&#39;actual default&#39;, &#39;predicted default&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | predicted default ## actual default | no | yes | Row Total | ## ---------------|-----------|-----------|-----------| ## no | 62 | 5 | 67 | ## | 0.620 | 0.050 | | ## ---------------|-----------|-----------|-----------| ## yes | 13 | 20 | 33 | ## | 0.130 | 0.200 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 75 | 25 | 100 | ## ---------------|-----------|-----------|-----------| ## ## 5.5 Making some mistakes more costly than others create dimensions for a cost matrix matrix_dimensions &lt;- list(c(&quot;no&quot;, &quot;yes&quot;), c(&quot;no&quot;, &quot;yes&quot;)) names(matrix_dimensions) &lt;- c(&quot;predicted&quot;, &quot;actual&quot;) matrix_dimensions ## $predicted ## [1] &quot;no&quot; &quot;yes&quot; ## ## $actual ## [1] &quot;no&quot; &quot;yes&quot; build the matrix error_cost &lt;- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions) error_cost ## actual ## predicted no yes ## no 0 4 ## yes 1 0 apply the cost matrix to the tree credit_cost &lt;- C5.0(credit_train[-17], credit_train$default, costs = error_cost) credit_cost_pred &lt;- predict(credit_cost, credit_test) CrossTable(credit_test$default, credit_cost_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&#39;actual default&#39;, &#39;predicted default&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | predicted default ## actual default | no | yes | Row Total | ## ---------------|-----------|-----------|-----------| ## no | 37 | 30 | 67 | ## | 0.370 | 0.300 | | ## ---------------|-----------|-----------|-----------| ## yes | 7 | 26 | 33 | ## | 0.070 | 0.260 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 44 | 56 | 100 | ## ---------------|-----------|-----------|-----------| ## ## 5.6 Part 2: Rule Learners 5.7 Example: Identifying Poisonous Mushrooms 5.7.1 Step 2: Exploring and preparing the data mushrooms &lt;- read.csv(&quot;Chapter 05/mushrooms.csv&quot;, stringsAsFactors = TRUE) examine the structure of the data frame str(mushrooms) ## &#39;data.frame&#39;: 8124 obs. of 23 variables: ## $ type : Factor w/ 2 levels &quot;edible&quot;,&quot;poisonous&quot;: 2 1 1 2 1 1 1 1 2 1 ... ## $ cap_shape : Factor w/ 6 levels &quot;bell&quot;,&quot;conical&quot;,..: 3 3 1 3 3 3 1 1 3 1 ... ## $ cap_surface : Factor w/ 4 levels &quot;fibrous&quot;,&quot;grooves&quot;,..: 4 4 4 3 4 3 4 3 3 4 ... ## $ cap_color : Factor w/ 10 levels &quot;brown&quot;,&quot;buff&quot;,..: 1 10 9 9 4 10 9 9 9 10 ... ## $ bruises : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 2 1 2 2 2 2 2 ... ## $ odor : Factor w/ 9 levels &quot;almond&quot;,&quot;anise&quot;,..: 8 1 2 8 7 1 1 2 8 1 ... ## $ gill_attachment : Factor w/ 2 levels &quot;attached&quot;,&quot;free&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ gill_spacing : Factor w/ 2 levels &quot;close&quot;,&quot;crowded&quot;: 1 1 1 1 2 1 1 1 1 1 ... ## $ gill_size : Factor w/ 2 levels &quot;broad&quot;,&quot;narrow&quot;: 2 1 1 2 1 1 1 1 2 1 ... ## $ gill_color : Factor w/ 12 levels &quot;black&quot;,&quot;brown&quot;,..: 1 1 2 2 1 2 5 2 8 5 ... ## $ stalk_shape : Factor w/ 2 levels &quot;enlarging&quot;,&quot;tapering&quot;: 1 1 1 1 2 1 1 1 1 1 ... ## $ stalk_root : Factor w/ 5 levels &quot;bulbous&quot;,&quot;club&quot;,..: 3 2 2 3 3 2 2 2 3 2 ... ## $ stalk_surface_above_ring: Factor w/ 4 levels &quot;fibrous&quot;,&quot;scaly&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ stalk_surface_below_ring: Factor w/ 4 levels &quot;fibrous&quot;,&quot;scaly&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ stalk_color_above_ring : Factor w/ 9 levels &quot;brown&quot;,&quot;buff&quot;,..: 8 8 8 8 8 8 8 8 8 8 ... ## $ stalk_color_below_ring : Factor w/ 9 levels &quot;brown&quot;,&quot;buff&quot;,..: 8 8 8 8 8 8 8 8 8 8 ... ## $ veil_type : Factor w/ 1 level &quot;partial&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ veil_color : Factor w/ 4 levels &quot;brown&quot;,&quot;orange&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ ring_number : Factor w/ 3 levels &quot;none&quot;,&quot;one&quot;,&quot;two&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ ring_type : Factor w/ 5 levels &quot;evanescent&quot;,&quot;flaring&quot;,..: 5 5 5 5 1 5 5 5 5 5 ... ## $ spore_print_color : Factor w/ 9 levels &quot;black&quot;,&quot;brown&quot;,..: 1 2 2 1 2 1 1 2 1 1 ... ## $ population : Factor w/ 6 levels &quot;abundant&quot;,&quot;clustered&quot;,..: 4 3 3 4 1 3 3 4 5 4 ... ## $ habitat : Factor w/ 7 levels &quot;grasses&quot;,&quot;leaves&quot;,..: 5 1 3 5 1 1 3 3 1 3 ... drop the veil_type feature mushrooms$veil_type &lt;- NULL examine the class distribution table(mushrooms$type) ## ## edible poisonous ## 4208 3916 5.7.2 Step 3: Training a model on the data train OneR() on the data mushroom_1R &lt;- OneR(type ~ ., data = mushrooms) 5.7.3 Step 4: Evaluating model performance mushroom_1R ## ## Call: ## OneR.formula(formula = type ~ ., data = mushrooms) ## ## Rules: ## If odor = almond then type = edible ## If odor = anise then type = edible ## If odor = creosote then type = poisonous ## If odor = fishy then type = poisonous ## If odor = foul then type = poisonous ## If odor = musty then type = poisonous ## If odor = none then type = edible ## If odor = pungent then type = poisonous ## If odor = spicy then type = poisonous ## ## Accuracy: ## 8004 of 8124 instances classified correctly (98.52%) summary(mushroom_1R) ## ## Call: ## OneR.formula(formula = type ~ ., data = mushrooms) ## ## Rules: ## If odor = almond then type = edible ## If odor = anise then type = edible ## If odor = creosote then type = poisonous ## If odor = fishy then type = poisonous ## If odor = foul then type = poisonous ## If odor = musty then type = poisonous ## If odor = none then type = edible ## If odor = pungent then type = poisonous ## If odor = spicy then type = poisonous ## ## Accuracy: ## 8004 of 8124 instances classified correctly (98.52%) ## ## Contingency table: ## odor ## type almond anise creosote fishy foul musty none pungent spicy ## edible * 400 * 400 0 0 0 0 * 3408 0 0 ## poisonous 0 0 * 192 * 576 * 2160 * 36 120 * 256 * 576 ## Sum 400 400 192 576 2160 36 3528 256 576 ## odor ## type Sum ## edible 4208 ## poisonous 3916 ## Sum 8124 ## --- ## Maximum in each column: &#39;*&#39; ## ## Pearson&#39;s Chi-squared test: ## X-squared = 7659.7, df = 8, p-value &lt; 2.2e-16 5.7.4 Step 5: Improving model performance # mushroom_JRip &lt;- JRip(type ~ ., data = mushrooms) # mushroom_JRip # summary(mushroom_JRip) Rule Learner Using C5.0 Decision Trees (not in text) mushroom_c5rules &lt;- C5.0(type ~ odor + gill_size, data = mushrooms, rules = TRUE) summary(mushroom_c5rules) ## ## Call: ## C5.0.formula(formula = type ~ odor + gill_size, data = mushrooms, rules ## = TRUE) ## ## ## C5.0 [Release 2.07 GPL Edition] Wed Jan 2 16:31:56 2019 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 8124 cases (3 attributes) from undefined.data ## ## Rules: ## ## Rule 1: (4328/120, lift 1.9) ## odor in {almond, anise, none} ## -&gt; class edible [0.972] ## ## Rule 2: (3796, lift 2.1) ## odor in {creosote, fishy, foul, musty, pungent, spicy} ## -&gt; class poisonous [1.000] ## ## Default class: edible ## ## ## Evaluation on training data (8124 cases): ## ## Rules ## ---------------- ## No Errors ## ## 2 120( 1.5%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 4208 (a): class edible ## 120 3796 (b): class poisonous ## ## ## Attribute usage: ## ## 100.00% odor ## ## ## Time: 0.0 secs 5.8 Summary Two classification methods that use so-called “greedy” algorithms to partition the data according to feature values Decision trees use a divide and conquer strategy to create flowchart-like structures Rule learners separate and conquer data to identify logical if-else rules Both methods produce models that can be interpreted without a statistical background The C5.0 is a highly configurable decision tree algorithm The 1R algorithm used a single feature to achieve 99 percent accuracy The set of nine rules generated by the more sophisticated "],
["regression-methods.html", "Chapter 6 Regression Methods 6.1 Part 1: Linear Regression 6.2 Understanding regression 6.3 Example: Space Shuttle Launch Data 6.4 Example: Predicting Medical Expenses 6.5 Part 2: Regression Trees and Model Trees 6.6 Understanding regression trees and model trees 6.7 Example: Calculating SDR 6.8 Example: Estimating Wine Quality", " Chapter 6 Regression Methods 6.1 Part 1: Linear Regression Regression is concerned with specifying the relationship between a single numeric dependent variable (the value to be predicted) and one or more numeric independent variables (the predictors). Regression methods are also used for statistical hypothesis testing, which determines whether a premise is likely to be true or false in light of the observed data. basic linear regression models—those that use straight lines with only a single independent variable known as simple linear regression. the case of two or more independent variables, this is known as multiple linear regression, or simply “multiple regression”. Both of these techniques assume that the dependent variable is measured on a continuous scale logistic regression is used to model a binary categorical outcome Poisson regression models integer count data multinomial logistic regression models a categorical outcome 6.2 Understanding regression 6.3 Example: Space Shuttle Launch Data launch &lt;- read.csv(&quot;Chapter 06/challenger.csv&quot;) estimate beta manually b &lt;- cov(launch$temperature, launch$distress_ct) / var(launch$temperature) b ## [1] -0.04753968 estimate alpha manually a &lt;- mean(launch$distress_ct) - b * mean(launch$temperature) a ## [1] 3.698413 calculate the correlation of launch data r &lt;- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature) * sd(launch$distress_ct)) r ## [1] -0.5111264 cor(launch$temperature, launch$distress_ct) ## [1] -0.5111264 computing the slope using correlation r * (sd(launch$distress_ct) / sd(launch$temperature)) ## [1] -0.04753968 confirming the regression line using the lm function (not in text) model &lt;- lm(distress_ct ~ temperature + field_check_pressure + flight_num, data = launch) model ## ## Call: ## lm(formula = distress_ct ~ temperature + field_check_pressure + ## flight_num, data = launch) ## ## Coefficients: ## (Intercept) temperature field_check_pressure ## 3.527093 -0.051386 0.001757 ## flight_num ## 0.014293 summary(model) ## ## Call: ## lm(formula = distress_ct ~ temperature + field_check_pressure + ## flight_num, data = launch) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65003 -0.24414 -0.11219 0.01279 1.67530 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.527093 1.307024 2.699 0.0142 * ## temperature -0.051386 0.018341 -2.802 0.0114 * ## field_check_pressure 0.001757 0.003402 0.517 0.6115 ## flight_num 0.014293 0.035138 0.407 0.6887 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.565 on 19 degrees of freedom ## Multiple R-squared: 0.36, Adjusted R-squared: 0.259 ## F-statistic: 3.563 on 3 and 19 DF, p-value: 0.03371 creating a simple multiple regression function reg &lt;- function(y, x) { x &lt;- as.matrix(x) x &lt;- cbind(Intercept = 1, x) b &lt;- solve(t(x) %*% x) %*% t(x) %*% y colnames(b) &lt;- &quot;estimate&quot; print(b) } examine the launch data str(launch) ## &#39;data.frame&#39;: 23 obs. of 4 variables: ## $ distress_ct : int 0 1 0 0 0 0 0 0 1 1 ... ## $ temperature : int 66 70 69 68 67 72 73 70 57 63 ... ## $ field_check_pressure: int 50 50 50 50 50 50 100 100 200 200 ... ## $ flight_num : int 1 2 3 4 5 6 7 8 9 10 ... test regression model with simple linear regression reg(y = launch$distress_ct, x = launch[2]) ## estimate ## Intercept 3.69841270 ## temperature -0.04753968 use regression model with multiple regression reg(y = launch$distress_ct, x = launch[2:4]) ## estimate ## Intercept 3.527093383 ## temperature -0.051385940 ## field_check_pressure 0.001757009 ## flight_num 0.014292843 confirming the multiple regression result using the lm function (not in text) model &lt;- lm(distress_ct ~ temperature + field_check_pressure, data = launch) model ## ## Call: ## lm(formula = distress_ct ~ temperature + field_check_pressure, ## data = launch) ## ## Coefficients: ## (Intercept) temperature field_check_pressure ## 3.329831 -0.048671 0.002939 6.4 Example: Predicting Medical Expenses 6.4.1 Step 1: collecting data The insurance.csv file includes 1,338 examples of beneficiaries currently enrolled in the insurance plan, with features indicating characteristics of the patient as well as the total medical expenses charged to the plan for the calendar year. The features are: age: An integer indicating the age of the primary beneficiary (excluding those above 64 years, since they are generally covered by the government). sex: The policy holder’s gender, either male or female. bmi: The body mass index (BMI), which provides a sense of how over- or under-weight a person is relative to their height. BMI is equal to weight (in kilograms) divided by height (in meters) squared. An ideal BMI is within the range of 18.5 to 24.9. children: An integer indicating the number of children/dependents covered by the insurance plan. smoker: A yes or no categorical variable that indicates whether the insured regularly smokes tobacco. region: The beneficiary’s place of residence in the US, divided into four geographic regions: northeast, southeast, southwest, or northwest. 6.4.2 Step 2: Exploring and preparing the data insurance &lt;- read.csv(&quot;Chapter 06/insurance.csv&quot;) str(insurance) ## &#39;data.frame&#39;: 1338 obs. of 7 variables: ## $ age : int 19 18 28 33 32 31 46 37 37 60 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 1 1 1 2 1 ... ## $ bmi : num 27.9 33.8 33 22.7 28.9 25.7 33.4 27.7 29.8 25.8 ... ## $ children: int 0 1 3 0 0 0 1 3 2 0 ... ## $ smoker : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 1 1 1 1 1 1 1 1 ... ## $ region : Factor w/ 4 levels &quot;northeast&quot;,&quot;northwest&quot;,..: 4 3 3 2 2 3 3 2 1 2 ... ## $ expenses: num 16885 1726 4449 21984 3867 ... summarize the charges variable summary(insurance$expenses) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1122 4740 9382 13270 16640 63770 histogram of insurance charges hist(insurance$expenses) table of region table(insurance$region) ## ## northeast northwest southeast southwest ## 324 325 364 325 exploring relationships among features: correlation matrix cor(insurance[c(&quot;age&quot;, &quot;bmi&quot;, &quot;children&quot;, &quot;expenses&quot;)]) ## age bmi children expenses ## age 1.0000000 0.10934101 0.04246900 0.29900819 ## bmi 0.1093410 1.00000000 0.01264471 0.19857626 ## children 0.0424690 0.01264471 1.00000000 0.06799823 ## expenses 0.2990082 0.19857626 0.06799823 1.00000000 visualing relationships among features: scatterplot matrix pairs(insurance[c(&quot;age&quot;, &quot;bmi&quot;, &quot;children&quot;, &quot;expenses&quot;)]) more informative scatterplot matrix pairs.panels(insurance[c(&quot;age&quot;, &quot;bmi&quot;, &quot;children&quot;, &quot;expenses&quot;)]) 6.4.3 Step 3: Training a model on the data ins_model &lt;- lm(expenses ~ age + children + bmi + sex + smoker + region, data = insurance) ins_model &lt;- lm(expenses ~ ., data = insurance) # this is equivalent to above see the estimated beta coefficients ins_model ## ## Call: ## lm(formula = expenses ~ ., data = insurance) ## ## Coefficients: ## (Intercept) age sexmale bmi ## -11941.6 256.8 -131.4 339.3 ## children smokeryes regionnorthwest regionsoutheast ## 475.7 23847.5 -352.8 -1035.6 ## regionsouthwest ## -959.3 6.4.4 Step 4: Evaluating model performance see more detail about the estimated beta coefficients summary(ins_model) ## ## Call: ## lm(formula = expenses ~ ., data = insurance) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11302.7 -2850.9 -979.6 1383.9 29981.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -11941.6 987.8 -12.089 &lt; 2e-16 *** ## age 256.8 11.9 21.586 &lt; 2e-16 *** ## sexmale -131.3 332.9 -0.395 0.693255 ## bmi 339.3 28.6 11.864 &lt; 2e-16 *** ## children 475.7 137.8 3.452 0.000574 *** ## smokeryes 23847.5 413.1 57.723 &lt; 2e-16 *** ## regionnorthwest -352.8 476.3 -0.741 0.458976 ## regionsoutheast -1035.6 478.7 -2.163 0.030685 * ## regionsouthwest -959.3 477.9 -2.007 0.044921 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6062 on 1329 degrees of freedom ## Multiple R-squared: 0.7509, Adjusted R-squared: 0.7494 ## F-statistic: 500.9 on 8 and 1329 DF, p-value: &lt; 2.2e-16 6.4.5 Step 5: Improving model performance add a higher-order “age” term insurance$age2 &lt;- insurance$age^2 add an indicator for BMI &gt;= 30 insurance$bmi30 &lt;- ifelse(insurance$bmi &gt;= 30, 1, 0) create final model ins_model2 &lt;- lm(expenses ~ age + age2 + children + bmi + sex + bmi30*smoker + region, data = insurance) summary(ins_model2) ## ## Call: ## lm(formula = expenses ~ age + age2 + children + bmi + sex + bmi30 * ## smoker + region, data = insurance) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17297.1 -1656.0 -1262.7 -727.8 24161.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 139.0053 1363.1359 0.102 0.918792 ## age -32.6181 59.8250 -0.545 0.585690 ## age2 3.7307 0.7463 4.999 6.54e-07 *** ## children 678.6017 105.8855 6.409 2.03e-10 *** ## bmi 119.7715 34.2796 3.494 0.000492 *** ## sexmale -496.7690 244.3713 -2.033 0.042267 * ## bmi30 -997.9355 422.9607 -2.359 0.018449 * ## smokeryes 13404.5952 439.9591 30.468 &lt; 2e-16 *** ## regionnorthwest -279.1661 349.2826 -0.799 0.424285 ## regionsoutheast -828.0345 351.6484 -2.355 0.018682 * ## regionsouthwest -1222.1619 350.5314 -3.487 0.000505 *** ## bmi30:smokeryes 19810.1534 604.6769 32.762 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4445 on 1326 degrees of freedom ## Multiple R-squared: 0.8664, Adjusted R-squared: 0.8653 ## F-statistic: 781.7 on 11 and 1326 DF, p-value: &lt; 2.2e-16 6.5 Part 2: Regression Trees and Model Trees nown as regression trees, were introduced in the 1980s as part of the seminal Classification and Regression Tree (CART) algorithm. Despite the name, regression trees do not use linear regression methods as described earlier in this chapter, rather they make predictions based on the average value of examples that reach a leaf. 6.6 Understanding regression trees and model trees 6.7 Example: Calculating SDR set up the data tee &lt;- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7) at1 &lt;- c(1, 1, 1, 2, 2, 3, 4, 5, 5) at2 &lt;- c(6, 6, 7, 7, 7, 7) bt1 &lt;- c(1, 1, 1, 2, 2, 3, 4) bt2 &lt;- c(5, 5, 6, 6, 7, 7, 7, 7) compute the SDR sdr_a &lt;- sd(tee) - (length(at1) / length(tee) * sd(at1) + length(at2) / length(tee) * sd(at2)) sdr_b &lt;- sd(tee) - (length(bt1) / length(tee) * sd(bt1) + length(bt2) / length(tee) * sd(bt2)) compare the SDR for each split sdr_a ## [1] 1.202815 sdr_b ## [1] 1.392751 6.8 Example: Estimating Wine Quality 6.8.1 Step 2: Exploring and preparing the data wine &lt;- read.csv(&quot;Chapter 06/whitewines.csv&quot;) examine the wine data str(wine) ## &#39;data.frame&#39;: 4898 obs. of 12 variables: ## $ fixed.acidity : num 6.7 5.7 5.9 5.3 6.4 7 7.9 6.6 7 6.5 ... ## $ volatile.acidity : num 0.62 0.22 0.19 0.47 0.29 0.14 0.12 0.38 0.16 0.37 ... ## $ citric.acid : num 0.24 0.2 0.26 0.1 0.21 0.41 0.49 0.28 0.3 0.33 ... ## $ residual.sugar : num 1.1 16 7.4 1.3 9.65 0.9 5.2 2.8 2.6 3.9 ... ## $ chlorides : num 0.039 0.044 0.034 0.036 0.041 0.037 0.049 0.043 0.043 0.027 ... ## $ free.sulfur.dioxide : num 6 41 33 11 36 22 33 17 34 40 ... ## $ total.sulfur.dioxide: num 62 113 123 74 119 95 152 67 90 130 ... ## $ density : num 0.993 0.999 0.995 0.991 0.993 ... ## $ pH : num 3.41 3.22 3.49 3.48 2.99 3.25 3.18 3.21 2.88 3.28 ... ## $ sulphates : num 0.32 0.46 0.42 0.54 0.34 0.43 0.47 0.47 0.47 0.39 ... ## $ alcohol : num 10.4 8.9 10.1 11.2 10.9 ... ## $ quality : int 5 6 6 4 6 6 6 6 6 7 ... the distribution of quality ratings hist(wine$quality) summary statistics of the wine data summary(wine) ## fixed.acidity volatile.acidity citric.acid residual.sugar ## Min. : 3.800 Min. :0.0800 Min. :0.0000 Min. : 0.600 ## 1st Qu.: 6.300 1st Qu.:0.2100 1st Qu.:0.2700 1st Qu.: 1.700 ## Median : 6.800 Median :0.2600 Median :0.3200 Median : 5.200 ## Mean : 6.855 Mean :0.2782 Mean :0.3342 Mean : 6.391 ## 3rd Qu.: 7.300 3rd Qu.:0.3200 3rd Qu.:0.3900 3rd Qu.: 9.900 ## Max. :14.200 Max. :1.1000 Max. :1.6600 Max. :65.800 ## chlorides free.sulfur.dioxide total.sulfur.dioxide ## Min. :0.00900 Min. : 2.00 Min. : 9.0 ## 1st Qu.:0.03600 1st Qu.: 23.00 1st Qu.:108.0 ## Median :0.04300 Median : 34.00 Median :134.0 ## Mean :0.04577 Mean : 35.31 Mean :138.4 ## 3rd Qu.:0.05000 3rd Qu.: 46.00 3rd Qu.:167.0 ## Max. :0.34600 Max. :289.00 Max. :440.0 ## density pH sulphates alcohol ## Min. :0.9871 Min. :2.720 Min. :0.2200 Min. : 8.00 ## 1st Qu.:0.9917 1st Qu.:3.090 1st Qu.:0.4100 1st Qu.: 9.50 ## Median :0.9937 Median :3.180 Median :0.4700 Median :10.40 ## Mean :0.9940 Mean :3.188 Mean :0.4898 Mean :10.51 ## 3rd Qu.:0.9961 3rd Qu.:3.280 3rd Qu.:0.5500 3rd Qu.:11.40 ## Max. :1.0390 Max. :3.820 Max. :1.0800 Max. :14.20 ## quality ## Min. :3.000 ## 1st Qu.:5.000 ## Median :6.000 ## Mean :5.878 ## 3rd Qu.:6.000 ## Max. :9.000 wine_train &lt;- wine[1:3750, ] wine_test &lt;- wine[3751:4898, ] 6.8.2 Step 3: Training a model on the data regression tree using rpart m.rpart &lt;- rpart(quality ~ ., data = wine_train) get basic information about the tree m.rpart ## n= 3750 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 3750 2945.53200 5.870933 ## 2) alcohol&lt; 10.85 2372 1418.86100 5.604975 ## 4) volatile.acidity&gt;=0.2275 1611 821.30730 5.432030 ## 8) volatile.acidity&gt;=0.3025 688 278.97670 5.255814 * ## 9) volatile.acidity&lt; 0.3025 923 505.04230 5.563380 * ## 5) volatile.acidity&lt; 0.2275 761 447.36400 5.971091 * ## 3) alcohol&gt;=10.85 1378 1070.08200 6.328737 ## 6) free.sulfur.dioxide&lt; 10.5 84 95.55952 5.369048 * ## 7) free.sulfur.dioxide&gt;=10.5 1294 892.13600 6.391036 ## 14) alcohol&lt; 11.76667 629 430.11130 6.173291 ## 28) volatile.acidity&gt;=0.465 11 10.72727 4.545455 * ## 29) volatile.acidity&lt; 0.465 618 389.71680 6.202265 * ## 15) alcohol&gt;=11.76667 665 403.99400 6.596992 * get more detailed information about the tree summary(m.rpart) ## Call: ## rpart(formula = quality ~ ., data = wine_train) ## n= 3750 ## ## CP nsplit rel error xerror xstd ## 1 0.15501053 0 1.0000000 1.0006401 0.02444955 ## 2 0.05098911 1 0.8449895 0.8458444 0.02332215 ## 3 0.02796998 2 0.7940004 0.8045001 0.02276382 ## 4 0.01970128 3 0.7660304 0.7850044 0.02181383 ## 5 0.01265926 4 0.7463291 0.7656874 0.02109208 ## 6 0.01007193 5 0.7336698 0.7651232 0.02112280 ## 7 0.01000000 6 0.7235979 0.7617566 0.02108417 ## ## Variable importance ## alcohol density volatile.acidity ## 34 21 15 ## chlorides total.sulfur.dioxide free.sulfur.dioxide ## 11 7 6 ## residual.sugar sulphates citric.acid ## 3 1 1 ## ## Node number 1: 3750 observations, complexity param=0.1550105 ## mean=5.870933, MSE=0.7854751 ## left son=2 (2372 obs) right son=3 (1378 obs) ## Primary splits: ## alcohol &lt; 10.85 to the left, improve=0.15501050, (0 missing) ## density &lt; 0.992035 to the right, improve=0.10915940, (0 missing) ## chlorides &lt; 0.0395 to the right, improve=0.07682258, (0 missing) ## total.sulfur.dioxide &lt; 158.5 to the right, improve=0.04089663, (0 missing) ## citric.acid &lt; 0.235 to the left, improve=0.03636458, (0 missing) ## Surrogate splits: ## density &lt; 0.991995 to the right, agree=0.869, adj=0.644, (0 split) ## chlorides &lt; 0.0375 to the right, agree=0.757, adj=0.339, (0 split) ## total.sulfur.dioxide &lt; 103.5 to the right, agree=0.690, adj=0.155, (0 split) ## residual.sugar &lt; 5.375 to the right, agree=0.667, adj=0.094, (0 split) ## sulphates &lt; 0.345 to the right, agree=0.647, adj=0.038, (0 split) ## ## Node number 2: 2372 observations, complexity param=0.05098911 ## mean=5.604975, MSE=0.5981709 ## left son=4 (1611 obs) right son=5 (761 obs) ## Primary splits: ## volatile.acidity &lt; 0.2275 to the right, improve=0.10585250, (0 missing) ## free.sulfur.dioxide &lt; 13.5 to the left, improve=0.03390500, (0 missing) ## citric.acid &lt; 0.235 to the left, improve=0.03204075, (0 missing) ## alcohol &lt; 10.11667 to the left, improve=0.03136524, (0 missing) ## chlorides &lt; 0.0585 to the right, improve=0.01633599, (0 missing) ## Surrogate splits: ## pH &lt; 3.485 to the left, agree=0.694, adj=0.047, (0 split) ## sulphates &lt; 0.755 to the left, agree=0.685, adj=0.020, (0 split) ## total.sulfur.dioxide &lt; 105.5 to the right, agree=0.683, adj=0.011, (0 split) ## residual.sugar &lt; 0.75 to the right, agree=0.681, adj=0.007, (0 split) ## chlorides &lt; 0.0285 to the right, agree=0.680, adj=0.003, (0 split) ## ## Node number 3: 1378 observations, complexity param=0.02796998 ## mean=6.328737, MSE=0.7765472 ## left son=6 (84 obs) right son=7 (1294 obs) ## Primary splits: ## free.sulfur.dioxide &lt; 10.5 to the left, improve=0.07699080, (0 missing) ## alcohol &lt; 11.76667 to the left, improve=0.06210660, (0 missing) ## total.sulfur.dioxide &lt; 67.5 to the left, improve=0.04438619, (0 missing) ## residual.sugar &lt; 1.375 to the left, improve=0.02905351, (0 missing) ## fixed.acidity &lt; 7.35 to the right, improve=0.02613259, (0 missing) ## Surrogate splits: ## total.sulfur.dioxide &lt; 53.5 to the left, agree=0.952, adj=0.214, (0 split) ## volatile.acidity &lt; 0.875 to the right, agree=0.940, adj=0.024, (0 split) ## ## Node number 4: 1611 observations, complexity param=0.01265926 ## mean=5.43203, MSE=0.5098121 ## left son=8 (688 obs) right son=9 (923 obs) ## Primary splits: ## volatile.acidity &lt; 0.3025 to the right, improve=0.04540111, (0 missing) ## alcohol &lt; 10.05 to the left, improve=0.03874403, (0 missing) ## free.sulfur.dioxide &lt; 13.5 to the left, improve=0.03338886, (0 missing) ## chlorides &lt; 0.0495 to the right, improve=0.02574623, (0 missing) ## citric.acid &lt; 0.195 to the left, improve=0.02327981, (0 missing) ## Surrogate splits: ## citric.acid &lt; 0.215 to the left, agree=0.633, adj=0.141, (0 split) ## free.sulfur.dioxide &lt; 20.5 to the left, agree=0.600, adj=0.063, (0 split) ## chlorides &lt; 0.0595 to the right, agree=0.593, adj=0.047, (0 split) ## residual.sugar &lt; 1.15 to the left, agree=0.583, adj=0.023, (0 split) ## total.sulfur.dioxide &lt; 219.25 to the right, agree=0.582, adj=0.022, (0 split) ## ## Node number 5: 761 observations ## mean=5.971091, MSE=0.5878633 ## ## Node number 6: 84 observations ## mean=5.369048, MSE=1.137613 ## ## Node number 7: 1294 observations, complexity param=0.01970128 ## mean=6.391036, MSE=0.6894405 ## left son=14 (629 obs) right son=15 (665 obs) ## Primary splits: ## alcohol &lt; 11.76667 to the left, improve=0.06504696, (0 missing) ## chlorides &lt; 0.0395 to the right, improve=0.02758705, (0 missing) ## fixed.acidity &lt; 7.35 to the right, improve=0.02750932, (0 missing) ## pH &lt; 3.055 to the left, improve=0.02307356, (0 missing) ## total.sulfur.dioxide &lt; 191.5 to the right, improve=0.02186818, (0 missing) ## Surrogate splits: ## density &lt; 0.990885 to the right, agree=0.720, adj=0.424, (0 split) ## volatile.acidity &lt; 0.2675 to the left, agree=0.637, adj=0.253, (0 split) ## chlorides &lt; 0.0365 to the right, agree=0.630, adj=0.238, (0 split) ## residual.sugar &lt; 1.475 to the left, agree=0.575, adj=0.126, (0 split) ## total.sulfur.dioxide &lt; 128.5 to the right, agree=0.574, adj=0.124, (0 split) ## ## Node number 8: 688 observations ## mean=5.255814, MSE=0.4054895 ## ## Node number 9: 923 observations ## mean=5.56338, MSE=0.5471747 ## ## Node number 14: 629 observations, complexity param=0.01007193 ## mean=6.173291, MSE=0.6838017 ## left son=28 (11 obs) right son=29 (618 obs) ## Primary splits: ## volatile.acidity &lt; 0.465 to the right, improve=0.06897561, (0 missing) ## total.sulfur.dioxide &lt; 200 to the right, improve=0.04223066, (0 missing) ## residual.sugar &lt; 0.975 to the left, improve=0.03061714, (0 missing) ## fixed.acidity &lt; 7.35 to the right, improve=0.02978501, (0 missing) ## sulphates &lt; 0.575 to the left, improve=0.02165970, (0 missing) ## Surrogate splits: ## citric.acid &lt; 0.045 to the left, agree=0.986, adj=0.182, (0 split) ## total.sulfur.dioxide &lt; 279.25 to the right, agree=0.986, adj=0.182, (0 split) ## ## Node number 15: 665 observations ## mean=6.596992, MSE=0.6075098 ## ## Node number 28: 11 observations ## mean=4.545455, MSE=0.9752066 ## ## Node number 29: 618 observations ## mean=6.202265, MSE=0.6306098 a basic decision tree diagram rpart.plot(m.rpart, digits = 3) a few adjustments to the diagram rpart.plot(m.rpart, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101) 6.8.3 Step 4: Evaluate model performance generate predictions for the testing dataset p.rpart &lt;- predict(m.rpart, wine_test) compare the distribution of predicted values vs. actual values summary(p.rpart) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.545 5.563 5.971 5.893 6.202 6.597 summary(wine_test$quality) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.000 5.000 6.000 5.901 6.000 9.000 compare the correlation cor(p.rpart, wine_test$quality) ## [1] 0.5369525 function to calculate the mean absolute error MAE &lt;- function(actual, predicted) { mean(abs(actual - predicted)) } mean absolute error between predicted and actual values MAE(p.rpart, wine_test$quality) ## [1] 0.5872652 mean absolute error between actual values and mean value mean(wine_train$quality) # result = 5.87 ## [1] 5.870933 MAE(5.87, wine_test$quality) ## [1] 0.6722474 6.8.4 Step 5: Improving model performance # # train a M5&#39; Model Tree # m.m5p &lt;- M5P(quality ~ ., data = wine_train) # # # display the tree # m.m5p # # # get a summary of the model&#39;s performance # summary(m.m5p) # # # generate predictions for the model # p.m5p &lt;- predict(m.m5p, wine_test) # # # summary statistics about the predictions # summary(p.m5p) # # # correlation between the predicted and true values # cor(p.m5p, wine_test$quality) # # # mean absolute error of predicted and true values # # (uses a custom function defined above) # MAE(wine_test$quality, p.m5p) "],
["neural-networks-and-support-vector-machines.html", "Chapter 7 Neural Networks and Support Vector Machines 7.1 Part 1: Neural Networks 7.2 Example: Modeling the Strength of Concrete 7.3 Part 2: Support Vector Machines 7.4 Example: Optical Character Recognition", " Chapter 7 Neural Networks and Support Vector Machines 7.1 Part 1: Neural Networks 7.2 Example: Modeling the Strength of Concrete 7.2.1 Step 2: Exploring and preparing the data read in data and examine structure concrete &lt;- read.csv(&quot;Chapter 07/concrete.csv&quot;) str(concrete) ## &#39;data.frame&#39;: 1030 obs. of 9 variables: ## $ cement : num 141 169 250 266 155 ... ## $ slag : num 212 42.2 0 114 183.4 ... ## $ ash : num 0 124.3 95.7 0 0 ... ## $ water : num 204 158 187 228 193 ... ## $ superplastic: num 0 10.8 5.5 0 9.1 0 0 6.4 0 9 ... ## $ coarseagg : num 972 1081 957 932 1047 ... ## $ fineagg : num 748 796 861 670 697 ... ## $ age : int 28 14 28 28 28 90 7 56 28 28 ... ## $ strength : num 29.9 23.5 29.2 45.9 18.3 ... custom normalization function normalize &lt;- function(x) { return((x - min(x)) / (max(x) - min(x))) } apply normalization to entire data frame concrete_norm &lt;- as.data.frame(lapply(concrete, normalize)) confirm that the range is now between zero and one summary(concrete_norm$strength) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.2664 0.4001 0.4172 0.5457 1.0000 compared to the original minimum and maximum summary(concrete$strength) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.33 23.71 34.45 35.82 46.13 82.60 create training and test data concrete_train &lt;- concrete_norm[1:773, ] concrete_test &lt;- concrete_norm[774:1030, ] 7.2.2 Step 3: Training a model on the data simple ANN with only a single hidden neuron set.seed(12345) # to guarantee repeatable results concrete_model &lt;- neuralnet(formula = strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age, data = concrete_train) visualize the network topology plot(concrete_model) 7.2.3 Step 4: Evaluating model performance obtain model results model_results &lt;- compute(concrete_model, concrete_test[1:8]) obtain predicted strength values predicted_strength &lt;- model_results$net.result examine the correlation between predicted and actual values cor(predicted_strength, concrete_test$strength) ## [,1] ## [1,] 0.8064655576 7.2.4 Step 5: Improving model performance a more complex neural network topology with 5 hidden neurons set.seed(12345) # to guarantee repeatable results concrete_model2 &lt;- neuralnet(strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age, data = concrete_train, hidden = 5) plot the network plot(concrete_model2) evaluate the results as we did before model_results2 &lt;- compute(concrete_model2, concrete_test[1:8]) predicted_strength2 &lt;- model_results2$net.result cor(predicted_strength2, concrete_test$strength) ## [,1] ## [1,] 0.9244533426 7.3 Part 2: Support Vector Machines 7.4 Example: Optical Character Recognition 7.4.1 Step 2: Exploring and preparing the data read in data and examine structure letters &lt;- read.csv(&quot;Chapter 07/letterdata.csv&quot;) str(letters) ## &#39;data.frame&#39;: 20000 obs. of 17 variables: ## $ letter: Factor w/ 26 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 20 9 4 14 7 19 2 1 10 13 ... ## $ xbox : int 2 5 4 7 2 4 4 1 2 11 ... ## $ ybox : int 8 12 11 11 1 11 2 1 2 15 ... ## $ width : int 3 3 6 6 3 5 5 3 4 13 ... ## $ height: int 5 7 8 6 1 8 4 2 4 9 ... ## $ onpix : int 1 2 6 3 1 3 4 1 2 7 ... ## $ xbar : int 8 10 10 5 8 8 8 8 10 13 ... ## $ ybar : int 13 5 6 9 6 8 7 2 6 2 ... ## $ x2bar : int 0 5 2 4 6 6 6 2 2 6 ... ## $ y2bar : int 6 4 6 6 6 9 6 2 6 2 ... ## $ xybar : int 6 13 10 4 6 5 7 8 12 12 ... ## $ x2ybar: int 10 3 3 4 5 6 6 2 4 1 ... ## $ xy2bar: int 8 9 7 10 9 6 6 8 8 9 ... ## $ xedge : int 0 2 3 6 1 0 2 1 1 8 ... ## $ xedgey: int 8 8 7 10 7 8 8 6 6 1 ... ## $ yedge : int 0 4 3 2 5 9 7 2 1 1 ... ## $ yedgex: int 8 10 9 8 10 7 10 7 7 8 ... divide into training and test data letters_train &lt;- letters[1:16000, ] letters_test &lt;- letters[16001:20000, ] 7.4.2 Step 3: Training a model on the data begin by training a simple linear SVM letter_classifier &lt;- ksvm(letter ~ ., data = letters_train, kernel = &quot;vanilladot&quot;) ## Setting default kernel parameters look at basic information about the model letter_classifier ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Linear (vanilla) kernel function. ## ## Number of Support Vectors : 7037 ## ## Objective Function Value : -14.1746 -20.0072 -23.5628 -6.2009 -7.5524 -32.7694 -49.9786 -18.1824 -62.1111 -32.7284 -16.2209 -32.2837 -28.9777 -51.2195 -13.276 -35.6217 -30.8612 -16.5256 -14.6811 -32.7475 -30.3219 -7.7956 -11.8138 -32.3463 -13.1262 -9.2692 -153.1654 -52.9678 -76.7744 -119.2067 -165.4437 -54.6237 -41.9809 -67.2688 -25.1959 -27.6371 -26.4102 -35.5583 -41.2597 -122.164 -187.9178 -222.0856 -21.4765 -10.3752 -56.3684 -12.2277 -49.4899 -9.3372 -19.2092 -11.1776 -100.2186 -29.1397 -238.0516 -77.1985 -8.3339 -4.5308 -139.8534 -80.8854 -20.3642 -13.0245 -82.5151 -14.5032 -26.7509 -18.5713 -23.9511 -27.3034 -53.2731 -11.4773 -5.12 -13.9504 -4.4982 -3.5755 -8.4914 -40.9716 -49.8182 -190.0269 -43.8594 -44.8667 -45.2596 -13.5561 -17.7664 -87.4105 -107.1056 -37.0245 -30.7133 -112.3218 -32.9619 -27.2971 -35.5836 -17.8586 -5.1391 -43.4094 -7.7843 -16.6785 -58.5103 -159.9936 -49.0782 -37.8426 -32.8002 -74.5249 -133.3423 -11.1638 -5.3575 -12.438 -30.9907 -141.6924 -54.2953 -179.0114 -99.8896 -10.288 -15.1553 -3.7815 -67.6123 -7.696 -88.9304 -47.6448 -94.3718 -70.2733 -71.5057 -21.7854 -12.7657 -7.4383 -23.502 -13.1055 -239.9708 -30.4193 -25.2113 -136.2795 -140.9565 -9.8122 -34.4584 -6.3039 -60.8421 -66.5793 -27.2816 -214.3225 -34.7796 -16.7631 -135.7821 -160.6279 -45.2949 -25.1023 -144.9059 -82.2352 -327.7154 -142.0613 -158.8821 -32.2181 -32.8887 -52.9641 -25.4937 -47.9936 -6.8991 -9.7293 -36.436 -70.3907 -187.7611 -46.9371 -89.8103 -143.4213 -624.3645 -119.2204 -145.4435 -327.7748 -33.3255 -64.0607 -145.4831 -116.5903 -36.2977 -66.3762 -44.8248 -7.5088 -217.9246 -12.9699 -30.504 -2.0369 -6.126 -14.4448 -21.6337 -57.3084 -20.6915 -184.3625 -20.1052 -4.1484 -4.5344 -0.828 -121.4411 -7.9486 -58.5604 -21.4878 -13.5476 -5.646 -15.629 -28.9576 -20.5959 -76.7111 -27.0119 -94.7101 -15.1713 -10.0222 -7.6394 -1.5784 -87.6952 -6.2239 -99.3711 -101.0906 -45.6639 -24.0725 -61.7702 -24.1583 -52.2368 -234.3264 -39.9749 -48.8556 -34.1464 -20.9664 -11.4525 -123.0277 -6.4903 -5.1865 -8.8016 -9.4618 -21.7742 -24.2361 -123.3984 -31.4404 -88.3901 -30.0924 -13.8198 -9.2701 -3.0823 -87.9624 -6.3845 -13.968 -65.0702 -105.523 -13.7403 -13.7625 -50.4223 -2.933 -8.4289 -80.3381 -36.4147 -112.7485 -4.1711 -7.8989 -1.2676 -90.8037 -21.4919 -7.2235 -47.9557 -3.383 -20.433 -64.6138 -45.5781 -56.1309 -6.1345 -18.6307 -2.374 -72.2553 -111.1885 -106.7664 -23.1323 -19.3765 -54.9819 -34.2953 -64.4756 -20.4115 -6.689 -4.378 -59.141 -34.2468 -58.1509 -33.8665 -10.6902 -53.1387 -13.7478 -20.1987 -55.0923 -3.8058 -60.0382 -235.4841 -12.6837 -11.7407 -17.3058 -9.7167 -65.8498 -17.1051 -42.8131 -53.1054 -25.0437 -15.302 -44.0749 -16.9582 -62.9773 -5.204 -5.2963 -86.1704 -3.7209 -6.3445 -1.1264 -122.5771 -23.9041 -355.0145 -31.1013 -32.619 -4.9664 -84.1048 -134.5957 -72.8371 -23.9002 -35.3077 -11.7119 -22.2889 -1.8598 -59.2174 -8.8994 -150.742 -1.8533 -1.9711 -9.9676 -0.5207 -26.9229 -30.429 -5.6289 ## Training error : 0.130062 7.4.3 Step 4: Evaluating model performance predictions on testing dataset letter_predictions &lt;- predict(letter_classifier, letters_test) head(letter_predictions) ## [1] U N V X N H ## Levels: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z table(letter_predictions, letters_test$letter) ## ## letter_predictions A B C D E F G H I J K L M N ## A 144 0 0 0 0 0 0 0 0 1 0 0 1 2 ## B 0 121 0 5 2 0 1 2 0 0 1 0 1 0 ## C 0 0 120 0 4 0 10 2 2 0 1 3 0 0 ## D 2 2 0 156 0 1 3 10 4 3 4 3 0 5 ## E 0 0 5 0 127 3 1 1 0 0 3 4 0 0 ## F 0 0 0 0 0 138 2 2 6 0 0 0 0 0 ## G 1 1 2 1 9 2 123 2 0 0 1 2 1 0 ## H 0 0 0 1 0 1 0 102 0 2 3 2 3 4 ## I 0 1 0 0 0 1 0 0 141 8 0 0 0 0 ## J 0 1 0 0 0 1 0 2 5 128 0 0 0 0 ## K 1 1 9 0 0 0 2 5 0 0 118 0 0 2 ## L 0 0 0 0 2 0 1 1 0 0 0 133 0 0 ## M 0 0 1 1 0 0 1 1 0 0 0 0 135 4 ## N 0 0 0 0 0 1 0 1 0 0 0 0 0 145 ## O 1 0 2 1 0 0 1 2 0 1 0 0 0 1 ## P 0 0 0 1 0 2 1 0 0 0 0 0 0 0 ## Q 0 0 0 0 0 0 8 2 0 0 0 3 0 0 ## R 0 7 0 0 1 0 3 8 0 0 13 0 0 1 ## S 1 1 0 0 1 0 3 0 1 1 0 1 0 0 ## T 0 0 0 0 3 2 0 0 0 0 1 0 0 0 ## U 1 0 3 1 0 0 0 2 0 0 0 0 0 0 ## V 0 0 0 0 0 1 3 4 0 0 0 0 1 2 ## W 0 0 0 0 0 0 1 0 0 0 0 0 2 0 ## X 0 1 0 0 2 0 0 1 3 0 1 6 0 0 ## Y 3 0 0 0 0 0 0 1 0 0 0 0 0 0 ## Z 2 0 0 0 1 0 0 0 3 4 0 0 0 0 ## ## letter_predictions O P Q R S T U V W X Y Z ## A 2 0 5 0 1 1 1 0 1 0 0 1 ## B 0 2 2 3 5 0 0 2 0 1 0 0 ## C 2 0 0 0 0 0 0 0 0 0 0 0 ## D 5 3 1 4 0 0 0 0 0 3 3 1 ## E 0 0 2 0 10 0 0 0 0 2 0 3 ## F 0 16 0 0 3 0 0 1 0 1 2 0 ## G 1 2 8 2 4 3 0 0 0 1 0 0 ## H 20 0 2 3 0 3 0 2 0 0 1 0 ## I 0 1 0 0 3 0 0 0 0 5 1 1 ## J 1 1 3 0 2 0 0 0 0 1 0 6 ## K 0 1 0 7 0 1 3 0 0 5 0 0 ## L 0 0 1 0 5 0 0 0 0 0 0 1 ## M 0 0 0 0 0 0 3 0 8 0 0 0 ## N 0 0 0 3 0 0 1 0 2 0 0 0 ## O 99 3 3 0 0 0 3 0 0 0 0 0 ## P 2 130 0 0 0 0 0 0 0 0 1 0 ## Q 3 1 124 0 5 0 0 0 0 0 2 0 ## R 1 1 0 138 0 1 0 1 0 0 0 0 ## S 0 0 14 0 101 3 0 0 0 2 0 10 ## T 0 0 0 0 3 133 1 0 0 0 2 2 ## U 1 0 0 0 0 0 152 0 0 1 1 0 ## V 1 0 3 1 0 0 0 126 1 0 4 0 ## W 0 0 0 0 0 0 4 4 127 0 0 0 ## X 1 0 0 0 1 0 0 0 0 137 1 1 ## Y 0 7 0 0 0 3 0 0 0 0 127 0 ## Z 0 0 0 0 18 3 0 0 0 0 0 132 look only at agreement vs. non-agreement construct a vector of TRUE/FALSE indicating correct/incorrect predictions agreement &lt;- letter_predictions == letters_test$letter table(agreement) ## agreement ## FALSE TRUE ## 643 3357 prop.table(table(agreement)) ## agreement ## FALSE TRUE ## 0.16075 0.83925 7.4.4 Step 5: Improving model performance set.seed(12345) letter_classifier_rbf &lt;- ksvm(letter ~ ., data = letters_train, kernel = &quot;rbfdot&quot;) letter_predictions_rbf &lt;- predict(letter_classifier_rbf, letters_test) agreement_rbf &lt;- letter_predictions_rbf == letters_test$letter table(agreement_rbf) ## agreement_rbf ## FALSE TRUE ## 275 3725 prop.table(table(agreement_rbf)) ## agreement_rbf ## FALSE TRUE ## 0.06875 0.93125 "],
["association-rules.html", "Chapter 8 Association Rules 8.1 Market Basket Analysis 8.2 Example: Identifying Frequently-Purchased Groceries", " Chapter 8 Association Rules 8.1 Market Basket Analysis 8.2 Example: Identifying Frequently-Purchased Groceries 8.2.1 Step 2: Exploring and preparing the data load the grocery data into a sparse matrix groceries &lt;- read.transactions(&quot;Chapter 08/groceries.csv&quot;, sep = &quot;,&quot;) summary(groceries) ## transactions as itemMatrix in sparse format with ## 9835 rows (elements/itemsets/transactions) and ## 169 columns (items) and a density of 0.02609146 ## ## most frequent items: ## whole milk other vegetables rolls/buns soda ## 2513 1903 1809 1715 ## yogurt (Other) ## 1372 34055 ## ## element (itemset/transaction) length distribution: ## sizes ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 ## 16 17 18 19 20 21 22 23 24 26 27 28 29 32 ## 46 29 14 14 9 11 4 6 1 1 1 1 3 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 3.000 4.409 6.000 32.000 ## ## includes extended item information - examples: ## labels ## 1 abrasive cleaner ## 2 artif. sweetener ## 3 baby cosmetics look at the first five transactions inspect(groceries[1:5]) ## items ## [1] {citrus fruit, ## margarine, ## ready soups, ## semi-finished bread} ## [2] {coffee, ## tropical fruit, ## yogurt} ## [3] {whole milk} ## [4] {cream cheese, ## meat spreads, ## pip fruit, ## yogurt} ## [5] {condensed milk, ## long life bakery product, ## other vegetables, ## whole milk} examine the frequency of items itemFrequency(groceries[, 1:3]) ## abrasive cleaner artif. sweetener baby cosmetics ## 0.0035587189 0.0032536858 0.0006100661 plot the frequency of items itemFrequencyPlot(groceries, support = 0.1) itemFrequencyPlot(groceries, topN = 20) a visualization of the sparse matrix for the first five transactions image(groceries[1:5]) visualization of a random sample of 100 transactions image(sample(groceries, 100)) 8.2.2 Step 3: Training a model on the data default settings result in zero rules learned apriori(groceries) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.1 1 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 983 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s]. ## sorting and recoding items ... [8 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 done [0.00s]. ## writing ... [0 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. ## set of 0 rules set better support and confidence levels to learn more rules groceryrules &lt;- apriori(groceries, parameter = list(support = 0.006, confidence = 0.25, minlen = 2)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.25 0.1 1 none FALSE TRUE 5 0.006 2 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 59 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[169 item(s), 9835 transaction(s)] done [0.01s]. ## sorting and recoding items ... [109 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 done [0.00s]. ## writing ... [463 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. groceryrules ## set of 463 rules 8.2.3 Step 4: Evaluating model performance summary of grocery association rules summary(groceryrules) ## set of 463 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 ## 150 297 16 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.000 3.000 2.711 3.000 4.000 ## ## summary of quality measures: ## support confidence lift count ## Min. :0.006101 Min. :0.2500 Min. :0.9932 Min. : 60.0 ## 1st Qu.:0.007117 1st Qu.:0.2971 1st Qu.:1.6229 1st Qu.: 70.0 ## Median :0.008744 Median :0.3554 Median :1.9332 Median : 86.0 ## Mean :0.011539 Mean :0.3786 Mean :2.0351 Mean :113.5 ## 3rd Qu.:0.012303 3rd Qu.:0.4495 3rd Qu.:2.3565 3rd Qu.:121.0 ## Max. :0.074835 Max. :0.6600 Max. :3.9565 Max. :736.0 ## ## mining info: ## data ntransactions support confidence ## groceries 9835 0.006 0.25 look at the first three rules inspect(groceryrules[1:3]) ## lhs rhs support confidence lift ## [1] {potted plants} =&gt; {whole milk} 0.006914082 0.4000000 1.565460 ## [2] {pasta} =&gt; {whole milk} 0.006100661 0.4054054 1.586614 ## [3] {herbs} =&gt; {root vegetables} 0.007015760 0.4312500 3.956477 ## count ## [1] 68 ## [2] 60 ## [3] 69 8.2.4 Step 5: Improving model performance sorting grocery rules by lift inspect(sort(groceryrules, by = &quot;lift&quot;)[1:5]) ## lhs rhs support confidence lift count ## [1] {herbs} =&gt; {root vegetables} 0.007015760 0.4312500 3.956477 69 ## [2] {berries} =&gt; {whipped/sour cream} 0.009049314 0.2721713 3.796886 89 ## [3] {other vegetables, ## tropical fruit, ## whole milk} =&gt; {root vegetables} 0.007015760 0.4107143 3.768074 69 ## [4] {beef, ## other vegetables} =&gt; {root vegetables} 0.007930859 0.4020619 3.688692 78 ## [5] {other vegetables, ## tropical fruit} =&gt; {pip fruit} 0.009456024 0.2634561 3.482649 93 finding subsets of rules containing any berry items berryrules &lt;- subset(groceryrules, items %in% &quot;berries&quot;) inspect(berryrules) ## lhs rhs support confidence lift ## [1] {berries} =&gt; {whipped/sour cream} 0.009049314 0.2721713 3.796886 ## [2] {berries} =&gt; {yogurt} 0.010574479 0.3180428 2.279848 ## [3] {berries} =&gt; {other vegetables} 0.010269446 0.3088685 1.596280 ## [4] {berries} =&gt; {whole milk} 0.011794611 0.3547401 1.388328 ## count ## [1] 89 ## [2] 104 ## [3] 101 ## [4] 116 writing the rules to a CSV file write(groceryrules, file = &quot;Chapter 08/groceryrules.csv&quot;, sep = &quot;,&quot;, quote = TRUE, row.names = FALSE) converting the rule set to a data frame groceryrules_df &lt;- as(groceryrules, &quot;data.frame&quot;) str(groceryrules_df) ## &#39;data.frame&#39;: 463 obs. of 5 variables: ## $ rules : Factor w/ 463 levels &quot;{baking powder} =&gt; {other vegetables}&quot;,..: 340 302 207 206 208 341 402 21 139 140 ... ## $ support : num 0.00691 0.0061 0.00702 0.00773 0.00773 ... ## $ confidence: num 0.4 0.405 0.431 0.475 0.475 ... ## $ lift : num 1.57 1.59 3.96 2.45 1.86 ... ## $ count : num 68 60 69 76 76 69 70 67 63 88 ... "],
["clustering-with-k-means.html", "Chapter 9 Clustering with k-means 9.1 k-means 9.2 Example: Finding Teen Market Segments", " Chapter 9 Clustering with k-means 9.1 k-means 9.2 Example: Finding Teen Market Segments 9.2.1 Step 2: Exploring and preparing the data teens &lt;- read.csv(&quot;Chapter 09/snsdata.csv&quot;) str(teens) ## &#39;data.frame&#39;: 30000 obs. of 40 variables: ## $ gradyear : int 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 ... ## $ gender : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 1 2 1 NA 1 1 2 1 1 ... ## $ age : num 19 18.8 18.3 18.9 19 ... ## $ friends : int 7 0 69 0 10 142 72 17 52 39 ... ## $ basketball : int 0 0 0 0 0 0 0 0 0 0 ... ## $ football : int 0 1 1 0 0 0 0 0 0 0 ... ## $ soccer : int 0 0 0 0 0 0 0 0 0 0 ... ## $ softball : int 0 0 0 0 0 0 0 1 0 0 ... ## $ volleyball : int 0 0 0 0 0 0 0 0 0 0 ... ## $ swimming : int 0 0 0 0 0 0 0 0 0 0 ... ## $ cheerleading: int 0 0 0 0 0 0 0 0 0 0 ... ## $ baseball : int 0 0 0 0 0 0 0 0 0 0 ... ## $ tennis : int 0 0 0 0 0 0 0 0 0 0 ... ## $ sports : int 0 0 0 0 0 0 0 0 0 0 ... ## $ cute : int 0 1 0 1 0 0 0 0 0 1 ... ## $ sex : int 0 0 0 0 1 1 0 2 0 0 ... ## $ sexy : int 0 0 0 0 0 0 0 1 0 0 ... ## $ hot : int 0 0 0 0 0 0 0 0 0 1 ... ## $ kissed : int 0 0 0 0 5 0 0 0 0 0 ... ## $ dance : int 1 0 0 0 1 0 0 0 0 0 ... ## $ band : int 0 0 2 0 1 0 1 0 0 0 ... ## $ marching : int 0 0 0 0 0 1 1 0 0 0 ... ## $ music : int 0 2 1 0 3 2 0 1 0 1 ... ## $ rock : int 0 2 0 1 0 0 0 1 0 1 ... ## $ god : int 0 1 0 0 1 0 0 0 0 6 ... ## $ church : int 0 0 0 0 0 0 0 0 0 0 ... ## $ jesus : int 0 0 0 0 0 0 0 0 0 2 ... ## $ bible : int 0 0 0 0 0 0 0 0 0 0 ... ## $ hair : int 0 6 0 0 1 0 0 0 0 1 ... ## $ dress : int 0 4 0 0 0 1 0 0 0 0 ... ## $ blonde : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mall : int 0 1 0 0 0 0 2 0 0 0 ... ## $ shopping : int 0 0 0 0 2 1 0 0 0 1 ... ## $ clothes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ hollister : int 0 0 0 0 0 0 2 0 0 0 ... ## $ abercrombie : int 0 0 0 0 0 0 0 0 0 0 ... ## $ die : int 0 0 0 0 0 0 0 0 0 0 ... ## $ death : int 0 0 1 0 0 0 0 0 0 0 ... ## $ drunk : int 0 0 0 0 1 1 0 0 0 0 ... ## $ drugs : int 0 0 0 0 1 0 0 0 0 0 ... look at missing data for female variable table(teens$gender) ## ## F M ## 22054 5222 table(teens$gender, useNA = &quot;ifany&quot;) ## ## F M &lt;NA&gt; ## 22054 5222 2724 look at missing data for age variable summary(teens$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 3.086 16.312 17.287 17.994 18.259 106.927 5086 eliminate age outliers teens$age &lt;- ifelse(teens$age &gt;= 13 &amp; teens$age &lt; 20, teens$age, NA) summary(teens$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 13.03 16.30 17.27 17.25 18.22 20.00 5523 reassign missing gender values to “unknown” teens$female &lt;- ifelse(teens$gender == &quot;F&quot; &amp; !is.na(teens$gender), 1, 0) teens$no_gender &lt;- ifelse(is.na(teens$gender), 1, 0) check our recoding work table(teens$gender, useNA = &quot;ifany&quot;) ## ## F M &lt;NA&gt; ## 22054 5222 2724 table(teens$female, useNA = &quot;ifany&quot;) ## ## 0 1 ## 7946 22054 table(teens$no_gender, useNA = &quot;ifany&quot;) ## ## 0 1 ## 27276 2724 finding the mean age by cohort mean(teens$age) # doesn&#39;t work ## [1] NA mean(teens$age, na.rm = TRUE) # works ## [1] 17.25243 age by cohort aggregate(data = teens, age ~ gradyear, mean, na.rm = TRUE) ## gradyear age ## 1 2006 18.65586 ## 2 2007 17.70617 ## 3 2008 16.76770 ## 4 2009 15.81957 create a vector with the average age for each gradyear, repeated by person ave_age &lt;- ave(teens$age, teens$gradyear, FUN = function(x) mean(x, na.rm = TRUE)) teens$age &lt;- ifelse(is.na(teens$age), ave_age, teens$age) check the summary results to ensure missing values are eliminated summary(teens$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 13.03 16.28 17.24 17.24 18.21 20.00 9.2.2 Step 3: Training a model on the data interests &lt;- teens[5:40] interests_z &lt;- as.data.frame(lapply(interests, scale)) set.seed(2345) teen_clusters &lt;- kmeans(interests_z, 5) 9.2.3 Step 4: Evaluating model performance look at the size of the clusters teen_clusters$size ## [1] 871 600 5981 1034 21514 look at the cluster centers teen_clusters$centers ## basketball football soccer softball volleyball swimming ## 1 0.16001227 0.2364174 0.10385512 0.07232021 0.18897158 0.23970234 ## 2 -0.09195886 0.0652625 -0.09932124 -0.01739428 -0.06219308 0.03339844 ## 3 0.52755083 0.4873480 0.29778605 0.37178877 0.37986175 0.29628671 ## 4 0.34081039 0.3593965 0.12722250 0.16384661 0.11032200 0.26943332 ## 5 -0.16695523 -0.1641499 -0.09033520 -0.11367669 -0.11682181 -0.10595448 ## cheerleading baseball tennis sports cute ## 1 0.3931445 0.02993479 0.13532387 0.10257837 0.37884271 ## 2 -0.1101103 -0.11487510 0.04062204 -0.09899231 -0.03265037 ## 3 0.3303485 0.35231971 0.14057808 0.32967130 0.54442929 ## 4 0.1856664 0.27527088 0.10980958 0.79711920 0.47866008 ## 5 -0.1136077 -0.10918483 -0.05097057 -0.13135334 -0.18878627 ## sex sexy hot kissed dance band ## 1 0.020042068 0.11740551 0.41389104 0.06787768 0.22780899 -0.10257102 ## 2 -0.042486141 -0.04329091 -0.03812345 -0.04554933 0.04573186 4.06726666 ## 3 0.002913623 0.24040196 0.38551819 -0.03356121 0.45662534 -0.02120728 ## 4 2.028471066 0.51266080 0.31708549 2.97973077 0.45535061 0.38053621 ## 5 -0.097928345 -0.09501817 -0.13810894 -0.13535855 -0.15932739 -0.12167214 ## marching music rock god church jesus ## 1 -0.10942590 0.1378306 0.05905951 0.03651755 -0.00709374 0.01458533 ## 2 5.25757242 0.4981238 0.15963917 0.09283620 0.06414651 0.04801941 ## 3 -0.10880541 0.2844999 0.21436936 0.35014919 0.53739806 0.27843424 ## 4 -0.02014608 1.1367885 1.21013948 0.41679142 0.16627797 0.12988313 ## 5 -0.11098063 -0.1532006 -0.12460034 -0.12144246 -0.15889274 -0.08557822 ## bible hair dress blonde mall shopping ## 1 -0.03692278 0.43807926 0.14905267 0.06137340 0.60368108 0.79806891 ## 2 0.05863810 -0.04484083 0.07201611 -0.01146396 -0.08724304 -0.03865318 ## 3 0.22990963 0.23612853 0.39407628 0.03471458 0.48318495 0.66327838 ## 4 0.08478769 2.55623737 0.53852195 0.36134138 0.62256686 0.27101815 ## 5 -0.06813159 -0.20498730 -0.14348036 -0.02918252 -0.18625656 -0.22865236 ## clothes hollister abercrombie die death ## 1 0.5651537331 4.1521844 3.96493810 0.043475966 0.09857501 ## 2 -0.0003526292 -0.1678300 -0.14129577 0.009447317 0.05135888 ## 3 0.3759725120 -0.0553846 -0.07417839 0.037989066 0.11972190 ## 4 1.2306917174 0.1610784 0.26324494 1.712181870 0.93631312 ## 5 -0.1865419798 -0.1557662 -0.14861104 -0.094875180 -0.08370729 ## drunk drugs ## 1 0.035614771 0.03443294 ## 2 -0.086773220 -0.06878491 ## 3 -0.009688746 -0.05973769 ## 4 1.897388200 2.73326605 ## 5 -0.087520105 -0.11423381 9.2.4 Step 5: Improving model performance apply the cluster IDs to the original data frame teens$cluster &lt;- teen_clusters$cluster look at the first five records teens[1:5, c(&quot;cluster&quot;, &quot;gender&quot;, &quot;age&quot;, &quot;friends&quot;)] ## cluster gender age friends ## 1 5 M 18.982 7 ## 2 3 F 18.801 0 ## 3 5 M 18.335 69 ## 4 5 F 18.875 0 ## 5 4 &lt;NA&gt; 18.995 10 mean age by cluster aggregate(data = teens, age ~ cluster, mean) ## cluster age ## 1 1 16.86497 ## 2 2 17.39037 ## 3 3 17.07656 ## 4 4 17.11957 ## 5 5 17.29849 proportion of females by cluster aggregate(data = teens, female ~ cluster, mean) ## cluster female ## 1 1 0.8381171 ## 2 2 0.7250000 ## 3 3 0.8378198 ## 4 4 0.8027079 ## 5 5 0.6994515 mean number of friends by cluster aggregate(data = teens, friends ~ cluster, mean) ## cluster friends ## 1 1 41.43054 ## 2 2 32.57333 ## 3 3 37.16185 ## 4 4 30.50290 ## 5 5 27.70052 "],
["evaluating-model-performance.html", "Chapter 10 Evaluating Model Performance 10.1 Confusion matrixes in R 10.2 Beyond accuracy: other performance measures 10.3 Visualizing Performance Tradeoffs 10.4 Estimating Future Performance", " Chapter 10 Evaluating Model Performance sms_raw &lt;- read.csv(&quot;Chapter 04/sms_spam.csv&quot;) sms_raw$type &lt;- factor(sms_raw$type) sms_corpus &lt;- VCorpus(VectorSource(sms_raw$text)) sms_corpus_clean &lt;- tm_map(sms_corpus, content_transformer(tolower)) sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removeNumbers) # remove numbers sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removeWords, stopwords()) # remove stop words sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removePunctuation) # remove punctuation replacePunctuation &lt;- function(x) { gsub(&quot;[[:punct:]]+&quot;, &quot; &quot;, x) } sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stemDocument) sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stripWhitespace) # eliminate unneeded whitespace lapply(sms_corpus[1:3], as.character) ## $`1` ## [1] &quot;Hope you are having a good week. Just checking in&quot; ## ## $`2` ## [1] &quot;K..give back my thanks.&quot; ## ## $`3` ## [1] &quot;Am also doing in cbe only. But have to pay.&quot; lapply(sms_corpus_clean[1:3], as.character) ## $`1` ## [1] &quot;hope good week just check&quot; ## ## $`2` ## [1] &quot;kgive back thank&quot; ## ## $`3` ## [1] &quot;also cbe pay&quot; sms_dtm &lt;- DocumentTermMatrix(sms_corpus_clean) sms_dtm_train &lt;- sms_dtm[1:4169, ] sms_dtm_test &lt;- sms_dtm[4170:5559, ] sms_train_labels &lt;- sms_raw[1:4169, ]$type sms_test_labels &lt;- sms_raw[4170:5559, ]$type sms_freq_words &lt;- findFreqTerms(sms_dtm_train, 5) sms_dtm_freq_train &lt;- sms_dtm_train[ , sms_freq_words] sms_dtm_freq_test &lt;- sms_dtm_test[ , sms_freq_words] convert_counts &lt;- function(x) { x &lt;- ifelse(x &gt; 0, &quot;Yes&quot;, &quot;No&quot;) } sms_train &lt;- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts) sms_test &lt;- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts) sms_classifier &lt;- naiveBayes(sms_train, sms_train_labels) sms_test_pred &lt;- predict(sms_classifier, sms_test) obtain the predicted probabilities sms_test_prob &lt;- predict(sms_classifier, sms_test, type = &quot;raw&quot;) head(sms_test_prob) ## ham spam ## [1,] 9.999996e-01 4.018928e-07 ## [2,] 9.999921e-01 7.852061e-06 ## [3,] 9.998548e-01 1.452211e-04 ## [4,] 9.999612e-01 3.875736e-05 ## [5,] 4.293821e-10 1.000000e+00 ## [6,] 9.998511e-01 1.489135e-04 combine the results into a data frame sms_results &lt;- data.frame(actual_type = sms_test_labels, predict_type = sms_test_pred, prob_spam = round(sms_test_prob[ , 2], 5), prob_ham = round(sms_test_prob[ , 1], 5)) uncomment this line to output the sms_results to CSV write.csv(sms_results, &quot;Chapter 10/sms_results.csv&quot;, row.names = FALSE) 10.1 Confusion matrixes in R sms_results &lt;- read.csv(&quot;Chapter 10/sms_results.csv&quot;) the first several test cases head(sms_results) ## actual_type predict_type prob_spam prob_ham ## 1 ham ham 0.00000 1.00000 ## 2 ham ham 0.00001 0.99999 ## 3 ham ham 0.00015 0.99985 ## 4 ham ham 0.00004 0.99996 ## 5 spam spam 1.00000 0.00000 ## 6 ham ham 0.00015 0.99985 test cases where the model is less confident head(subset(sms_results, prob_spam &gt; 0.40 &amp; prob_spam &lt; 0.60)) ## actual_type predict_type prob_spam prob_ham ## 717 ham spam 0.51364 0.48636 ## 732 spam ham 0.49940 0.50060 ## 1311 ham spam 0.54128 0.45872 ## 1324 spam spam 0.53398 0.46602 test cases where the model was wrong head(subset(sms_results, actual_type != predict_type)) ## actual_type predict_type prob_spam prob_ham ## 53 spam ham 0.00065 0.99935 ## 59 spam ham 0.00355 0.99645 ## 73 spam ham 0.01305 0.98695 ## 76 spam ham 0.00625 0.99375 ## 184 spam ham 0.01590 0.98410 ## 187 ham spam 0.72303 0.27697 specifying vectors table(sms_results$actual_type, sms_results$predict_type) ## ## ham spam ## ham 1201 6 ## spam 30 153 alternative solution using the formula interface (not shown in book) xtabs(~ actual_type + predict_type, sms_results) ## predict_type ## actual_type ham spam ## ham 1201 6 ## spam 30 153 using the CrossTable function CrossTable(sms_results$actual_type, sms_results$predict_type) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 1390 ## ## ## | sms_results$predict_type ## sms_results$actual_type | ham | spam | Row Total | ## ------------------------|-----------|-----------|-----------| ## ham | 1201 | 6 | 1207 | ## | 16.317 | 126.328 | | ## | 0.995 | 0.005 | 0.868 | ## | 0.976 | 0.038 | | ## | 0.864 | 0.004 | | ## ------------------------|-----------|-----------|-----------| ## spam | 30 | 153 | 183 | ## | 107.620 | 833.210 | | ## | 0.164 | 0.836 | 0.132 | ## | 0.024 | 0.962 | | ## | 0.022 | 0.110 | | ## ------------------------|-----------|-----------|-----------| ## Column Total | 1231 | 159 | 1390 | ## | 0.886 | 0.114 | | ## ------------------------|-----------|-----------|-----------| ## ## accuracy and error rate calculation accuracy (152 + 1203) / (152 + 1203 + 4 + 31) ## [1] 0.9748201 error rate (4 + 31) / (152 + 1203 + 4 + 31) ## [1] 0.02517986 error rate = 1 - accuracy 1 - 0.9748201 ## [1] 0.0251799 10.2 Beyond accuracy: other performance measures confusionMatrix(sms_results$predict_type, sms_results$actual_type, positive = &quot;spam&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction ham spam ## ham 1201 30 ## spam 6 153 ## ## Accuracy : 0.9741 ## 95% CI : (0.9643, 0.9818) ## No Information Rate : 0.8683 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.8801 ## Mcnemar&#39;s Test P-Value : 0.0001264 ## ## Sensitivity : 0.8361 ## Specificity : 0.9950 ## Pos Pred Value : 0.9623 ## Neg Pred Value : 0.9756 ## Prevalence : 0.1317 ## Detection Rate : 0.1101 ## Detection Prevalence : 0.1144 ## Balanced Accuracy : 0.9155 ## ## &#39;Positive&#39; Class : spam ## Kappa statistic example using SMS classifier pr_a &lt;- 0.865 + 0.109 pr_a ## [1] 0.974 pr_e &lt;- 0.868 * 0.888 + 0.132 * 0.112 pr_e ## [1] 0.785568 k &lt;- (pr_a - pr_e) / (1 - pr_e) k ## [1] 0.8787494 calculate kappa via the vcd package Kappa(table(sms_results$actual_type, sms_results$predict_type)) ## value ASE z Pr(&gt;|z|) ## Unweighted 0.8801 0.01962 44.85 0 ## Weighted 0.8801 0.01962 44.85 0 calculate kappa via the irr package kappa2(sms_results[1:2]) ## Cohen&#39;s Kappa for 2 Raters (Weights: unweighted) ## ## Subjects = 1390 ## Raters = 2 ## Kappa = 0.88 ## ## z = 32.9 ## p-value = 0 Sensitivity and specificity example using SMS classifier sens &lt;- 152 / (152 + 31) sens ## [1] 0.8306011 spec &lt;- 1203 / (1203 + 4) spec ## [1] 0.996686 example using the caret package sensitivity(sms_results$predict_type, sms_results$actual_type, positive = &quot;spam&quot;) ## [1] 0.8360656 specificity(sms_results$predict_type, sms_results$actual_type, negative = &quot;ham&quot;) ## [1] 0.995029 Precision and recall prec &lt;- 152 / (152 + 4) prec ## [1] 0.974359 rec &lt;- 152 / (152 + 31) rec ## [1] 0.8306011 example using the caret package posPredValue(sms_results$predict_type, sms_results$actual_type, positive = &quot;spam&quot;) ## [1] 0.9622642 sensitivity(sms_results$predict_type, sms_results$actual_type, positive = &quot;spam&quot;) ## [1] 0.8360656 F-measure f &lt;- (2 * prec * rec) / (prec + rec) f ## [1] 0.8967552 f &lt;- (2 * 152) / (2 * 152 + 4 + 31) f ## [1] 0.8967552 10.3 Visualizing Performance Tradeoffs pred &lt;- prediction(predictions = sms_results$prob_spam, labels = sms_results$actual_type) ROC curves add a reference line to the graph perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) plot(perf, main = &quot;ROC curve for SMS spam filter&quot;, col = &quot;blue&quot;, lwd = 2) abline(a = 0, b = 1, lwd = 2, lty = 2) calculate AUC perf.auc &lt;- performance(pred, measure = &quot;auc&quot;) str(perf.auc) ## Formal class &#39;performance&#39; [package &quot;ROCR&quot;] with 6 slots ## ..@ x.name : chr &quot;None&quot; ## ..@ y.name : chr &quot;Area under the ROC curve&quot; ## ..@ alpha.name : chr &quot;none&quot; ## ..@ x.values : list() ## ..@ y.values :List of 1 ## .. ..$ : num 0.984 ## ..@ alpha.values: list() unlist(perf.auc@y.values) ## [1] 0.9839846 10.4 Estimating Future Performance partitioning data credit &lt;- read.csv(&quot;Chapter 10/credit.csv&quot;) Holdout method using random IDs random_ids &lt;- order(runif(1000)) credit_train &lt;- credit[random_ids[1:500],] credit_validate &lt;- credit[random_ids[501:750], ] credit_test &lt;- credit[random_ids[751:1000], ] using caret function in_train &lt;- createDataPartition(credit$default, p = 0.75, list = FALSE) credit_train &lt;- credit[in_train, ] credit_test &lt;- credit[-in_train, ] 10-fold CV folds &lt;- createFolds(credit$default, k = 10) str(folds) ## List of 10 ## $ Fold01: int [1:100] 34 42 57 72 106 112 121 125 151 167 ... ## $ Fold02: int [1:100] 11 17 19 22 55 86 87 89 93 95 ... ## $ Fold03: int [1:100] 3 21 32 41 45 49 63 65 82 101 ... ## $ Fold04: int [1:100] 2 14 25 29 38 43 56 59 83 84 ... ## $ Fold05: int [1:100] 9 20 35 36 53 75 111 129 130 175 ... ## $ Fold06: int [1:100] 5 10 28 30 31 52 73 78 85 96 ... ## $ Fold07: int [1:100] 8 16 24 33 40 60 62 67 68 74 ... ## $ Fold08: int [1:100] 1 6 13 15 27 46 51 69 70 81 ... ## $ Fold09: int [1:100] 4 12 23 26 39 47 50 58 61 66 ... ## $ Fold10: int [1:100] 7 18 37 44 48 54 64 71 77 80 ... credit01_test &lt;- credit[folds$Fold01, ] credit01_train &lt;- credit[-folds$Fold01, ] Automating 10-fold CV for a C5.0 Decision Tree using lapply credit &lt;- read.csv(&quot;Chapter 10/credit.csv&quot;) set.seed(123) folds &lt;- createFolds(credit$default, k = 10) cv_results &lt;- lapply(folds, function(x) { credit_train &lt;- credit[-x, ] credit_test &lt;- credit[x, ] credit_model &lt;- C5.0(default ~ ., data = credit_train) credit_pred &lt;- predict(credit_model, credit_test) credit_actual &lt;- credit_test$default kappa &lt;- kappa2(data.frame(credit_actual, credit_pred))$value return(kappa) }) str(cv_results) ## List of 10 ## $ Fold01: num 0.343 ## $ Fold02: num 0.255 ## $ Fold03: num 0.109 ## $ Fold04: num 0.107 ## $ Fold05: num 0.338 ## $ Fold06: num 0.474 ## $ Fold07: num 0.245 ## $ Fold08: num 0.0365 ## $ Fold09: num 0.425 ## $ Fold10: num 0.505 mean(unlist(cv_results)) ## [1] 0.283796 "]
]
