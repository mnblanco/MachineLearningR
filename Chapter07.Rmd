# Neural Networks and Support Vector Machines

```{r setup, include=FALSE}
library(neuralnet)
library(kernlab)
```

## Part 1: Neural Networks

## Example: Modeling the Strength of Concrete

### Step 2: Exploring and preparing the data

- read in data and examine structure

```{r}
concrete <- read.csv("Chapter 07/concrete.csv")
str(concrete)
```

- custom normalization function

```{r}
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}
```

- apply normalization to entire data frame

```{r}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
```

- confirm that the range is now between zero and one

```{r}
summary(concrete_norm$strength)
```

- compared to the original minimum and maximum

```{r}
summary(concrete$strength)
```

- create training and test data

```{r}
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
```

### Step 3: Training a model on the data

- simple ANN with only a single hidden neuron

```{r}
set.seed(12345) # to guarantee repeatable results
concrete_model <- neuralnet(formula = strength ~ cement + slag +
                              ash + water + superplastic + 
                              coarseagg + fineagg + age,
                              data = concrete_train)
```

- visualize the network topology

```{r}
plot(concrete_model)
```

### Step 4: Evaluating model performance

- obtain model results

```{r}
model_results <- compute(concrete_model, concrete_test[1:8])
```

- obtain predicted strength values

```{r}
predicted_strength <- model_results$net.result
```

- examine the correlation between predicted and actual values

```{r}
cor(predicted_strength, concrete_test$strength)
```

### Step 5: Improving model performance

- a more complex neural network topology with 5 hidden neurons

```{r}
set.seed(12345) # to guarantee repeatable results
concrete_model2 <- neuralnet(strength ~ cement + slag +
                               ash + water + superplastic + 
                               coarseagg + fineagg + age,
                               data = concrete_train, hidden = 5)
```

- plot the network

```{r}
plot(concrete_model2)
```

- evaluate the results as we did before

```{r}
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
```

## Part 2: Support Vector Machines

## Example: Optical Character Recognition

### Step 2: Exploring and preparing the data

- read in data and examine structure

```{r}
letters <- read.csv("Chapter 07/letterdata.csv")
str(letters)
```

- divide into training and test data

```{r}
letters_train <- letters[1:16000, ]
letters_test  <- letters[16001:20000, ]
```

### Step 3: Training a model on the data

- begin by training a simple linear SVM

```{r}
letter_classifier <- ksvm(letter ~ ., data = letters_train,
                          kernel = "vanilladot")
```

- look at basic information about the model

```{r}
letter_classifier
```

### Step 4: Evaluating model performance

- predictions on testing dataset

```{r}
letter_predictions <- predict(letter_classifier, letters_test)

head(letter_predictions)

table(letter_predictions, letters_test$letter)
```

- look only at agreement vs. non-agreement
- construct a vector of TRUE/FALSE indicating correct/incorrect predictions

```{r}
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
```

### Step 5: Improving model performance

```{r}
set.seed(12345)
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train, kernel = "rbfdot")
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test)

agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
```