# Classification using Decision Trees and Rules

```{r setup, include=FALSE}
library(C50)
#library(RWeka)
library(gmodels)
library(OneR)
#https://www.engineeringbigdata.com/mushroom-classification-oner-jrip-r/
```

## Part 1: Decision Trees

- Decision tree learners are powerful classifiers, which utilize a tree structure to model the relationships among the features and the potential outcomes

- Classification rules represent knowledge in the form of logical if-else statements that assign a class to unlabeled examples

- The antecedent comprises certain combinations of feature values, while the consequent specifies the class value to assign when the rule's conditions are met

## Understanding Decision Trees

- calculate entropy of a two-class segment

```{r}
-0.60 * log2(0.60) - 0.40 * log2(0.40)

curve(-x * log2(x) - (1 - x) * log2(1 - x),
      col = "red", xlab = "x", ylab = "Entropy", lwd = 4)
```

## Example: Identifying Risky Bank Loans

### Step 2: Exploring and preparing the data

```{r}
credit <- read.csv("Chapter 05/credit.csv")
str(credit)
```

- look at two characteristics of the applicant

```{r}
table(credit$checking_balance)
table(credit$savings_balance)
```

- look at two characteristics of the loan

```{r}
summary(credit$months_loan_duration)
summary(credit$amount)
```

- look at the class variable

```{r}
table(credit$default)
```

- create a random sample for training and test data
- use set.seed to use the same random number sequence as the tutorial

```{r}
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
```

- split the data frames

```{r}
credit_train <- credit[train_sample, ]
credit_test  <- credit[-train_sample, ]
```

- check the proportion of class variable

```{r}
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
```

### Step 3: Training a model on the data

- build the simplest decision tree

```{r}
credit_model <- C5.0(credit_train[-17], credit_train$default)
```

- display simple facts about the tree

```{r}
credit_model
```

- display detailed information about the tree

```{r}
summary(credit_model)
```

### Step 4: Evaluating model performance

- create a factor vector of predictions on test data

```{r}
credit_pred <- predict(credit_model, credit_test)
```

- cross tabulation of predicted versus actual classes

```{r}
CrossTable(credit_test$default, credit_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```

### Step 5: Improving model performance

## Boosting the accuracy of decision trees

- boosted decision tree with 10 trials

```{r}
credit_boost10 <- C5.0(credit_train[-17], credit_train$default,
                       trials = 10)
credit_boost10
summary(credit_boost10)
credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost_pred10,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```

## Making some mistakes more costly than others

- create dimensions for a cost matrix

```{r}
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")
matrix_dimensions
```

- build the matrix

```{r}
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)
error_cost
```

- apply the cost matrix to the tree

```{r}
credit_cost <- C5.0(credit_train[-17], credit_train$default,
                          costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)

CrossTable(credit_test$default, credit_cost_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```

## Part 2: Rule Learners

## Example: Identifying Poisonous Mushrooms

### Step 2: Exploring and preparing the data

```{r}
mushrooms <- read.csv("Chapter 05/mushrooms.csv", stringsAsFactors = TRUE)
```

- examine the structure of the data frame

```{r}
str(mushrooms)
```

- drop the veil_type feature

```{r}
mushrooms$veil_type <- NULL
```

- examine the class distribution

```{r}
table(mushrooms$type)
```

### Step 3: Training a model on the data

- train OneR() on the data

```{r}
mushroom_1R <- OneR(type ~ ., data = mushrooms)
```

### Step 4: Evaluating model performance

```{r}
mushroom_1R
summary(mushroom_1R)
```

### Step 5: Improving model performance

```{r}
# mushroom_JRip <- JRip(type ~ ., data = mushrooms)
# mushroom_JRip
# summary(mushroom_JRip)
```

- Rule Learner Using C5.0 Decision Trees (not in text)

```{r}
mushroom_c5rules <- C5.0(type ~ odor + gill_size, data = mushrooms, rules = TRUE)
summary(mushroom_c5rules)
```

## Summary

- Two classification methods that use so-called "greedy" algorithms to partition the data according to feature values
- Decision trees use a divide and conquer strategy to create flowchart-like structures
- Rule learners separate and conquer data to identify logical if-else rules
- Both methods produce models that can be interpreted without a statistical background

- The C5.0 is a highly configurable decision tree algorithm
- The 1R algorithm used a single feature to achieve 99 percent accuracy
- The set of nine rules generated by the more sophisticated
